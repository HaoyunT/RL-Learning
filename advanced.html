<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>å¼ºåŒ–å­¦ä¹ è¿›é˜¶ç®—æ³• | Yun</title>
<!-- æ•°å­¦å…¬å¼æ¸²æŸ“ -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Prism.js æ ·å¼ä¸è„šæœ¬ -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-coy.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-javascript.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-bash.min.js"></script>
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>

<style>
body {
  margin: 0;
  font-family: "Segoe UI", "Roboto", -apple-system, sans-serif;
  background: linear-gradient(135deg, #121620 0%, #1b1f2b 40%, #221814 100%);
  color: #f5f5f5;
  line-height: 1.8;
  scroll-behavior: smooth;
}

header {
  position: sticky;
  top: 0;
  z-index: 100;
  text-align: center;
  background: rgba(20, 18, 24, 0.92);
  padding: 28px 20px 18px;
  backdrop-filter: blur(10px);
  border-bottom: 1px solid rgba(255, 140, 66, 0.35);
  box-shadow: 0 6px 22px rgba(0, 0, 0, 0.5);
}

header h1 {
  font-size: 2.3rem;
  color: #ff9740;
  margin: 0;
  font-weight: 700;
  letter-spacing: 1.5px;
}

nav ul {
  list-style: none;
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  gap: 10px;
  padding: 0;
  margin: 18px 0 0;
}

nav a {
  color: #f0dccc;
  text-decoration: none;
  font-weight: 600;
  font-size: 0.95rem;
  padding: 6px 16px;
  border-radius: 6px;
  transition: all 0.3s ease;
  background: rgba(255, 140, 66, 0.1);
  border: 1px solid rgba(255, 140, 66, 0.2);
}

nav a:hover, nav a.active {
  color: #ff9740;
  background: rgba(255, 140, 66, 0.2);
  border-color: rgba(255, 140, 66, 0.5);
  box-shadow: 0 0 14px rgba(255, 140, 66, 0.35);
}

.container {
  display: flex;
  position: relative;
  z-index: 5;
  max-width: 1400px;
  margin: 30px auto;
  gap: 30px;
  padding: 0 20px 40px;
}

aside {
  position: sticky;
  top: 130px;
  width: 260px;
  height: fit-content;
  background: rgba(28, 24, 32, 0.88);
  border: 1px solid rgba(255, 140, 66, 0.25);
  border-radius: 12px;
  padding: 22px;
  backdrop-filter: blur(12px);
  flex-shrink: 0;
}

aside h3 {
  color: #ff9740;
  font-size: 1.15rem;
  margin: 0 0 16px 0;
  padding-bottom: 10px;
  border-bottom: 2px solid rgba(255, 140, 66, 0.35);
}

aside ul {
  list-style: none;
  margin: 0;
  padding: 0;
  display: grid;
  gap: 6px;
}

aside a {
  color: #f0dccc;
  text-decoration: none;
  font-size: 0.9rem;
  padding: 8px 12px;
  border-radius: 6px;
  transition: all 0.2s ease;
  border-left: 3px solid transparent;
  background: transparent;
}

aside a:hover, aside a.active {
  color: #ff9740;
  background: rgba(255, 140, 66, 0.12);
  border-left-color: #ff9740;
  padding-left: 16px;
}

main {
  flex: 1;
  min-width: 0;
}

.chapter {
  background: rgba(28, 24, 32, 0.82);
  border: 1px solid rgba(255, 140, 66, 0.2);
  border-radius: 14px;
  padding: 36px;
  margin-bottom: 35px;
  transition: all 0.3s ease;
  box-shadow: 0 10px 30px rgba(0, 0, 0, 0.45);
}

.chapter:hover {
  border-color: rgba(255, 140, 66, 0.4);
  transform: translateY(-3px);
  box-shadow: 0 16px 38px rgba(0, 0, 0, 0.55);
}

.chapter h2 {
  color: #ff8c42;
  font-size: 1.85rem;
  margin: 0 0 24px 0;
  padding-bottom: 12px;
  border-bottom: 2px solid rgba(255, 140, 66, 0.3);
  font-weight: 700;
}

.chapter h3 {
  color: #ffb061;
  font-size: 1.25rem;
  margin-top: 26px;
  border-left: 4px solid rgba(255, 176, 97, 0.6);
  padding-left: 12px;
}

.chapter p, .chapter li {
  color: #f2e6dc;
}

.chapter strong {
  color: #ffb061;
}

.chapter code {
  background: rgba(255, 140, 66, 0.15);
  color: #ffd9b3;
  padding: 3px 6px;
  border-radius: 4px;
}

/* è§£å†³è¡Œå·é®æŒ¡ä»£ç é—®é¢˜ */
pre[class*="language-"] {
  background: linear-gradient(135deg, #1a0d1f 0%, #2d1b3d 100%) !important;
  padding: 20px !important;
  padding-left: 3.2em !important; /* ç»™ä»£ç æ­£æ–‡è®©å‡ºç©ºé—´ */
  border-radius: 10px !important;
  overflow-x: auto !important;
  font-size: 0.92rem !important;
  line-height: 1.6 !important;
  border: 1px solid rgba(156, 39, 176, 0.4) !important;
  position: relative;
  font-family: 'Fira Code', 'Courier New', 'Consolas', 'Monaco', monospace !important;
  box-shadow: 0 4px 20px rgba(156, 39, 176, 0.2) !important;
  text-rendering: optimizeLegibility !important;
  -webkit-font-smoothing: antialiased !important;
  -moz-osx-font-smoothing: grayscale !important;
}

/* ä»£ç è¡Œå· */
pre.line-numbers {
  padding-left: 60px !important;
}

.line-numbers .line-numbers-rows {
  left: 0 !important;
  width: 3em !important;
  background: transparent !important; /* å»æ‰èƒŒæ™¯é®ç½© */
  border-right: 1px solid #444 !important; /* åˆ†å‰²çº¿ */
}

.line-numbers-rows > span:before {
  color: rgba(186, 104, 200, 0.8) !important;
  font-weight: bold;
}

pre code {
  color: inherit !important;
  background: none !important;
  font-family: inherit !important;
  font-size: inherit !important;
  text-rendering: optimizeLegibility !important;
  -webkit-font-smoothing: antialiased !important;
  font-feature-settings: "liga" 0 !important;
}

/* ä»£ç å—å†…å„å…ƒç´ é«˜äº® */
.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
  color: #9c9c9c !important;
  font-style: italic;
}

.token.punctuation {
  color: #f8f8f2 !important;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
  color: #ff79c6 !important;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
  color: #50fa7b !important;
}

.token.operator,
.token.entity,
.token.url {
  color: #8be9fd !important;
}

.token.atrule,
.token.attr-value,
.token.keyword {
  color: #bd93f9 !important;
}

.token.function,
.token.class-name {
  color: #ffb86c !important;
}

.token.regex,
.token.important,
.token.variable {
  color: #f1fa8c !important;
}

/* ç¡®ä¿æ‰€æœ‰ä»£ç æ–‡æœ¬éƒ½æœ‰åŸºç¡€é¢œè‰² */
pre[class*="language-"] code,
pre[class*="language-"] {
  color: #f8f8f2 !important;
}

/* ä¿®å¤ç­‰å·å’Œç¬¦å·çš„æ¸²æŸ“é—®é¢˜ */
pre[class*="language-"] .token.operator,
pre[class*="language-"] .token.punctuation {
  background: none !important;
  text-shadow: none !important;
  font-weight: normal !important;
}

/* é˜²æ­¢å­—ä½“è¿å­—å½±å“ç¬¦å·æ˜¾ç¤º */
pre[class*="language-"] code {
  font-variant-ligatures: none !important;
  font-feature-settings: "liga" 0, "clig" 0 !important;
}

/* ä»£ç å—é¼ æ ‡æ‚¬åœæ•ˆæœ */
pre[class*="language-"]:hover {
  box-shadow: 
    0 12px 48px rgba(156, 39, 176, 0.3),
    inset 0 1px 0 rgba(255,255,255,0.1) !important;
  transform: translateY(-2px);
  transition: all 0.3s ease;
}

/* æ»šåŠ¨æ¡ç¾åŒ– */
pre[class*="language-"]::-webkit-scrollbar {
  height: 10px;
  background: rgba(156, 39, 176, 0.1);
}

pre[class*="language-"]::-webkit-scrollbar-thumb {
  background: rgba(156, 39, 176, 0.5);
  border-radius: 5px;
  transition: background 0.3s;
}

pre[class*="language-"]::-webkit-scrollbar-thumb:hover {
  background: rgba(156, 39, 176, 0.8);
}

table {
  width: 100%;
  border-collapse: collapse;
  margin: 18px 0;
  background: rgba(255, 140, 66, 0.06);
  border-radius: 10px;
  overflow: hidden;
  box-shadow: 0 6px 20px rgba(0, 0, 0, 0.35);
}

table th, table td {
  padding: 12px 16px;
  border-bottom: 1px solid rgba(255, 140, 66, 0.2);
  color: #f7ede2;
}

table th {
  background: rgba(255, 140, 66, 0.22);
  color: #fff1e6;
  font-size: 1.05rem;
}

table tr:last-child td {
  border-bottom: none;
}

/* é˜²æ­¢é®æŒ¡çš„é˜´å½±æˆ–æ¨¡ç³Šæ•ˆæœ */
pre[class*="language-"]::before,
pre[class*="language-"]::after {
  box-shadow: none !important;
  background: none !important;
}

footer {
  text-align: center;
  padding: 28px 20px;
  background: rgba(20, 18, 24, 0.9);
  border-top: 1px solid rgba(255, 140, 66, 0.3);
  color: #f0dccc;
  margin-top: 60px;
  font-size: 0.9rem;
}

footer span {
  color: #ff9740;
  font-weight: 600;
}

@media (max-width: 1024px) {
  .container {
    flex-direction: column;
  }
  aside {
    position: relative;
    top: auto;
    width: 100%;
  }
}

@media (max-width: 768px) {
  header h1 {
    font-size: 2rem;
  }
  nav a {
    font-size: 0.85rem;
    padding: 4px 12px;
  }
  .container {
    padding: 0 12px 30px;
  }
  .chapter {
    padding: 24px;
  }
}
</style>
</head>
<body>
<header>
  <h1>å¼ºåŒ–å­¦ä¹ ç¬”è®°ï¼ˆè¿›é˜¶ç¯‡ï¼‰</h1>
  <nav>
    <ul>
      <li><a href="index.html">é¦–é¡µ</a></li>
      <li><a href="home.html">åŸºç¡€ç¬”è®°</a></li>
      <li><a href="#chapter14">Dueling DQN</a></li>
      <li><a href="#chapter15">PER</a></li>
      <li><a href="#chapter16">NoisyNet</a></li>
      <li><a href="#chapter17">Rainbow</a></li>
  <li><a href="#chapter18">DDPG</a></li>
  <li><a href="#chapter19">TD3</a></li>
  <li><a href="#chapter20">TRPO</a></li>
  <li><a href="#chapter21">PPO</a></li>
  <li><a href="#chapter22">SAC</a></li>
  <li><a href="#chapter23">ç»„åˆç­–ç•¥</a></li>
  <li><a href="#chapter_summary">è¿›é˜¶æ€»ç»“</a></li>
    </ul>
  </nav>
</header>

<div class="container">
  <aside>
    <h3>ğŸ”¥ è¿›é˜¶ç›®å½•</h3>
    <ul>
      <li><a href="#chapter14">ç¬¬åå››ç« ï¼šDueling DQN</a></li>
      <li><a href="#chapter15">ç¬¬åäº”ç« ï¼šPER</a></li>
      <li><a href="#chapter16">ç¬¬åå…­ç« ï¼šNoisyNet DQN</a></li>
      <li><a href="#chapter17">ç¬¬åä¸ƒç« ï¼šRainbow DQN</a></li>
      <li><a href="#chapter18">ç¬¬åå…«ç« ï¼šDDPG</a></li>
      <li><a href="#chapter19">ç¬¬åä¹ç« ï¼šTD3</a></li>
      <li><a href="#chapter20">ç¬¬äºŒåç« ï¼šTRPO</a></li>
      <li><a href="#chapter21">ç¬¬äºŒåä¸€ç« ï¼šPPO</a></li>
      <li><a href="#chapter22">ç¬¬äºŒåäºŒç« ï¼šSAC</a></li>
      <li><a href="#chapter23">ç¬¬äºŒåä¸‰ç« ï¼šç­–ç•¥æ¢¯åº¦å¢å¼ºç»„åˆ</a></li>
      <li><a href="#chapter_summary">è¿›é˜¶æ€»ç»“</a></li>
    </ul>
  </aside>

  <main>
  <section id="chapter14" class="chapter">
    <h2>ç¬¬åå››ç« ï¼šDueling DQNï¼ˆåŒæµæ¶æ„ï¼‰</h2>
    <p>Dueling DQN å°† Q ç½‘ç»œæ‹†åˆ†ä¸º<strong>çŠ¶æ€ä»·å€¼åˆ†æ”¯ V(s)</strong>ä¸<strong>åŠ¨ä½œä¼˜åŠ¿åˆ†æ”¯ A(s,a)</strong>ï¼Œç”¨ä¸¤æ¡å¹¶è¡Œçš„å­ç½‘ç»œåˆ†åˆ«ä¼°è®¡çŠ¶æ€çš„æ•´ä½“ä»·å€¼ä¸åŠ¨ä½œçš„ç›¸å¯¹ä¼˜åŠ¿ï¼Œå†å°†äºŒè€…ç»„åˆå¾—åˆ° Q å€¼ã€‚è¿™ç§è®¾è®¡åœ¨ä¸€äº›çŠ¶æ€ä¸‹åŠ¨ä½œå·®å¼‚è¾ƒå°ï¼ˆæˆ–åŠ¨ä½œæ— å…³ï¼‰çš„åœºæ™¯ä¸‹èƒ½æ˜¾è‘—æå‡ä¼°è®¡ç¨³å®šæ€§ä¸æ”¶æ•›é€Ÿåº¦ã€‚</p>

    <h3>1. æ ¸å¿ƒç»“æ„ä¸æ•°å­¦å½¢å¼</h3>
    <p>ä¸»å¹²ç½‘ç»œï¼ˆå·ç§¯æˆ–å‰é¦ˆï¼‰æå–å…±äº«ç‰¹å¾ååˆ†ä¸ºä¸¤æ¡åˆ†æ”¯ï¼š</p>
    <ul>
      <li><strong>Value Streamï¼š</strong>è¾“å‡ºæ ‡é‡ $V(s)$ï¼Œè¡¨ç¤ºçŠ¶æ€æœ¬èº«çš„ä»·å€¼ï¼›</li>
      <li><strong>Advantage Streamï¼š</strong>è¾“å‡ºå‘é‡ $A(s,a)$ï¼Œè¡¨ç¤ºåœ¨çŠ¶æ€ $s$ ä¸‹å„åŠ¨ä½œçš„ç›¸å¯¹ä¼˜åŠ¿ã€‚</li>
    </ul>
    <p>ä¸ºä½¿åˆ†è§£å¯è¾¨è¯†ï¼ˆidentifiableï¼‰ï¼Œé€šå¸¸ç”¨ä¸‹å¼é‡ç»„ Q å€¼ï¼š</p>
    <p>$$Q(s, a) = V(s) + \Big(A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a')\Big)$$</p>
    <p>å‡å»ä¼˜åŠ¿å‡å€¼èƒ½ä¿è¯ $\sum_a A(s,a)=0$ï¼Œä»è€Œé¿å…ä»»æ„å¸¸æ•°åœ¨ V ä¸ A ä¹‹é—´è½¬ç§»å¯¼è‡´çš„ä¸å¯è¾¨è¯†é—®é¢˜ï¼ˆè§ä¸‹é¢çš„å”¯ä¸€åˆ†è§£è¯æ˜ï¼‰ã€‚</p>

    <h3>2. ç›´è§‰ä¸ä¼˜ç‚¹</h3>
    <ul>
      <li>å½“æŸäº›çŠ¶æ€ä¸‹ä¸åŒåŠ¨ä½œæ”¶ç›Šæ¥è¿‘æ—¶ï¼Œä¸“é—¨å­¦ä¹  $V(s)$ ä¼šæ›´ç¨³å®šï¼›</li>
      <li>$A(s,a)$ èšç„¦åŠ¨ä½œé—´å·®å¼‚ï¼Œèƒ½æ›´å¿«æ•æ‰å¾®å°ç­–ç•¥ä¼˜åŠ¿ï¼›</li>
      <li>å¯ä¸ Double DQNã€PERã€NoisyNet ç­‰æ”¹è¿›æ–¹æ³•ç›´æ¥ç»“åˆï¼Œå¸¸è§äºå¼ºåŒ–å­¦ä¹ ç«æŠ€å¹³å°ï¼ˆAtariã€ProcGen ç­‰ï¼‰ã€‚</li>
    </ul>

    <h3>3. å”¯ä¸€åˆ†è§£ï¼ˆIdentifiabilityï¼‰è¯æ˜</h3>
    <p>é—®é¢˜ï¼šç»™å®š Q(s,a)ï¼Œå°†å…¶å†™æˆ $Q(s,a)=V(s)+A(s,a)$ æ˜¯å¦å”¯ä¸€ï¼Ÿç­”æ¡ˆï¼šåœ¨æ²¡æœ‰çº¦æŸæ—¶ä¸æ˜¯å”¯ä¸€çš„ï¼›è‹¥å¯¹æ¯ä¸ª s æ–½åŠ çº¦æŸ $\sum_a A(s,a)=0$ï¼ˆæˆ–ç­‰ä»·çš„å¸¸æ•°çº¦æŸï¼‰ï¼Œåˆ™åˆ†è§£å”¯ä¸€ã€‚</p>
    <p><strong>è¯æ˜æ­¥éª¤ï¼š</strong></p>
    <p>å‡è®¾å­˜åœ¨ä¸¤ç»„åˆ†è§£ $(V, A)$ ä¸ $(V', A')$ æ»¡è¶³å¯¹æ‰€æœ‰ $(s,a)$ï¼š</p>
    <p>$$Q(s,a)=V(s)+A(s,a)=V'(s)+A'(s,a)$$</p>
    <p>ä»¤ $D(s)=V(s)-V'(s)$ï¼Œ$E(s,a)=A(s,a)-A'(s,a)$ï¼Œåˆ™å¯¹æ‰€æœ‰ $(s,a)$ æœ‰</p>
    <p>$$D(s)+E(s,a)=0 \quad \Rightarrow \quad E(s,a) = -D(s)$$</p>
    <p>å¯¹åŠ¨ä½œé›†åˆæ±‚å’Œå¹¶ä½¿ç”¨çº¦æŸ $\sum_a E(s,a)=0$ï¼š</p>
    <p>$$\sum_a E(s,a) = -\sum_a D(s) = -|\mathcal{A}|\, D(s) = 0 \Rightarrow D(s)=0$$</p>
    <p>äºæ˜¯ $D(s)=0$ï¼Œè¿›è€Œ $E(s,a)=0$ï¼Œå³ $(V,A)$ ä¸ $(V',A')$ åœ¨æ–½åŠ å‡å€¼çº¦æŸä¸‹ç›¸åŒï¼Œåˆ†è§£å”¯ä¸€ã€‚</p>

    <h3>4. ç½‘ç»œæ¶æ„è¯¦è§£</h3>
    <p>Dueling DQN çš„ç½‘ç»œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š</p>
    <ol>
      <li><strong>å…±äº«ç‰¹å¾æå–å±‚</strong>ï¼šå·ç§¯æˆ–å…¨è¿æ¥å±‚ç”¨äºæå–çŠ¶æ€ç‰¹å¾ï¼ˆä½œä¸ºåä¸¤ä¸ªåˆ†æ”¯çš„è¾“å…¥ï¼‰ï¼›</li>
      <li><strong>Value åˆ†æ”¯</strong>ï¼šç‹¬ç«‹çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œæœ€ç»ˆè¾“å‡ºå•ä¸ªæ ‡é‡ $V(s)$ï¼›</li>
      <li><strong>Advantage åˆ†æ”¯</strong>ï¼šç‹¬ç«‹çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œæœ€ç»ˆè¾“å‡º $|\mathcal{A}|$ ç»´å‘é‡ $A(s,a)$ã€‚</li>
    </ol>
    <p>å‰å‘ä¼ æ’­æ—¶ï¼Œå¯¹ Advantage å±‚è¾“å‡ºè¿›è¡Œ<strong>å»å‡å€¼å¤„ç†</strong>åä¸ Value å±‚ç»„åˆï¼š</p>
    <p>$$Q(s,a) = V(s) + \Big(A(s,a) - \frac{1}{|\mathcal{A}|}\sum_a A(s,a)\Big)$$</p>

    <h3>5. åˆå§‹åŒ–ä¸è®­ç»ƒç»†èŠ‚</h3>
    <p>ä¸ºç¡®ä¿ç½‘ç»œç¨³å®šæ€§å’Œå¯è¾¨è¯†æ€§ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„åˆå§‹åŒ–ï¼š</p>
    <ul>
      <li><strong>æƒé‡åˆå§‹åŒ–</strong>ï¼šä½¿ç”¨ Xavier uniform æˆ– Kaiming åˆå§‹åŒ–ï¼›å¯¹æ‰€æœ‰çº¿æ€§å±‚çš„åç½®åˆå§‹åŒ–ä¸º 0ï¼›</li>
      <li><strong>ä¸ºä»€ä¹ˆéœ€è¦ç‰¹æ®Šåˆå§‹åŒ–</strong>ï¼šAdvantage åˆ†æ”¯åˆæœŸåº”è¾“å‡ºæ¥è¿‘ 0 çš„å€¼ï¼Œè¿™æ · $A - \text{mean}(A) \approx 0$ï¼Œç¡®ä¿ Q å€¼åˆæœŸä¸»è¦ç”± $V(s)$ ä¸»å¯¼ï¼Œé¿å…ä¼˜åŠ¿åˆ†æ”¯è¿‡å¤§å¯¼è‡´çš„æ•°å€¼ä¸ç¨³å®šï¼›</li>
      <li><strong>ä¸»è¦ä¼˜åŠ¿</strong>ï¼šå°åˆå§‹ä¼˜åŠ¿ä¿è¯äº†åˆ†è§£çš„å¯è¾¨è¯†æ€§ï¼Œä½¿ V ä¸ A ä¸ä¼šåœ¨å‚æ•°ç©ºé—´ä¸­äº’ç›¸æŠµæ¶ˆã€‚</li>
    </ul>

    <h3>6. å®ç°æ­¥éª¤ï¼ˆé€æ­¥è¯´æ˜ï¼‰</h3>
    <p>ä¸‹é¢æŠŠ Dueling DQN çš„è®­ç»ƒè¿‡ç¨‹æ‹†æˆå¯ç›´æ¥è½åœ°çš„æ­¥éª¤ï¼Œä¾¿äºæŠŠä»£ç æ˜ å°„åˆ°å…·ä½“å®ç°ï¼š</p>
    <ol>
      <li><strong>å®šä¹‰ç½‘ç»œ</strong>ï¼šå…±äº«ä¸»å¹²æå–ç‰¹å¾ï¼›åˆ†å‡ºä¸¤ä¸ªå¤´â€”â€”Value è¾“å‡ºæ ‡é‡ V(s)ï¼ŒAdvantage è¾“å‡ºå‘é‡ A(s,a)ã€‚å‰å‘æ—¶å¯¹ A å»å‡å€¼å†ä¸ V é‡æ„ Qã€‚</li>
      <li><strong>ç»éªŒæ”¶é›†</strong>ï¼šä½¿ç”¨ Îµ-greedy æˆ–å¯å­¦ä¹ å™ªå£°ç­–ç•¥é‡‡æ · transition $(s,a,r,s',\text{done})$ï¼Œå¹¶å­˜å…¥ç»éªŒå›æ”¾æ±  ğ’Ÿã€‚</li>
      <li><strong>å°æ‰¹é‡é‡‡æ ·</strong>ï¼šä» ğ’Ÿ ä¸­éšæœºé‡‡æ · batchï¼›è‹¥ä½¿ç”¨ PER è¯·ç”¨é‡è¦æ€§æƒé‡ä¿®æ­£æŸå¤±ã€‚</li>
      <li><strong>ç›®æ ‡è®¡ç®—</strong>ï¼šç”¨ç›®æ ‡ç½‘ç»œ $\theta^-$ å‰å‘å¾—åˆ° $V'(s')$ ä¸ $A'(s',\cdot)$ï¼Œå¯¹ $A'$ å»å‡å€¼å¹¶é‡æ„ Q'ï¼›ç›®æ ‡ $y = r + \gamma \cdot \max_{a'} Q'(s',a')$ï¼ˆè‹¥ done åˆ™ $y = r$ï¼‰ã€‚</li>
      <li><strong>æŸå¤±ä¸æ›´æ–°</strong>ï¼šè®¡ç®— $L = \text{MSE}(y, Q(s,a;\theta))$ æˆ– Huber Lossï¼Œåå‘ä¼ æ’­å¹¶ç”¨ä¼˜åŒ–å™¨ step æ›´æ–°ä¸»ç½‘ç»œ $\theta$ã€‚</li>
      <li><strong>ç›®æ ‡ç½‘ç»œåŒæ­¥</strong>ï¼šé‡‡ç”¨ç¡¬æ›´æ–°ï¼ˆæ¯ C æ­¥å¤åˆ¶ï¼‰æˆ–è½¯æ›´æ–°ï¼ˆ$\theta^- \leftarrow \tau\theta + (1-\tau)\theta^-$ï¼‰ã€‚</li>
    </ol>

    <h3>7. PyTorch ä»£ç ç¤ºä¾‹</h3>
  <pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import deque

class DuelingNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DuelingNetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        
        # Value åˆ†æ”¯ï¼šè¾“å‡ºå•ä¸ªæ ‡é‡
        self.value_stream = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # Advantage åˆ†æ”¯ï¼šè¾“å‡º action_dim ç»´å‘é‡
        self.advantage_stream = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
        
        # æƒé‡åˆå§‹åŒ–
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        # æå–å…±äº«ç‰¹å¾
        features = self.feature(state)
        
        # Value ä¸ Advantage ç‹¬ç«‹å‰å‘
        v = self.value_stream(features)
        a = self.advantage_stream(features)
        
        # é‡ç»„ Q å€¼ï¼ˆå»å‡å€¼ä¿è¯å¯è¾¨è¯†ï¼‰
        a_mean = a.mean(dim=1, keepdim=True)
        q = v + (a - a_mean)
        
        return q

class DuelingDQN:
    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, epsilon=0.1):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        
        # ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_net = DuelingNetwork(state_dim, action_dim)
        self.target_q_net = DuelingNetwork(state_dim, action_dim)
        self.target_q_net.load_state_dict(self.q_net.state_dict())
        
        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        # ç»éªŒå›æ”¾æ± 
        self.memory = deque(maxlen=10000)
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state):
        """Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            with torch.no_grad():
                q_values = self.q_net(torch.FloatTensor(state).unsqueeze(0))
            return q_values.max(1)[1].item()
    
    def train_batch(self, batch_size):
        """è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡"""
        if len(self.memory) < batch_size:
            return
        
        # éšæœºé‡‡æ ·
        indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        
        states = torch.FloatTensor(np.array([x[0] for x in batch]))
        actions = torch.LongTensor([x[1] for x in batch])
        rewards = torch.FloatTensor([x[2] for x in batch])
        next_states = torch.FloatTensor(np.array([x[3] for x in batch]))
        dones = torch.FloatTensor([x[4] for x in batch])
        
        # å½“å‰ Q å€¼
        q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # ç›®æ ‡ Q å€¼
        with torch.no_grad():
            max_next_q = self.target_q_net(next_states).max(1)[0]
            target_q = rewards + self.gamma * max_next_q * (1 - dones)
        
        # æŸå¤±å’Œåå‘ä¼ æ’­
        loss = self.loss_fn(q_values, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)
        self.optimizer.step()
        
        return loss.item()
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_q_net.load_state_dict(self.q_net.state_dict())

# è®­ç»ƒç¤ºä¾‹
if __name__ == "__main__":
    state_dim = 4
    action_dim = 2
    agent = DuelingDQN(state_dim, action_dim)
    
    # æ¨¡æ‹Ÿç¯å¢ƒäº¤äº’
    for episode in range(100):
        state = np.random.randn(state_dim)
        episode_reward = 0
        
        for step in range(50):
            action = agent.act(state)
            next_state = np.random.randn(state_dim)
            reward = np.random.randn()
            done = step == 49
            
            agent.remember(state, action, reward, next_state, float(done))
            agent.train_batch(batch_size=32)
            
            episode_reward += reward
            state = next_state
        
        if (episode + 1) % 20 == 0:
            agent.update_target_network()
            print(f"Episode {episode+1}, Reward: {episode_reward:.2f}")
  </code></pre>
    <table>
      <tr>
        <th>é—®é¢˜</th>
        <th>è§£ç­”</th>
      </tr>
      <tr>
        <td>ä¸ºä»€ä¹ˆè¦å¯¹ Advantage å»å‡å€¼ï¼Ÿ</td>
        <td>å»å‡å€¼ä¿è¯äº†åˆ†è§£çš„å”¯ä¸€æ€§ã€‚è‹¥ä¸å»å‡å€¼ï¼Œå¯ä»¥åœ¨ V ä¸ A é—´ä»»æ„è½¬ç§»å¸¸æ•°å€¼è€Œäº§ç”Ÿç›¸åŒ Qï¼Œå¯¼è‡´å‚æ•°ä¸å¯è¾¨è¯†ã€‚</td>
      </tr>
      <tr>
        <td>V ä¸ A æ˜¯å¦éœ€è¦åŒæ—¶è®­ç»ƒï¼Ÿ</td>
        <td>æ˜¯çš„ã€‚ä¸¤ä¸ªåˆ†æ”¯å…±äº«ä¸»å¹²ï¼Œåå‘ä¼ æ’­ä¼šåŒæ—¶æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚V å­¦ä¹ çŠ¶æ€ä»·å€¼ï¼ŒA å­¦ä¹ åŠ¨ä½œç›¸å¯¹ä¼˜åŠ¿ï¼Œä¸¤è€…åˆ†å·¥æ˜ç¡®ã€‚</td>
      </tr>
      <tr>
        <td>åˆå§‹åŒ–ä¸ºä»€ä¹ˆå¾ˆé‡è¦ï¼Ÿ</td>
        <td>åˆæœŸ Advantage åº”æ¥è¿‘ 0ï¼Œä½¿ Q â‰ˆ Vï¼Œç¡®ä¿ç½‘ç»œç¨³å®šå¯åŠ¨ã€‚è¿‡å¤§çš„åˆå§‹ A ä¼šå¯¼è‡´æ•°å€¼éœ‡è¡ï¼Œå½±å“æ”¶æ•›ã€‚</td>
      </tr>
      <tr>
        <td>å¦‚ä½•ä¸ Double DQN ç»“åˆï¼Ÿ</td>
        <td>ç›®æ ‡è®¡ç®—æ—¶ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°ä»·å€¼ï¼Œä¸æ ‡å‡† Double DQN ç±»ä¼¼ï¼Œåªæ˜¯ç½‘ç»œç»“æ„ä»å•ä¸ª Q æ”¹ä¸º V+Aã€‚</td>
      </tr>
    </table>

    <h3>9. å®è·µè¦ç‚¹ä¸ç»„åˆç­–ç•¥</h3>
    <ul>
      <li><strong>ä¸ Double DQN ç»“åˆ</strong>ï¼šç›®æ ‡è®¡ç®—ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°ä»·å€¼ï¼Œæ¶ˆé™¤è¿‡ä¼°è®¡åå·®ï¼›</li>
      <li><strong>ä¸ PER ç»“åˆ</strong>ï¼šPrioritized Experience Replay èƒ½æ›´å¿«èšç„¦é«˜ TD è¯¯å·®æ ·æœ¬ï¼Œä¸ Dueling æ¶æ„ç›¸è¾…ç›¸æˆï¼›</li>
      <li><strong>ç¦»æ•£ vs è¿ç»­åŠ¨ä½œ</strong>ï¼šDueling DQN åœ¨ç¦»æ•£åŠ¨ä½œé—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ï¼›è¿ç»­æ§åˆ¶éœ€ä¸ç¡®å®šæ€§ç­–ç•¥ï¼ˆå¦‚ DDPGï¼‰æˆ–ç­–ç•¥åˆ†æ”¯æ”¹é€ ï¼›</li>
      <li><strong>è°ƒå‚å»ºè®®</strong>ï¼šé¿å…ä¼˜åŠ¿åˆ†æ”¯è¾“å‡ºè¿‡å¤§å¯¼è‡´æ•°å€¼ä¸ç¨³ï¼ˆå¯åŠ  L2 æ­£åˆ™æˆ–æ¢¯åº¦è£å‰ªï¼‰ï¼›ç›‘æ§ V ä¸ A çš„å¤§å°æ¯”ä¾‹ã€‚</li>
    </ul>
  </section>

    <section id="chapter15" class="chapter">
      <h2>ç¬¬åäº”ç« ï¼šPrioritized Experience Replayï¼ˆPERï¼‰</h2>
      <p>åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒPrioritized Experience Replay (PER) æ˜¯ä¸€ç§æ”¹è¿›çš„ç»éªŒå›æ”¾æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¼˜å…ˆé‡‡æ ·é‚£äº›å¯¹å­¦ä¹ æ›´æœ‰ä»·å€¼çš„ç»éªŒæ¥åŠ é€Ÿè®­ç»ƒã€‚PER é€šè¿‡è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ä¼˜å…ˆçº§ï¼ˆé€šå¸¸ä¸å…¶ TD è¯¯å·®æˆæ­£æ¯”ï¼‰ï¼Œæ¥æ§åˆ¶å“ªäº›ç»éªŒæ›´æœ‰å¯èƒ½è¢«é‡‡æ ·ï¼Œè¿›è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚</p>

      <h3>1. PER çš„å·¥ä½œåŸç†</h3>
      <p>åœ¨ä¼ ç»Ÿçš„ç»éªŒå›æ”¾ä¸­ï¼Œè®­ç»ƒæ ·æœ¬æ˜¯éšæœºé‡‡æ ·çš„ï¼Œå¯èƒ½ä¼šå¯¼è‡´è®¸å¤šæ— å…³çš„ç»éªŒå½±å“æ¨¡å‹å­¦ä¹ ã€‚è€Œ PER ä¼šæ ¹æ®æ¯ä¸ªç»éªŒçš„ <strong>TD è¯¯å·®ï¼ˆTemporal Difference Errorï¼‰</strong> æ¥è¯„ä¼°å®ƒçš„ä¼˜å…ˆçº§ï¼Œå¹¶æŒ‰ä¼˜å…ˆçº§æ¥é‡‡æ ·ç»éªŒã€‚</p>

      <h4>ä¼˜å…ˆçº§å½’ä¸€åŒ–</h4>
      <p>æ¯ä¸ªç»éªŒçš„ä¼˜å…ˆçº§é€šå¸¸æ˜¯ä¸å…¶ TD è¯¯å·® $\delta$ æˆæ­£æ¯”çš„ã€‚ä¸ºäº†ä½¿ä¼˜å…ˆçº§åœ¨åˆç†èŒƒå›´å†…ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šè¿›è¡Œ<strong>æœ€å¤§ä¼˜å…ˆçº§å½’ä¸€åŒ–</strong>ï¼Œä½¿æ¯ä¸ªä¼˜å…ˆçº§éƒ½åœ¨ $[0,1]$ ä¹‹é—´ã€‚</p>
      <p>$$\tilde{p}_i = \frac{p_i}{\max(p_1, p_2, \ldots, p_N)}$$</p>
      <p>å…¶ä¸­ï¼Œ$p_i$ æ˜¯æ ·æœ¬çš„åŸå§‹ä¼˜å…ˆçº§ï¼Œ$\tilde{p}_i$ æ˜¯å½’ä¸€åŒ–åçš„ä¼˜å…ˆçº§ã€‚</p>

      <h4>é‡‡æ ·æ¦‚ç‡</h4>
      <p>æ ¹æ®å½’ä¸€åŒ–åçš„ä¼˜å…ˆçº§ï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ªæ ·æœ¬è¢«é‡‡æ ·çš„æ¦‚ç‡ï¼š</p>
      <p>$$P(i) = \frac{\tilde{p}_i^\alpha}{\sum_{j=1}^N \tilde{p}_j^\alpha}$$</p>
      <p>å…¶ä¸­ $\alpha$ æ§åˆ¶ä¼˜å…ˆçº§å¯¹é‡‡æ ·æ¦‚ç‡çš„å½±å“ç¨‹åº¦ï¼ˆ$\alpha=0$ æ—¶æ˜¯å‡åŒ€é‡‡æ ·ï¼Œ$\alpha=1$ æ—¶å®Œå…¨æŒ‰ç…§ä¼˜å…ˆçº§é‡‡æ ·ï¼‰ã€‚</p>

      <h4>é‡è¦æ€§é‡‡æ ·æƒé‡ï¼ˆIS æƒé‡ï¼‰</h4>
      <p>ä¸ºäº†æ ¡æ­£é‡‡æ ·åå·®ï¼Œæˆ‘ä»¬å¼•å…¥<strong>é‡è¦æ€§é‡‡æ ·ï¼ˆImportance Sampling, ISï¼‰</strong>æƒé‡ï¼Œå¯¹æŸå¤±è¿›è¡ŒåŠ æƒã€‚è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š</p>
      <p>$$w_i = \left(\frac{1/N}{P(i)}\right)^\beta$$</p>
      <p>å…¶ä¸­ï¼Œ$\beta$ æ˜¯æ§åˆ¶é‡è¦æ€§é‡‡æ ·æƒé‡çš„å‚æ•°ï¼Œé€šå¸¸ä¼šä»è¾ƒå°å€¼ï¼ˆå¦‚ 0.4ï¼‰é€æ­¥å¢åŠ åˆ° 1ã€‚</p>

      <h4>åŠ æƒæŸå¤±</h4>
      <p>è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç”¨ IS æƒé‡å¯¹æŸå¤±è¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°åŠ æƒåçš„æŸå¤±ï¼š</p>
      <p>$$L_{\text{weighted}} = \frac{1}{N} \sum_{i=1}^N \tilde{w}_i \cdot L_i$$</p>
      <p>å…¶ä¸­ï¼Œ$L_i$ æ˜¯æ ·æœ¬ $i$ çš„æŸå¤±ã€‚</p>

      <h3>2. PER çš„ä¼˜ç‚¹</h3>
      <ul>
        <li><strong>åŠ é€Ÿå­¦ä¹ </strong>ï¼šé€šè¿‡ä¼˜å…ˆé‡‡æ ·é«˜ TD è¯¯å·®çš„æ ·æœ¬ï¼Œæ¨¡å‹å¯ä»¥æ›´å¿«åœ°ä¿®æ­£é”™è¯¯ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚</li>
        <li><strong>æé«˜æ ·æœ¬æ•ˆç‡</strong>ï¼šPER ä¼šè®©ç½‘ç»œæ›´å¤šåœ°å…³æ³¨é‡è¦ç»éªŒï¼Œå‡å°‘ä¸å¿…è¦çš„è®­ç»ƒæ ·æœ¬ï¼Œä»è€Œæé«˜æ ·æœ¬åˆ©ç”¨æ•ˆç‡ã€‚</li>
      </ul>

      <h3>3. Dueling DQN ä¸ PER ç»“åˆ</h3>
      <p>åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒDueling DQN æ˜¯ DQN çš„ä¸€ç§æ”¹è¿›ï¼Œå®ƒé€šè¿‡åˆ†å¼€è®¡ç®—<strong>çŠ¶æ€ä»·å€¼ï¼ˆVï¼‰</strong>å’Œ<strong>ä¼˜åŠ¿ï¼ˆAï¼‰</strong>æ¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆæœã€‚ç»“åˆ PERï¼Œå¯ä»¥è®© Dueling DQN æ›´åŠ é«˜æ•ˆåœ°ä»ç»éªŒæ± ä¸­å­¦ä¹ ã€‚</p>

      <h3>4. å®Œæ•´å®ç°ä»£ç </h3>

      <h4>Dueling DQN ç½‘ç»œç»“æ„</h4>
  <pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F

class DuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DuelingDQN, self).__init__()
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        self.value_stream = nn.Linear(128, 1)  # çŠ¶æ€ä»·å€¼ V(s)
        self.advantage_stream = nn.Linear(128, action_dim)  # ä¼˜åŠ¿ A(s, a)

    def forward(self, x):
        x = self.feature(x)
        value = self.value_stream(x)  # çŠ¶æ€ä»·å€¼ V(s)
        advantage = self.advantage_stream(x)  # ä¼˜åŠ¿ A(s, a)
        q_value = value + (advantage - advantage.mean(dim=1, keepdim=True))  # Q(s, a)
        return q_value
  </code></pre>

      <h4>Prioritized Experience Replay (PER) Buffer</h4>
  <pre><code class="language-python">
import numpy as np
import random

class SumTree:
    def __init__(self, capacity):
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1)  # SumTreeç»“æ„
        self.data = [None] * capacity
        self.write = 0
        self.n_entries = 0

    def _propagate(self, idx, change):
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)

    def update(self, idx, priority):
        change = priority - self.tree[idx]
        self.tree[idx] = priority
        self._propagate(idx, change)

    def add(self, priority, data):
        leaf = self.write + self.capacity - 1
        self.data[self.write] = data
        self.update(leaf, priority)
        self.write += 1
        if self.write >= self.capacity:
            self.write = 0
        self.n_entries = min(self.n_entries + 1, self.capacity)

    def get(self, s):
        idx = 0
        while True:
            left = 2 * idx + 1
            right = left + 1
            if left >= len(self.tree):
                leaf = idx
                break
            else:
                if s <= self.tree[left]:
                    idx = left
                else:
                    s -= self.tree[left]
                    idx = right
        data_idx = leaf - self.capacity + 1
        return leaf, self.tree[leaf], self.data[data_idx]

    @property
    def total(self):
        return self.tree[0]

class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6):
        self.tree = SumTree(capacity)
        self.alpha = alpha
        self.eps = 1e-6

    def add(self, error, sample):
        p = (abs(error) + self.eps) ** self.alpha
        self.tree.add(p, sample)

    def sample(self, n, beta=0.4):
        batch = []
        idxs = []
        segment = self.tree.total / n
        priorities = []
        for i in range(n):
            a = segment * i
            b = segment * (i + 1)
            s = random.uniform(a, b)
            idx, p, data = self.tree.get(s)
            batch.append(data)
            idxs.append(idx)
            priorities.append(p)
        probs = np.array(priorities) / self.tree.total
        weights = (self.tree.n_entries * probs) ** (-beta)
        weights /= weights.max()
        return batch, idxs, weights

    def update(self, idx, error):
        p = (abs(error) + self.eps) ** self.alpha
        self.tree.update(idx, p)
  </code></pre>

      <h4>è®­ç»ƒå¾ªç¯ï¼šé›†æˆ Dueling DQN å’Œ PER</h4>
  <pre><code class="language-python">
import torch.optim as optim

# åˆå§‹åŒ–ç¯å¢ƒï¼Œæ¨¡å‹ï¼Œä¼˜åŒ–å™¨
state_dim = 4
action_dim = 2
model = DuelingDQN(state_dim, action_dim)
target_model = DuelingDQN(state_dim, action_dim)
target_model.load_state_dict(model.state_dict())  # å°†ç›®æ ‡æ¨¡å‹åˆå§‹åŒ–ä¸ºä¸»æ¨¡å‹

optimizer = optim.Adam(model.parameters(), lr=1e-4)

# åˆå§‹åŒ– PER buffer
buffer = PrioritizedReplayBuffer(capacity=10000, alpha=0.6)

# è®­ç»ƒå¾ªç¯
for episode in range(1000):
    state = env.reset()  # é‡ç½®ç¯å¢ƒ
    done = False
    total_reward = 0
    
    while not done:
        # é€‰æ‹©åŠ¨ä½œ
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        q_values = model(state_tensor)
        action = torch.argmax(q_values, dim=1).item()

        # æ‰§è¡ŒåŠ¨ä½œå¹¶è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±
        next_state, reward, done, _ = env.step(action)
        total_reward += reward

        # å­˜å‚¨ç»éªŒåˆ° buffer
        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)
        next_q_values = target_model(next_state_tensor)
        max_next_q = torch.max(next_q_values).item()

        td_error = reward + 0.99 * max_next_q - q_values[0, action].item()  # TDè¯¯å·®
        buffer.add(td_error, (state, action, reward, next_state, done))

        state = next_state

        # æ¯éš”ä¸€å®šæ­¥æ•°è®­ç»ƒ
        if len(buffer.tree) > 32:
            batch, idxs, is_weights = buffer.sample(32, beta=0.4)

            states = torch.tensor([b[0] for b in batch], dtype=torch.float32)
            actions = torch.tensor([b[1] for b in batch], dtype=torch.long)
            rewards = torch.tensor([b[2] for b in batch], dtype=torch.float32)
            next_states = torch.tensor([b[3] for b in batch], dtype=torch.float32)
            dones = torch.tensor([b[4] for b in batch], dtype=torch.float32)

            q_values = model(states)
            next_q_values = target_model(next_states)
            max_next_q = next_q_values.max(1)[0]

            expected_q = rewards + (1 - dones) * 0.99 * max_next_q
            loss = F.mse_loss(q_values.gather(1, actions.unsqueeze(1)), expected_q.unsqueeze(1))

            # åŠ æƒæŸå¤±
            weighted_loss = (loss * is_weights).mean()

            optimizer.zero_grad()
            weighted_loss.backward()
            optimizer.step()

            # æ›´æ–°ä¼˜å…ˆçº§
            for i, idx in enumerate(idxs):
                td_error = abs(expected_q[i] - q_values[i, actions[i]]).item()
                buffer.update(idx, td_error)

    # æ¯éš”ä¸€å®šæ­¥æ•°æ›´æ–°ç›®æ ‡ç½‘ç»œ
    if episode % 10 == 0:
        target_model.load_state_dict(model.state_dict())
    
    print(f"Episode {episode}, Total Reward: {total_reward}")
  </code></pre>

      <h3>5. æ€»ç»“</h3>
      <ul>
        <li><strong>Prioritized Experience Replay (PER)</strong> é€šè¿‡ä¼˜å…ˆé‡‡æ ·å’Œé‡è¦æ€§é‡‡æ ·æƒé‡æ¥æ”¹è¿›ç»éªŒå›æ”¾ï¼Œæå‡å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ã€‚</li>
        <li>ç»“åˆ <strong>Dueling DQN</strong>ï¼Œå¯ä»¥é€šè¿‡åˆ†ç¦»çŠ¶æ€ä»·å€¼å’Œä¼˜åŠ¿æ¥æ›´ç¨³å®šåœ°å­¦ä¹ ï¼Œç»“åˆ PER è¿›ä¸€æ­¥æå‡å­¦ä¹ æ•ˆç‡ã€‚</li>
        <li>PER çš„æ ¸å¿ƒæ€æƒ³æ˜¯è®©æ¨¡å‹æ›´å¤šåœ°å…³æ³¨é‚£äº› TD è¯¯å·®è¾ƒå¤§çš„ç»éªŒï¼Œä»è€ŒåŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚</li>
        <li>å®ç°æ—¶éœ€è¦æ³¨æ„ SumTree æ•°æ®ç»“æ„çš„ç»´æŠ¤å’Œé‡è¦æ€§é‡‡æ ·æƒé‡çš„è®¡ç®—ã€‚</li>
      </ul>
    </section>

    <section id="chapter16" class="chapter">
      <h2>ç¬¬åå…­ç« ï¼šNoisyNet DQN</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter17" class="chapter">
      <h2>ç¬¬åä¸ƒç« ï¼šRainbow DQN</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter18" class="chapter">
      <h2>ç¬¬åå…«ç« ï¼šDDPGï¼ˆDeep Deterministic Policy Gradientï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter19" class="chapter">
      <h2>ç¬¬åä¹ç« ï¼šTD3ï¼ˆTwin Delayed DDPGï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter20" class="chapter">
      <h2>ç¬¬äºŒåç« ï¼šTRPOï¼ˆTrust Region Policy Optimizationï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter21" class="chapter">
      <h2>ç¬¬äºŒåä¸€ç« ï¼šPPOï¼ˆProximal Policy Optimizationï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter22" class="chapter">
      <h2>ç¬¬äºŒåäºŒç« ï¼šSACï¼ˆSoft Actor-Criticï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter23" class="chapter">
      <h2>ç¬¬äºŒåä¸‰ç« ï¼šç­–ç•¥æ¢¯åº¦å¢å¼ºç»„åˆ</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter_summary" class="chapter">
      <h2>è¿›é˜¶æ€»ç»“ä¸å­¦ä¹ å»ºè®®</h2>
      <h3>1. èƒ½åŠ›åœ°å›¾</h3>
      <table>
        <tr><th>èƒ½åŠ›</th><th>ç›¸å…³ç®—æ³•</th><th>æå‡è·¯å¾„</th></tr>
        <tr><td>æ ·æœ¬æ•ˆç‡</td><td>PERã€Rainbowã€SAC</td><td>ä¼˜å…ˆé‡‡æ · â†’ å¤šæ­¥ç›®æ ‡ â†’ æœ€å¤§ç†µ</td></tr>
        <tr><td>ç¨³å®šæ€§</td><td>Duelingã€TD3ã€PPO</td><td>æ¶æ„æ”¹è¿› â†’ åŒç½‘ç»œ â†’ å‰ªåˆ‡æŸå¤±</td></tr>
        <tr><td>è¿ç»­æ§åˆ¶</td><td>DDPGã€TD3ã€SAC</td><td>ç¡®å®šæ€§ç­–ç•¥ â†’ åŒ Critic â†’ æ¸©åº¦è‡ªé€‚åº”</td></tr>
      </table>

      <h3>2. å®æˆ˜è·¯çº¿</h3>
      <ol>
        <li>å…ˆåœ¨ <code>MuJoCo</code> ç­‰ç»å…¸ç¯å¢ƒå¤ç° PPO/SACï¼›</li>
        <li>å¼•å…¥ TD3 æˆ– Rainbow å¯¹æ¯”æŒ‡æ ‡ï¼›</li>
        <li>åœ¨è‡ªå®šä¹‰ç¯å¢ƒä¸­è°ƒå‚ï¼Œè§‚å¯Ÿå™ªå£°ã€ç†µç›®æ ‡ã€å­¦ä¹ ç‡å½±å“ã€‚</li>
      </ol>

      <h3>3. æ¨èé˜…è¯»</h3>
      <ul>
        <li>"Rainbow: Combining Improvements in Deep Reinforcement Learning"</li>
        <li>"Continuous Control with Deep Reinforcement Learning" (DDPG)</li>
        <li>"Addressing Function Approximation Error in Actor-Critic Methods" (TD3)</li>
        <li>"Proximal Policy Optimization Algorithms" (PPO)</li>
        <li>"Soft Actor-Critic" ç³»åˆ—è®ºæ–‡</li>
      </ul>
    </section>
  </main>
</div>

<footer>
  <p>Â© 2025 <span>Haoyun Tang</span> | Reinforcement Learning Advanced Notes</p>
</footer>

<script>
document.addEventListener('DOMContentLoaded', () => {
  document.querySelectorAll('pre[class*="language-"]').forEach(pre => {
    if (!pre.classList.contains('line-numbers')) {
      pre.classList.add('line-numbers');
    }
  });
  if (window.Prism) {
    Prism.highlightAll();
  }
});

window.addEventListener('load', () => {
  if (window.Prism) {
    Prism.highlightAll();
  }
});

const navLinks = document.querySelectorAll('nav a');
const asideLinks = document.querySelectorAll('aside a');
const combined = [...navLinks, ...asideLinks];

window.addEventListener('scroll', () => {
  const offset = window.scrollY + 160;
  combined.forEach(link => {
    const section = document.querySelector(link.getAttribute('href'));
    if (!section) return;
    if (section.offsetTop <= offset && section.offsetTop + section.offsetHeight > offset) {
      link.classList.add('active');
    } else {
      link.classList.remove('active');
    }
  });
});
</script>
</body>
</html>