<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>å¼ºåŒ–å­¦ä¹ è¿›é˜¶ç®—æ³• | Yun</title>
<!-- æ•°å­¦å…¬å¼æ¸²æŸ“ -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Prism.js æ ·å¼ä¸è„šæœ¬ -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-coy.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-javascript.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-bash.min.js"></script>
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>

<style>
body {
  margin: 0;
  font-family: "Segoe UI", "Roboto", -apple-system, sans-serif;
  background: linear-gradient(135deg, #121620 0%, #1b1f2b 40%, #221814 100%);
  color: #f5f5f5;
  line-height: 1.8;
  scroll-behavior: smooth;
}

header {
  position: sticky;
  top: 0;
  z-index: 100;
  text-align: center;
  background: rgba(20, 18, 24, 0.92);
  padding: 28px 20px 18px;
  backdrop-filter: blur(10px);
  border-bottom: 1px solid rgba(255, 140, 66, 0.35);
  box-shadow: 0 6px 22px rgba(0, 0, 0, 0.5);
}

header h1 {
  font-size: 2.3rem;
  color: #ff9740;
  margin: 0;
  font-weight: 700;
  letter-spacing: 1.5px;
}

nav ul {
  list-style: none;
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  gap: 10px;
  padding: 0;
  margin: 18px 0 0;
}

nav a {
  color: #f0dccc;
  text-decoration: none;
  font-weight: 600;
  font-size: 0.95rem;
  padding: 6px 16px;
  border-radius: 6px;
  transition: all 0.3s ease;
  background: rgba(255, 140, 66, 0.1);
  border: 1px solid rgba(255, 140, 66, 0.2);
}

nav a:hover, nav a.active {
  color: #ff9740;
  background: rgba(255, 140, 66, 0.2);
  border-color: rgba(255, 140, 66, 0.5);
  box-shadow: 0 0 14px rgba(255, 140, 66, 0.35);
}

.container {
  display: flex;
  position: relative;
  z-index: 5;
  max-width: 1400px;
  margin: 30px auto;
  gap: 30px;
  padding: 0 20px 40px;
}

aside {
  position: sticky;
  top: 130px;
  width: 260px;
  height: fit-content;
  background: rgba(28, 24, 32, 0.88);
  border: 1px solid rgba(255, 140, 66, 0.25);
  border-radius: 12px;
  padding: 22px;
  backdrop-filter: blur(12px);
  flex-shrink: 0;
}

aside h3 {
  color: #ff9740;
  font-size: 1.15rem;
  margin: 0 0 16px 0;
  padding-bottom: 10px;
  border-bottom: 2px solid rgba(255, 140, 66, 0.35);
}

aside ul {
  list-style: none;
  margin: 0;
  padding: 0;
  display: grid;
  gap: 6px;
}

aside a {
  color: #f0dccc;
  text-decoration: none;
  font-size: 0.9rem;
  padding: 8px 12px;
  border-radius: 6px;
  transition: all 0.2s ease;
  border-left: 3px solid transparent;
  background: transparent;
}

aside a:hover, aside a.active {
  color: #ff9740;
  background: rgba(255, 140, 66, 0.12);
  border-left-color: #ff9740;
  padding-left: 16px;
}

main {
  flex: 1;
  min-width: 0;
}

.chapter {
  background: rgba(28, 24, 32, 0.82);
  border: 1px solid rgba(255, 140, 66, 0.2);
  border-radius: 14px;
  padding: 36px;
  margin-bottom: 35px;
  transition: all 0.3s ease;
  box-shadow: 0 10px 30px rgba(0, 0, 0, 0.45);
}

.chapter:hover {
  border-color: rgba(255, 140, 66, 0.4);
  transform: translateY(-3px);
  box-shadow: 0 16px 38px rgba(0, 0, 0, 0.55);
}

.chapter h2 {
  color: #ff8c42;
  font-size: 1.85rem;
  margin: 0 0 24px 0;
  padding-bottom: 12px;
  border-bottom: 2px solid rgba(255, 140, 66, 0.3);
  font-weight: 700;
}

.chapter h3 {
  color: #ffb061;
  font-size: 1.25rem;
  margin-top: 26px;
  border-left: 4px solid rgba(255, 176, 97, 0.6);
  padding-left: 12px;
}

.chapter p, .chapter li {
  color: #f2e6dc;
}

.chapter strong {
  color: #ffb061;
}

.chapter code {
  background: rgba(255, 140, 66, 0.15);
  color: #ffd9b3;
  padding: 3px 6px;
  border-radius: 4px;
}

/* è§£å†³è¡Œå·é®æŒ¡ä»£ç é—®é¢˜ */
pre[class*="language-"] {
  background: linear-gradient(135deg, #1a0d1f 0%, #2d1b3d 100%) !important;
  padding: 20px !important;
  padding-left: 3.2em !important; /* ç»™ä»£ç æ­£æ–‡è®©å‡ºç©ºé—´ */
  border-radius: 10px !important;
  overflow-x: auto !important;
  font-size: 0.92rem !important;
  line-height: 1.6 !important;
  border: 1px solid rgba(156, 39, 176, 0.4) !important;
  position: relative;
  font-family: 'Fira Code', 'Courier New', 'Consolas', 'Monaco', monospace !important;
  box-shadow: 0 4px 20px rgba(156, 39, 176, 0.2) !important;
  text-rendering: optimizeLegibility !important;
  -webkit-font-smoothing: antialiased !important;
  -moz-osx-font-smoothing: grayscale !important;
}

/* ä»£ç è¡Œå· */
pre.line-numbers {
  padding-left: 60px !important;
}

.line-numbers .line-numbers-rows {
  left: 0 !important;
  width: 3em !important;
  background: transparent !important; /* å»æ‰èƒŒæ™¯é®ç½© */
  border-right: 1px solid #444 !important; /* åˆ†å‰²çº¿ */
}

.line-numbers-rows > span:before {
  color: rgba(186, 104, 200, 0.8) !important;
  font-weight: bold;
}

pre code {
  color: inherit !important;
  background: none !important;
  font-family: inherit !important;
  font-size: inherit !important;
  text-rendering: optimizeLegibility !important;
  -webkit-font-smoothing: antialiased !important;
  font-feature-settings: "liga" 0 !important;
}

/* ä»£ç å—å†…å„å…ƒç´ é«˜äº® */
.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
  color: #9c9c9c !important;
  font-style: italic;
}

.token.punctuation {
  color: #f8f8f2 !important;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
  color: #ff79c6 !important;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
  color: #50fa7b !important;
}

.token.operator,
.token.entity,
.token.url {
  color: #8be9fd !important;
}

.token.atrule,
.token.attr-value,
.token.keyword {
  color: #bd93f9 !important;
}

.token.function,
.token.class-name {
  color: #ffb86c !important;
}

.token.regex,
.token.important,
.token.variable {
  color: #f1fa8c !important;
}

/* ç¡®ä¿æ‰€æœ‰ä»£ç æ–‡æœ¬éƒ½æœ‰åŸºç¡€é¢œè‰² */
pre[class*="language-"] code,
pre[class*="language-"] {
  color: #f8f8f2 !important;
}

/* ä¿®å¤ç­‰å·å’Œç¬¦å·çš„æ¸²æŸ“é—®é¢˜ */
pre[class*="language-"] .token.operator,
pre[class*="language-"] .token.punctuation {
  background: none !important;
  text-shadow: none !important;
  font-weight: normal !important;
}

/* é˜²æ­¢å­—ä½“è¿å­—å½±å“ç¬¦å·æ˜¾ç¤º */
pre[class*="language-"] code {
  font-variant-ligatures: none !important;
  font-feature-settings: "liga" 0, "clig" 0 !important;
}

/* ä»£ç å—é¼ æ ‡æ‚¬åœæ•ˆæœ */
pre[class*="language-"]:hover {
  box-shadow: 
    0 12px 48px rgba(156, 39, 176, 0.3),
    inset 0 1px 0 rgba(255,255,255,0.1) !important;
  transform: translateY(-2px);
  transition: all 0.3s ease;
}

/* æ»šåŠ¨æ¡ç¾åŒ– */
pre[class*="language-"]::-webkit-scrollbar {
  height: 10px;
  background: rgba(156, 39, 176, 0.1);
}

pre[class*="language-"]::-webkit-scrollbar-thumb {
  background: rgba(156, 39, 176, 0.5);
  border-radius: 5px;
  transition: background 0.3s;
}

pre[class*="language-"]::-webkit-scrollbar-thumb:hover {
  background: rgba(156, 39, 176, 0.8);
}

table {
  width: 100%;
  border-collapse: collapse;
  margin: 18px 0;
  background: rgba(255, 140, 66, 0.06);
  border-radius: 10px;
  overflow: hidden;
  box-shadow: 0 6px 20px rgba(0, 0, 0, 0.35);
}

table th, table td {
  padding: 12px 16px;
  border-bottom: 1px solid rgba(255, 140, 66, 0.2);
  color: #f7ede2;
}

table th {
  background: rgba(255, 140, 66, 0.22);
  color: #fff1e6;
  font-size: 1.05rem;
}

table tr:last-child td {
  border-bottom: none;
}

/* é˜²æ­¢é®æŒ¡çš„é˜´å½±æˆ–æ¨¡ç³Šæ•ˆæœ */
pre[class*="language-"]::before,
pre[class*="language-"]::after {
  box-shadow: none !important;
  background: none !important;
}

footer {
  text-align: center;
  padding: 28px 20px;
  background: rgba(20, 18, 24, 0.9);
  border-top: 1px solid rgba(255, 140, 66, 0.3);
  color: #f0dccc;
  margin-top: 60px;
  font-size: 0.9rem;
}

footer span {
  color: #ff9740;
  font-weight: 600;
}

@media (max-width: 1024px) {
  .container {
    flex-direction: column;
  }
  aside {
    position: relative;
    top: auto;
    width: 100%;
  }
}

@media (max-width: 768px) {
  header h1 {
    font-size: 2rem;
  }
  nav a {
    font-size: 0.85rem;
    padding: 4px 12px;
  }
  .container {
    padding: 0 12px 30px;
  }
  .chapter {
    padding: 24px;
  }
}
</style>
</head>
<body>
<header>
  <h1>å¼ºåŒ–å­¦ä¹ ç¬”è®°ï¼ˆè¿›é˜¶ç¯‡ï¼‰</h1>
  <nav>
    <ul>
      <li><a href="index.html">é¦–é¡µ</a></li>
      <li><a href="home.html">åŸºç¡€ç¬”è®°</a></li>
      <li><a href="#chapter14">Dueling DQN</a></li>
      <li><a href="#chapter15">PER</a></li>
      <li><a href="#chapter16">NoisyNet</a></li>
      <li><a href="#chapter17">Rainbow</a></li>
  <li><a href="#chapter18">DDPG</a></li>
  <li><a href="#chapter19">TD3</a></li>
  <li><a href="#chapter20">TRPO</a></li>
  <li><a href="#chapter21">PPO</a></li>
  <li><a href="#chapter22">SAC</a></li>
  <li><a href="#chapter23">ç»„åˆç­–ç•¥</a></li>
  <li><a href="#chapter_summary">è¿›é˜¶æ€»ç»“</a></li>
    </ul>
  </nav>
</header>

<div class="container">
  <aside>
    <h3>ğŸ”¥ è¿›é˜¶ç›®å½•</h3>
    <ul>
      <li><a href="#chapter14">ç¬¬åå››ç« ï¼šDueling DQN</a></li>
      <li><a href="#chapter15">ç¬¬åäº”ç« ï¼šPER</a></li>
      <li><a href="#chapter16">ç¬¬åå…­ç« ï¼šNoisyNet DQN</a></li>
      <li><a href="#chapter17">ç¬¬åä¸ƒç« ï¼šRainbow DQN</a></li>
      <li><a href="#chapter18">ç¬¬åå…«ç« ï¼šDDPG</a></li>
      <li><a href="#chapter19">ç¬¬åä¹ç« ï¼šTD3</a></li>
      <li><a href="#chapter20">ç¬¬äºŒåç« ï¼šTRPO</a></li>
      <li><a href="#chapter21">ç¬¬äºŒåä¸€ç« ï¼šPPO</a></li>
      <li><a href="#chapter22">ç¬¬äºŒåäºŒç« ï¼šSAC</a></li>
      <li><a href="#chapter23">ç¬¬äºŒåä¸‰ç« ï¼šç­–ç•¥æ¢¯åº¦å¢å¼ºç»„åˆ</a></li>
      <li><a href="#chapter_summary">è¿›é˜¶æ€»ç»“</a></li>
    </ul>
  </aside>

  <main>
  <section id="chapter14" class="chapter">
    <h2>ç¬¬åå››ç« ï¼šDueling DQNï¼ˆåŒæµæ¶æ„ï¼‰</h2>
    <p>Dueling DQN å°† Q ç½‘ç»œæ‹†åˆ†ä¸º<strong>çŠ¶æ€ä»·å€¼åˆ†æ”¯ V(s)</strong>ä¸<strong>åŠ¨ä½œä¼˜åŠ¿åˆ†æ”¯ A(s,a)</strong>ï¼Œç”¨ä¸¤æ¡å¹¶è¡Œçš„å­ç½‘ç»œåˆ†åˆ«ä¼°è®¡çŠ¶æ€çš„æ•´ä½“ä»·å€¼ä¸åŠ¨ä½œçš„ç›¸å¯¹ä¼˜åŠ¿ï¼Œå†å°†äºŒè€…ç»„åˆå¾—åˆ° Q å€¼ã€‚è¿™ç§è®¾è®¡åœ¨ä¸€äº›çŠ¶æ€ä¸‹åŠ¨ä½œå·®å¼‚è¾ƒå°ï¼ˆæˆ–åŠ¨ä½œæ— å…³ï¼‰çš„åœºæ™¯ä¸‹èƒ½æ˜¾è‘—æå‡ä¼°è®¡ç¨³å®šæ€§ä¸æ”¶æ•›é€Ÿåº¦ã€‚</p>

    <h3>1. æ ¸å¿ƒç»“æ„ä¸æ•°å­¦å½¢å¼</h3>
    <p>ä¸»å¹²ç½‘ç»œï¼ˆå·ç§¯æˆ–å‰é¦ˆï¼‰æå–å…±äº«ç‰¹å¾ååˆ†ä¸ºä¸¤æ¡åˆ†æ”¯ï¼š</p>
    <ul>
      <li><strong>Value Streamï¼š</strong>è¾“å‡ºæ ‡é‡ $V(s)$ï¼Œè¡¨ç¤ºçŠ¶æ€æœ¬èº«çš„ä»·å€¼ï¼›</li>
      <li><strong>Advantage Streamï¼š</strong>è¾“å‡ºå‘é‡ $A(s,a)$ï¼Œè¡¨ç¤ºåœ¨çŠ¶æ€ $s$ ä¸‹å„åŠ¨ä½œçš„ç›¸å¯¹ä¼˜åŠ¿ã€‚</li>
    </ul>
    <p>ä¸ºä½¿åˆ†è§£å¯è¾¨è¯†ï¼ˆidentifiableï¼‰ï¼Œé€šå¸¸ç”¨ä¸‹å¼é‡ç»„ Q å€¼ï¼š</p>
    <p>$$Q(s, a) = V(s) + \Big(A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a')\Big)$$</p>
    <p>å‡å»ä¼˜åŠ¿å‡å€¼èƒ½ä¿è¯ $\sum_a A(s,a)=0$ï¼Œä»è€Œé¿å…ä»»æ„å¸¸æ•°åœ¨ V ä¸ A ä¹‹é—´è½¬ç§»å¯¼è‡´çš„ä¸å¯è¾¨è¯†é—®é¢˜ï¼ˆè§ä¸‹é¢çš„å”¯ä¸€åˆ†è§£è¯æ˜ï¼‰ã€‚</p>

    <h3>2. ç›´è§‰ä¸ä¼˜ç‚¹</h3>
    <ul>
      <li>å½“æŸäº›çŠ¶æ€ä¸‹ä¸åŒåŠ¨ä½œæ”¶ç›Šæ¥è¿‘æ—¶ï¼Œä¸“é—¨å­¦ä¹  $V(s)$ ä¼šæ›´ç¨³å®šï¼›</li>
      <li>$A(s,a)$ èšç„¦åŠ¨ä½œé—´å·®å¼‚ï¼Œèƒ½æ›´å¿«æ•æ‰å¾®å°ç­–ç•¥ä¼˜åŠ¿ï¼›</li>
      <li>å¯ä¸ Double DQNã€PERã€NoisyNet ç­‰æ”¹è¿›æ–¹æ³•ç›´æ¥ç»“åˆï¼Œå¸¸è§äºå¼ºåŒ–å­¦ä¹ ç«æŠ€å¹³å°ï¼ˆAtariã€ProcGen ç­‰ï¼‰ã€‚</li>
    </ul>

    <h3>3. å”¯ä¸€åˆ†è§£ï¼ˆIdentifiabilityï¼‰è¯æ˜</h3>
    <p>é—®é¢˜ï¼šç»™å®š Q(s,a)ï¼Œå°†å…¶å†™æˆ $Q(s,a)=V(s)+A(s,a)$ æ˜¯å¦å”¯ä¸€ï¼Ÿç­”æ¡ˆï¼šåœ¨æ²¡æœ‰çº¦æŸæ—¶ä¸æ˜¯å”¯ä¸€çš„ï¼›è‹¥å¯¹æ¯ä¸ª s æ–½åŠ çº¦æŸ $\sum_a A(s,a)=0$ï¼ˆæˆ–ç­‰ä»·çš„å¸¸æ•°çº¦æŸï¼‰ï¼Œåˆ™åˆ†è§£å”¯ä¸€ã€‚</p>
    <p><strong>è¯æ˜æ­¥éª¤ï¼š</strong></p>
    <p>å‡è®¾å­˜åœ¨ä¸¤ç»„åˆ†è§£ $(V, A)$ ä¸ $(V', A')$ æ»¡è¶³å¯¹æ‰€æœ‰ $(s,a)$ï¼š</p>
    <p>$$Q(s,a)=V(s)+A(s,a)=V'(s)+A'(s,a)$$</p>
    <p>ä»¤ $D(s)=V(s)-V'(s)$ï¼Œ$E(s,a)=A(s,a)-A'(s,a)$ï¼Œåˆ™å¯¹æ‰€æœ‰ $(s,a)$ æœ‰</p>
    <p>$$D(s)+E(s,a)=0 \quad \Rightarrow \quad E(s,a) = -D(s)$$</p>
    <p>å¯¹åŠ¨ä½œé›†åˆæ±‚å’Œå¹¶ä½¿ç”¨çº¦æŸ $\sum_a E(s,a)=0$ï¼š</p>
    <p>$$\sum_a E(s,a) = -\sum_a D(s) = -|\mathcal{A}|\, D(s) = 0 \Rightarrow D(s)=0$$</p>
    <p>äºæ˜¯ $D(s)=0$ï¼Œè¿›è€Œ $E(s,a)=0$ï¼Œå³ $(V,A)$ ä¸ $(V',A')$ åœ¨æ–½åŠ å‡å€¼çº¦æŸä¸‹ç›¸åŒï¼Œåˆ†è§£å”¯ä¸€ã€‚</p>

    <h3>4. ç½‘ç»œæ¶æ„è¯¦è§£</h3>
    <p>Dueling DQN çš„ç½‘ç»œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š</p>
    <ol>
      <li><strong>å…±äº«ç‰¹å¾æå–å±‚</strong>ï¼šå·ç§¯æˆ–å…¨è¿æ¥å±‚ç”¨äºæå–çŠ¶æ€ç‰¹å¾ï¼ˆä½œä¸ºåä¸¤ä¸ªåˆ†æ”¯çš„è¾“å…¥ï¼‰ï¼›</li>
      <li><strong>Value åˆ†æ”¯</strong>ï¼šç‹¬ç«‹çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œæœ€ç»ˆè¾“å‡ºå•ä¸ªæ ‡é‡ $V(s)$ï¼›</li>
      <li><strong>Advantage åˆ†æ”¯</strong>ï¼šç‹¬ç«‹çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œæœ€ç»ˆè¾“å‡º $|\mathcal{A}|$ ç»´å‘é‡ $A(s,a)$ã€‚</li>
    </ol>
    <p>å‰å‘ä¼ æ’­æ—¶ï¼Œå¯¹ Advantage å±‚è¾“å‡ºè¿›è¡Œ<strong>å»å‡å€¼å¤„ç†</strong>åä¸ Value å±‚ç»„åˆï¼š</p>
    <p>$$Q(s,a) = V(s) + \Big(A(s,a) - \frac{1}{|\mathcal{A}|}\sum_a A(s,a)\Big)$$</p>

    <h3>5. åˆå§‹åŒ–ä¸è®­ç»ƒç»†èŠ‚</h3>
    <p>ä¸ºç¡®ä¿ç½‘ç»œç¨³å®šæ€§å’Œå¯è¾¨è¯†æ€§ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„åˆå§‹åŒ–ï¼š</p>
    <ul>
      <li><strong>æƒé‡åˆå§‹åŒ–</strong>ï¼šä½¿ç”¨ Xavier uniform æˆ– Kaiming åˆå§‹åŒ–ï¼›å¯¹æ‰€æœ‰çº¿æ€§å±‚çš„åç½®åˆå§‹åŒ–ä¸º 0ï¼›</li>
      <li><strong>ä¸ºä»€ä¹ˆéœ€è¦ç‰¹æ®Šåˆå§‹åŒ–</strong>ï¼šAdvantage åˆ†æ”¯åˆæœŸåº”è¾“å‡ºæ¥è¿‘ 0 çš„å€¼ï¼Œè¿™æ · $A - \text{mean}(A) \approx 0$ï¼Œç¡®ä¿ Q å€¼åˆæœŸä¸»è¦ç”± $V(s)$ ä¸»å¯¼ï¼Œé¿å…ä¼˜åŠ¿åˆ†æ”¯è¿‡å¤§å¯¼è‡´çš„æ•°å€¼ä¸ç¨³å®šï¼›</li>
      <li><strong>ä¸»è¦ä¼˜åŠ¿</strong>ï¼šå°åˆå§‹ä¼˜åŠ¿ä¿è¯äº†åˆ†è§£çš„å¯è¾¨è¯†æ€§ï¼Œä½¿ V ä¸ A ä¸ä¼šåœ¨å‚æ•°ç©ºé—´ä¸­äº’ç›¸æŠµæ¶ˆã€‚</li>
    </ul>

    <h3>6. å®ç°æ­¥éª¤ï¼ˆé€æ­¥è¯´æ˜ï¼‰</h3>
    <p>ä¸‹é¢æŠŠ Dueling DQN çš„è®­ç»ƒè¿‡ç¨‹æ‹†æˆå¯ç›´æ¥è½åœ°çš„æ­¥éª¤ï¼Œä¾¿äºæŠŠä»£ç æ˜ å°„åˆ°å…·ä½“å®ç°ï¼š</p>
    <ol>
      <li><strong>å®šä¹‰ç½‘ç»œ</strong>ï¼šå…±äº«ä¸»å¹²æå–ç‰¹å¾ï¼›åˆ†å‡ºä¸¤ä¸ªå¤´â€”â€”Value è¾“å‡ºæ ‡é‡ V(s)ï¼ŒAdvantage è¾“å‡ºå‘é‡ A(s,a)ã€‚å‰å‘æ—¶å¯¹ A å»å‡å€¼å†ä¸ V é‡æ„ Qã€‚</li>
      <li><strong>ç»éªŒæ”¶é›†</strong>ï¼šä½¿ç”¨ Îµ-greedy æˆ–å¯å­¦ä¹ å™ªå£°ç­–ç•¥é‡‡æ · transition $(s,a,r,s',\text{done})$ï¼Œå¹¶å­˜å…¥ç»éªŒå›æ”¾æ±  ğ’Ÿã€‚</li>
      <li><strong>å°æ‰¹é‡é‡‡æ ·</strong>ï¼šä» ğ’Ÿ ä¸­éšæœºé‡‡æ · batchï¼›è‹¥ä½¿ç”¨ PER è¯·ç”¨é‡è¦æ€§æƒé‡ä¿®æ­£æŸå¤±ã€‚</li>
      <li><strong>ç›®æ ‡è®¡ç®—</strong>ï¼šç”¨ç›®æ ‡ç½‘ç»œ $\theta^-$ å‰å‘å¾—åˆ° $V'(s')$ ä¸ $A'(s',\cdot)$ï¼Œå¯¹ $A'$ å»å‡å€¼å¹¶é‡æ„ Q'ï¼›ç›®æ ‡ $y = r + \gamma \cdot \max_{a'} Q'(s',a')$ï¼ˆè‹¥ done åˆ™ $y = r$ï¼‰ã€‚</li>
      <li><strong>æŸå¤±ä¸æ›´æ–°</strong>ï¼šè®¡ç®— $L = \text{MSE}(y, Q(s,a;\theta))$ æˆ– Huber Lossï¼Œåå‘ä¼ æ’­å¹¶ç”¨ä¼˜åŒ–å™¨ step æ›´æ–°ä¸»ç½‘ç»œ $\theta$ã€‚</li>
      <li><strong>ç›®æ ‡ç½‘ç»œåŒæ­¥</strong>ï¼šé‡‡ç”¨ç¡¬æ›´æ–°ï¼ˆæ¯ C æ­¥å¤åˆ¶ï¼‰æˆ–è½¯æ›´æ–°ï¼ˆ$\theta^- \leftarrow \tau\theta + (1-\tau)\theta^-$ï¼‰ã€‚</li>
    </ol>

    <h3>7. PyTorch ä»£ç ç¤ºä¾‹</h3>
  <pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import deque

class DuelingNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DuelingNetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        
        # Value åˆ†æ”¯ï¼šè¾“å‡ºå•ä¸ªæ ‡é‡
        self.value_stream = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # Advantage åˆ†æ”¯ï¼šè¾“å‡º action_dim ç»´å‘é‡
        self.advantage_stream = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
        
        # æƒé‡åˆå§‹åŒ–
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        # æå–å…±äº«ç‰¹å¾
        features = self.feature(state)
        
        # Value ä¸ Advantage ç‹¬ç«‹å‰å‘
        v = self.value_stream(features)
        a = self.advantage_stream(features)
        
        # é‡ç»„ Q å€¼ï¼ˆå»å‡å€¼ä¿è¯å¯è¾¨è¯†ï¼‰
        a_mean = a.mean(dim=1, keepdim=True)
        q = v + (a - a_mean)
        
        return q

class DuelingDQN:
    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, epsilon=0.1):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        
        # ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_net = DuelingNetwork(state_dim, action_dim)
        self.target_q_net = DuelingNetwork(state_dim, action_dim)
        self.target_q_net.load_state_dict(self.q_net.state_dict())
        
        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        # ç»éªŒå›æ”¾æ± 
        self.memory = deque(maxlen=10000)
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state):
        """Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            with torch.no_grad():
                q_values = self.q_net(torch.FloatTensor(state).unsqueeze(0))
            return q_values.max(1)[1].item()
    
    def train_batch(self, batch_size):
        """è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡"""
        if len(self.memory) < batch_size:
            return
        
        # éšæœºé‡‡æ ·
        indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        
        states = torch.FloatTensor(np.array([x[0] for x in batch]))
        actions = torch.LongTensor([x[1] for x in batch])
        rewards = torch.FloatTensor([x[2] for x in batch])
        next_states = torch.FloatTensor(np.array([x[3] for x in batch]))
        dones = torch.FloatTensor([x[4] for x in batch])
        
        # å½“å‰ Q å€¼
        q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # ç›®æ ‡ Q å€¼
        with torch.no_grad():
            max_next_q = self.target_q_net(next_states).max(1)[0]
            target_q = rewards + self.gamma * max_next_q * (1 - dones)
        
        # æŸå¤±å’Œåå‘ä¼ æ’­
        loss = self.loss_fn(q_values, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)
        self.optimizer.step()
        
        return loss.item()
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_q_net.load_state_dict(self.q_net.state_dict())

# è®­ç»ƒç¤ºä¾‹
if __name__ == "__main__":
    state_dim = 4
    action_dim = 2
    agent = DuelingDQN(state_dim, action_dim)
    
    # æ¨¡æ‹Ÿç¯å¢ƒäº¤äº’
    for episode in range(100):
        state = np.random.randn(state_dim)
        episode_reward = 0
        
        for step in range(50):
            action = agent.act(state)
            next_state = np.random.randn(state_dim)
            reward = np.random.randn()
            done = step == 49
            
            agent.remember(state, action, reward, next_state, float(done))
            agent.train_batch(batch_size=32)
            
            episode_reward += reward
            state = next_state
        
        if (episode + 1) % 20 == 0:
            agent.update_target_network()
            print(f"Episode {episode+1}, Reward: {episode_reward:.2f}")
  </code></pre>
    <table>
      <tr>
        <th>é—®é¢˜</th>
        <th>è§£ç­”</th>
      </tr>
      <tr>
        <td>ä¸ºä»€ä¹ˆè¦å¯¹ Advantage å»å‡å€¼ï¼Ÿ</td>
        <td>å»å‡å€¼ä¿è¯äº†åˆ†è§£çš„å”¯ä¸€æ€§ã€‚è‹¥ä¸å»å‡å€¼ï¼Œå¯ä»¥åœ¨ V ä¸ A é—´ä»»æ„è½¬ç§»å¸¸æ•°å€¼è€Œäº§ç”Ÿç›¸åŒ Qï¼Œå¯¼è‡´å‚æ•°ä¸å¯è¾¨è¯†ã€‚</td>
      </tr>
      <tr>
        <td>V ä¸ A æ˜¯å¦éœ€è¦åŒæ—¶è®­ç»ƒï¼Ÿ</td>
        <td>æ˜¯çš„ã€‚ä¸¤ä¸ªåˆ†æ”¯å…±äº«ä¸»å¹²ï¼Œåå‘ä¼ æ’­ä¼šåŒæ—¶æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚V å­¦ä¹ çŠ¶æ€ä»·å€¼ï¼ŒA å­¦ä¹ åŠ¨ä½œç›¸å¯¹ä¼˜åŠ¿ï¼Œä¸¤è€…åˆ†å·¥æ˜ç¡®ã€‚</td>
      </tr>
      <tr>
        <td>åˆå§‹åŒ–ä¸ºä»€ä¹ˆå¾ˆé‡è¦ï¼Ÿ</td>
        <td>åˆæœŸ Advantage åº”æ¥è¿‘ 0ï¼Œä½¿ Q â‰ˆ Vï¼Œç¡®ä¿ç½‘ç»œç¨³å®šå¯åŠ¨ã€‚è¿‡å¤§çš„åˆå§‹ A ä¼šå¯¼è‡´æ•°å€¼éœ‡è¡ï¼Œå½±å“æ”¶æ•›ã€‚</td>
      </tr>
      <tr>
        <td>å¦‚ä½•ä¸ Double DQN ç»“åˆï¼Ÿ</td>
        <td>ç›®æ ‡è®¡ç®—æ—¶ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°ä»·å€¼ï¼Œä¸æ ‡å‡† Double DQN ç±»ä¼¼ï¼Œåªæ˜¯ç½‘ç»œç»“æ„ä»å•ä¸ª Q æ”¹ä¸º V+Aã€‚</td>
      </tr>
    </table>

    <h3>9. å®è·µè¦ç‚¹ä¸ç»„åˆç­–ç•¥</h3>
    <ul>
      <li><strong>ä¸ Double DQN ç»“åˆ</strong>ï¼šç›®æ ‡è®¡ç®—ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°ä»·å€¼ï¼Œæ¶ˆé™¤è¿‡ä¼°è®¡åå·®ï¼›</li>
      <li><strong>ä¸ PER ç»“åˆ</strong>ï¼šPrioritized Experience Replay èƒ½æ›´å¿«èšç„¦é«˜ TD è¯¯å·®æ ·æœ¬ï¼Œä¸ Dueling æ¶æ„ç›¸è¾…ç›¸æˆï¼›</li>
      <li><strong>ç¦»æ•£ vs è¿ç»­åŠ¨ä½œ</strong>ï¼šDueling DQN åœ¨ç¦»æ•£åŠ¨ä½œé—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ï¼›è¿ç»­æ§åˆ¶éœ€ä¸ç¡®å®šæ€§ç­–ç•¥ï¼ˆå¦‚ DDPGï¼‰æˆ–ç­–ç•¥åˆ†æ”¯æ”¹é€ ï¼›</li>
      <li><strong>è°ƒå‚å»ºè®®</strong>ï¼šé¿å…ä¼˜åŠ¿åˆ†æ”¯è¾“å‡ºè¿‡å¤§å¯¼è‡´æ•°å€¼ä¸ç¨³ï¼ˆå¯åŠ  L2 æ­£åˆ™æˆ–æ¢¯åº¦è£å‰ªï¼‰ï¼›ç›‘æ§ V ä¸ A çš„å¤§å°æ¯”ä¾‹ã€‚</li>
    </ul>
  </section>

    <section id="chapter15" class="chapter">
      <h2>ç¬¬åäº”ç« ï¼šPER</h2>
      

      <h3>PERï¼šä¼˜å…ˆç»éªŒå›æ”¾</h3>
      
      <p><strong>é—®é¢˜ï¼š</strong>æ™®é€šç»éªŒå›æ”¾æ˜¯å‡åŒ€éšæœºé‡‡æ ·ï¼Œä½†æœ‰äº›æ ·æœ¬ä¿¡æ¯é‡å¤§ï¼ˆæ¯”å¦‚ TD è¯¯å·®å¾ˆå¤§ï¼‰ï¼Œæœ‰äº›æ²¡å•¥ç”¨ã€‚</p>
      
      <p><strong>è§£å†³ï¼š</strong>ç»™æ¯ä¸ªæ ·æœ¬æ‰“åˆ†ï¼ŒTD è¯¯å·®è¶Šå¤§ â†’ ä¼˜å…ˆçº§è¶Šé«˜ â†’ è¶Šå®¹æ˜“è¢«é‡‡æ ·ã€‚</p>

      <h4>é‡‡æ ·æ¦‚ç‡</h4>
      <p>$$P(i) = \frac{(|\delta_i| + \varepsilon)^\alpha}{\sum_k (|\delta_k| + \varepsilon)^\alpha}$$</p>
      
      <table>
        <tr><th>å‚æ•°</th><th>å«ä¹‰</th><th>å¸¸ç”¨å€¼</th></tr>
        <tr><td>Î±</td><td>æ§åˆ¶ä¼˜å…ˆçº§å½±å“åŠ›åº¦</td><td>0.6</td></tr>
        <tr><td>Îµ</td><td>é˜²æ­¢ä¼˜å…ˆçº§ä¸º 0</td><td>1e-6</td></tr>
      </table>

      <h4>é‡è¦æ€§æƒé‡</h4>
      <p>å› ä¸ºæ”¹å˜äº†é‡‡æ ·åˆ†å¸ƒï¼Œéœ€è¦ç”¨æƒé‡ä¿®æ­£ï¼Œé¿å…åå·®ï¼š</p>
      <p>$$w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta$$</p>
      
      <p>Î² ä» 0.4 é€æ¸å‡åˆ° 1.0ï¼Œè®­ç»ƒå‰æœŸå…è®¸æœ‰ç‚¹åå·®ï¼ŒåæœŸä¸¥æ ¼ä¿®æ­£ã€‚</p>

      <p><strong>æœ€ç»ˆæŸå¤±ï¼š</strong></p>
      <p>$$L = \text{mean}(w_i \cdot \delta_i^2)$$</p>

      <h3>ä¸‰ã€æ•´åˆæµç¨‹ï¼šDueling + PER + Double</h3>
      
      <ol>
        <li>åˆå§‹åŒ– online ç½‘ç»œï¼ˆDueling ç»“æ„ï¼‰ã€target ç½‘ç»œã€PER ç¼“å†²åŒº</li>
        <li>ä¸ç¯å¢ƒäº¤äº’ï¼Œå¾—åˆ° (s, a, r, s', done)</li>
        <li>è®¡ç®— TD è¯¯å·®ï¼š
          $$\delta = \big|r + \gamma \cdot Q_{\text{target}}\big(s', \arg\max_{a'} Q_{\text{online}}(s',a')\big) - Q_{\text{online}}(s,a)\big|$$
        </li>
        <li>å­˜å…¥ PERï¼Œä¼˜å…ˆçº§ = Î´ + Îµ</li>
        <li>æŒ‰ä¼˜å…ˆçº§é‡‡æ · batchï¼Œå¾—åˆ°æƒé‡ w</li>
        <li>è®¡ç®—åŠ æƒæŸå¤±ï¼Œåå‘ä¼ æ’­</li>
        <li>æ›´æ–° PER ä¸­å„æ ·æœ¬çš„ä¼˜å…ˆçº§</li>
        <li>å®šæœŸåŒæ­¥ target ç½‘ç»œ</li>
      </ol>

      <p><strong>æ³¨æ„ï¼š</strong>è¿™é‡Œç”¨çš„æ˜¯ Double DQN çš„ç›®æ ‡è®¡ç®—æ–¹å¼ï¼ˆonline é€‰åŠ¨ä½œï¼Œtarget è¯„ä¼°ï¼‰ï¼Œé˜²æ­¢ Q å€¼è¿‡ä¼°è®¡ã€‚</p>

      <h3>å››ã€å®Œæ•´ä»£ç å®ç°</h3>

      <h4>Dueling ç½‘ç»œ</h4>
<pre><code class="language-python">
import torch
import torch.nn as nn

class DuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # å…±äº«ç‰¹å¾å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        # åˆ†æ”¯ 1ï¼šçŠ¶æ€ä»·å€¼
        self.value = nn.Linear(128, 1)
        # åˆ†æ”¯ 2ï¼šåŠ¨ä½œä¼˜åŠ¿
        self.advantage = nn.Linear(128, action_dim)

    def forward(self, x):
        feat = self.feature(x)
        v = self.value(feat)
        a = self.advantage(feat)
        # å»å‡å€¼ç»„åˆ
        q = v + (a - a.mean(dim=1, keepdim=True))
        return q
</code></pre>

      <h4>PER ç¼“å†²åŒºï¼ˆSumTree å®ç°ï¼‰</h4>
<pre><code class="language-python">
import numpy as np
import random

class SumTree:
    """SumTree ç”¨äºå¿«é€Ÿé‡‡æ ·ï¼Œæ—¶é—´å¤æ‚åº¦ O(log N)"""
    def __init__(self, capacity):
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1)
        self.data = [None] * capacity
        self.write = 0
        self.n_entries = 0

    def _propagate(self, idx, change):
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)

    def update(self, idx, priority):
        change = priority - self.tree[idx]
        self.tree[idx] = priority
        self._propagate(idx, change)

    def add(self, priority, data):
        idx = self.write + self.capacity - 1
        self.data[self.write] = data
        self.update(idx, priority)
        self.write = (self.write + 1) % self.capacity
        self.n_entries = min(self.n_entries + 1, self.capacity)

    def get(self, s):
        idx = 0
        while True:
            left = 2 * idx + 1
            if left >= len(self.tree):
                break
            if s <= self.tree[left]:
                idx = left
            else:
                s -= self.tree[left]
                idx = left + 1
        data_idx = idx - self.capacity + 1
        return idx, self.tree[idx], self.data[data_idx]

    @property
    def total(self):
        return self.tree[0]

class PERBuffer:
    def __init__(self, capacity, alpha=0.6, eps=1e-6):
        self.tree = SumTree(capacity)
        self.alpha = alpha
        self.eps = eps

    def add(self, td_error, sample):
        priority = (abs(td_error) + self.eps) ** self.alpha
        self.tree.add(priority, sample)

    def sample(self, batch_size, beta=0.4):
        batch, idxs, priorities = [], [], []
        segment = self.tree.total / batch_size
        
        for i in range(batch_size):
            a, b = segment * i, segment * (i + 1)
            s = random.uniform(a, b)
            idx, p, data = self.tree.get(s)
            batch.append(data)
            idxs.append(idx)
            priorities.append(p)
        
        # è®¡ç®—é‡è¦æ€§æƒé‡
        probs = np.array(priorities) / self.tree.total
        weights = (self.tree.n_entries * probs) ** (-beta)
        weights /= weights.max()  # å½’ä¸€åŒ–
        
        return batch, idxs, torch.FloatTensor(weights)

    def update_priority(self, idx, td_error):
        priority = (abs(td_error) + self.eps) ** self.alpha
        self.tree.update(idx, priority)
</code></pre>

      <h4>è®­ç»ƒæµç¨‹</h4>
<pre><code class="language-python">
import torch.optim as optim
import torch.nn.functional as F

# åˆå§‹åŒ–
online_net = DuelingDQN(state_dim=4, action_dim=2)
target_net = DuelingDQN(state_dim=4, action_dim=2)
target_net.load_state_dict(online_net.state_dict())
optimizer = optim.Adam(online_net.parameters(), lr=1e-4)

buffer = PERBuffer(capacity=10000, alpha=0.6)
gamma = 0.99
beta = 0.4

for episode in range(1000):
    state = env.reset()
    done = False
    
    while not done:
        # Îµ-greedy é€‰åŠ¨ä½œ
        if random.random() < epsilon:
            action = random.randint(0, action_dim-1)
        else:
            with torch.no_grad():
                q = online_net(torch.FloatTensor(state).unsqueeze(0))
                action = q.argmax().item()
        
        next_state, reward, done, _ = env.step(action)
        
        # è®¡ç®— TD è¯¯å·®
        with torch.no_grad():
            q_next = target_net(torch.FloatTensor(next_state).unsqueeze(0))
            best_action = online_net(torch.FloatTensor(next_state).unsqueeze(0)).argmax()
            td_target = reward + (1 - done) * gamma * q_next[0, best_action]
            q_current = online_net(torch.FloatTensor(state).unsqueeze(0))[0, action]
            td_error = abs(td_target - q_current).item()
        
        # å­˜å…¥ PER
        buffer.add(td_error, (state, action, reward, next_state, done))
        state = next_state
        
        # è®­ç»ƒ
        if buffer.tree.n_entries > 64:
            batch, idxs, weights = buffer.sample(32, beta)
            
            states = torch.FloatTensor([x[0] for x in batch])
            actions = torch.LongTensor([x[1] for x in batch])
            rewards = torch.FloatTensor([x[2] for x in batch])
            next_states = torch.FloatTensor([x[3] for x in batch])
            dones = torch.FloatTensor([x[4] for x in batch])
            
            # Double DQN ç›®æ ‡
            q_eval = online_net(states).gather(1, actions.unsqueeze(1)).squeeze()
            with torch.no_grad():
                best_actions = online_net(next_states).argmax(1)
                q_next = target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze()
                q_target = rewards + (1 - dones) * gamma * q_next
            
            # åŠ æƒæŸå¤±
            td_errors = (q_target - q_eval).abs()
            loss = (weights * F.mse_loss(q_eval, q_target, reduction='none')).mean()
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # æ›´æ–°ä¼˜å…ˆçº§
            for idx, err in zip(idxs, td_errors.detach().numpy()):
                buffer.update_priority(idx, err)
    
    # å®šæœŸåŒæ­¥ target ç½‘ç»œ
    if episode % 10 == 0:
        target_net.load_state_dict(online_net.state_dict())
</code></pre>

      <h3>äº”ã€æ•ˆæœå¯¹æ¯”</h3>
      <ul>
        <li><strong>Dueling</strong>ï¼šç½‘ç»œæ›´é«˜æ•ˆï¼Œå°¤å…¶åœ¨åŠ¨ä½œå½±å“å°çš„çŠ¶æ€</li>
        <li><strong>PER</strong>ï¼šæ”¶æ•›æ›´å¿«ï¼Œæ ·æœ¬åˆ©ç”¨ç‡æ›´é«˜</li>
        <li><strong>Double</strong>ï¼šé˜²æ­¢ Q å€¼è¿‡ä¼°è®¡</li>
      </ul>

      <p>ä¸‰è€…ç»„åˆæ˜¯ Rainbow DQN çš„æ ¸å¿ƒï¼Œåœ¨ Atari æ¸¸æˆä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>

      <p><strong>æ”¹è¿›è·¯çº¿ï¼š</strong>DQN â†’ Double DQN â†’ Dueling DQN â†’ PER â†’ Rainbow DQN</p>
    </section>

    <section id="chapter16" class="chapter">
      <h2>ç¬¬åå…­ç« ï¼šNoisyNet DQNï¼ˆè‡ªé€‚åº”æ¢ç´¢ï¼‰</h2>

      <p>å‰é¢é€šè¿‡ Dueling å’Œ PER æ”¹è¿›äº†ç½‘ç»œç»“æ„å’Œé‡‡æ ·æ•ˆç‡ï¼Œä½†æ¢ç´¢é—®é¢˜è¿˜æ²¡è§£å†³å¥½ã€‚</p>

      <p>ä¼ ç»Ÿçš„ <strong>Îµ-greedy</strong> æ¢ç´¢é äººå·¥è®¾å®š Îµï¼Œå®ƒçš„éšæœºæ€§è·Ÿç½‘ç»œå­¦ä¹ æ²¡ä»€ä¹ˆå…³ç³»ï¼Œå¾ˆéš¾è‡ªé€‚åº”è°ƒæ•´æ¢ç´¢å¼ºåº¦ã€‚<br>
      <strong>NoisyNet DQN</strong>ï¼ˆFortunato et al., 2017ï¼‰æ¢äº†ä¸ªæ€è·¯ â€”â€” ç›´æ¥åœ¨ç½‘ç»œå‚æ•°é‡ŒåŠ å¯å­¦ä¹ çš„å™ªå£°ï¼Œè®©æ¢ç´¢å˜æˆç½‘ç»œå­¦ä¹ çš„ä¸€éƒ¨åˆ†ã€‚</p>

      <h3>ä¸€ã€æ ¸å¿ƒæƒ³æ³•</h3>
      <p>ç”¨å¸¦å™ªå£°çš„çº¿æ€§å±‚æ›¿ä»£æ™®é€šå…¨è¿æ¥å±‚ï¼š</p>

      <pre><code class="language-python">
y = (W + Ïƒ_W âŠ™ Îµ_W) Â· x + (b + Ïƒ_b âŠ™ Îµ_b)
      </code></pre>

      <ul>
        <li><code>W, b</code>ï¼šæ ‡å‡†æƒé‡å’Œåç½®</li>
        <li><code>Ïƒ_W, Ïƒ_b</code>ï¼šå¯å­¦ä¹ çš„å™ªå£°å¼ºåº¦ï¼ˆç½‘ç»œè®­ç»ƒæ—¶ä¼šè°ƒæ•´ï¼‰</li>
        <li><code>Îµ_W, Îµ_b</code>ï¼šæ¯æ¬¡å‰å‘ä¼ æ’­é‡æ–°é‡‡æ ·çš„é«˜æ–¯å™ªå£°</li>
      </ul>

      <p>è¿™æ ·ç½‘ç»œè¾“å‡ºè‡ªå¸¦éšæœºæ€§ï¼Œå™ªå£°å¤§å°éšè®­ç»ƒè‡ªåŠ¨è°ƒæ•´ã€‚ä¸ç”¨æ‰‹åŠ¨è°ƒ Îµ äº†ï¼Œæ¢ç´¢å¼ºåº¦ç½‘ç»œè‡ªå·±å­¦ã€‚</p>

      <h3>äºŒã€ä»£ç å®ç°</h3>
<pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class NoisyLinear(nn.Module):
    def __init__(self, in_features, out_features, std_init=0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        # å¯å­¦ä¹ å‚æ•°
        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.register_buffer("weight_eps", torch.empty(out_features, in_features))

        self.bias_mu = nn.Parameter(torch.empty(out_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))
        self.register_buffer("bias_eps", torch.empty(out_features))

        self.reset_parameters()
        self.sample_noise()

    def reset_parameters(self):
        mu_range = 1 / math.sqrt(self.in_features)
        self.weight_mu.data.uniform_(-mu_range, mu_range)
        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))
        self.bias_mu.data.uniform_(-mu_range, mu_range)
        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))

    def sample_noise(self):
        """é‡æ–°é‡‡æ ·å™ªå£°"""
        self.weight_eps.normal_()
        self.bias_eps.normal_()

    def forward(self, x):
        if self.training:
            # è®­ç»ƒæ—¶åŠ å™ªå£°
            w = self.weight_mu + self.weight_sigma * self.weight_eps
            b = self.bias_mu + self.bias_sigma * self.bias_eps
        else:
            # æµ‹è¯•æ—¶ä¸åŠ å™ªå£°
            w, b = self.weight_mu, self.bias_mu
        return F.linear(x, w, b)
</code></pre>

      <h3>ä¸‰ã€ä¸ Dueling DQN ç»“åˆ</h3>
<pre><code class="language-python">
class NoisyDuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # å…±äº«ç‰¹å¾å±‚ï¼ˆæ™®é€šå±‚ï¼‰
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        # ä»·å€¼æµå’Œä¼˜åŠ¿æµç”¨ NoisyLinear
        self.value = NoisyLinear(128, 1)
        self.advantage = NoisyLinear(128, action_dim)

    def forward(self, x):
        feat = self.feature(x)
        v = self.value(feat)
        a = self.advantage(feat)
        q = v + (a - a.mean(dim=1, keepdim=True))
        return q
    
    def sample_noise(self):
        """é‡é‡‡æ ·æ‰€æœ‰å™ªå£°å±‚"""
        self.value.sample_noise()
        self.advantage.sample_noise()
</code></pre>

      <h3>å››ã€è®­ç»ƒæµç¨‹</h3>
      <ol>
        <li>æŠŠ Dueling DQN çš„ <code>Linear</code> å±‚æ¢æˆ <code>NoisyLinear</code></li>
        <li><strong>å»æ‰ Îµ-greedy</strong>ï¼Œæ¢ç´¢å®Œå…¨é å™ªå£°é©±åŠ¨</li>
        <li>æ¯æ¬¡å‰å‘ä¼ æ’­å‰è°ƒç”¨ <code>sample_noise()</code> é‡æ–°é‡‡æ ·å™ªå£°</li>
        <li>æŸå¤±å‡½æ•°ã€TD ç›®æ ‡ã€target æ›´æ–°éƒ½è·Ÿ DQN ä¸€æ ·</li>
      </ol>

<pre><code class="language-python">
# è®­ç»ƒæ—¶æ— éœ€ Îµ-greedy
for episode in range(1000):
    state = env.reset()
    online_net.sample_noise()  # æ¯ä¸ª episode å¼€å§‹é‡é‡‡æ ·å™ªå£°
    
    while not done:
        # ç›´æ¥é€‰æœ€ä¼˜åŠ¨ä½œï¼Œå™ªå£°å·²ç»åœ¨ç½‘ç»œé‡Œäº†
        with torch.no_grad():
            q = online_net(torch.FloatTensor(state).unsqueeze(0))
            action = q.argmax().item()
        
        next_state, reward, done, _ = env.step(action)
        buffer.add((state, action, reward, next_state, done))
        state = next_state
        
        # è®­ç»ƒæ­¥éª¤
        if len(buffer) > batch_size:
            batch = buffer.sample(batch_size)
            online_net.sample_noise()  # è®­ç»ƒå‰é‡é‡‡æ ·
            target_net.sample_noise()
            
            # è®¡ç®—æŸå¤±å¹¶æ›´æ–°...
</code></pre>

      <h3>äº”ã€ä¼˜ç‚¹</h3>
      <ul>
        <li><strong>è‡ªé€‚åº”æ¢ç´¢</strong>ï¼šç½‘ç»œåœ¨ä¸ç¡®å®šçš„çŠ¶æ€è‡ªåŠ¨å¢å¤§å™ªå£°</li>
        <li><strong>å‡å°‘è¶…å‚</strong>ï¼šä¸ç”¨æ‰‹åŠ¨è°ƒ Îµ å’Œè¡°å‡ç­–ç•¥äº†</li>
        <li><strong>æ€§èƒ½æ›´å¥½</strong>ï¼šåœ¨ Atari å’Œ MuJoCo ä¸Šæ™®éä¼˜äº Îµ-greedy</li>
      </ul>

      <h3>å…­ã€å…¸å‹ç»„åˆ</h3>
      <p>NoisyNet ç»å¸¸å’Œè¿™äº›ä¸€èµ·ç”¨ï¼š</p>
      <ul>
        <li><strong>Dueling DQN</strong> â†’ æå‡ç‰¹å¾è¡¨è¾¾</li>
        <li><strong>Double DQN</strong> â†’ å‡å°‘è¿‡ä¼°è®¡</li>
        <li><strong>PER</strong> â†’ åŠ é€Ÿæ ·æœ¬åˆ©ç”¨</li>
      </ul>
      <p>ç»„åˆèµ·æ¥å°±æ˜¯ Rainbow DQN çš„ç¬¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚</p>

      <h3>ä¸ƒã€å°ç»“</h3>
      <p>NoisyNet æŠŠæ¢ç´¢æœºåˆ¶èå…¥ç½‘ç»œå‚æ•°ï¼Œè®©æ™ºèƒ½ä½“è‡ªå·±è°ƒèŠ‚æ¢ç´¢å¼ºåº¦ã€‚<br>
      ç›¸æ¯”ä¼ ç»Ÿ Îµ-greedyï¼Œè¿™æ‰æ˜¯çœŸæ­£çš„"å¯å­¦ä¹ æ¢ç´¢"ï¼Œåœ¨å¤æ‚ç¯å¢ƒé‡Œæ›´é«˜æ•ˆã€æ›´æ™ºèƒ½ã€‚</p>

      <p><strong>æ”¹è¿›è·¯çº¿ï¼š</strong>DQN â†’ Double DQN â†’ Dueling DQN â†’ PER â†’ <strong>NoisyNet DQN</strong> â†’ Rainbow DQN</p>
    </section>

    <section id="chapter17" class="chapter">
      <h2>ç¬¬åä¸ƒç« ï¼šRainbow DQNï¼ˆé›†å¤§æˆè€…ï¼‰</h2>

      <h3>é¢„å¤‡çŸ¥è¯†ï¼šN-Step Learningï¼ˆå¤šæ­¥æ—¶åºå·®åˆ†ï¼‰</h3>

      <p><strong>æ ¸å¿ƒåŠ¨æœºï¼š</strong> ç›¸æ¯” 1-step TD åªçœ‹ä¸‹ä¸€æ­¥å¥–åŠ±ï¼ŒN-step å°†æœªæ¥ N æ­¥çš„çœŸå®å¥–åŠ±ä¸€æ¬¡æ€§çº³å…¥ç›®æ ‡ï¼Œè®©å¥–åŠ±ä¿¡å·æ›´å¿«åœ°å›ä¼ åˆ°å½“å‰çŠ¶æ€ï¼Œä»è€ŒåŠ é€Ÿå­¦ä¹ ã€æé«˜æ ·æœ¬åˆ©ç”¨ç‡ã€‚</p>

      <h4>1. ç†è®ºèƒŒæ™¯ï¼šä»‹äº TD ä¸ Monte Carlo ä¹‹é—´çš„æŠ˜ä¸­</h4>
      <p>ä¼ ç»Ÿæ—¶åºå·®åˆ†ï¼ˆTDï¼‰æ¯æ­¥æ›´æ–°ä¸€æ¬¡ï¼Œåªåˆ©ç”¨ä¸‹ä¸€æ­¥å¥–åŠ±ï¼š</p>
      <pre><code>y_t^{(1)} = r_t + Î³Â·max_a Q_target(s_{t+1}, a)
</code></pre>
      <p>è€Œ Monte Carlo æ–¹æ³•ç­‰æ•´å±€ç»“æŸåç”¨å®Œæ•´å›æŠ¥æ›´æ–°ï¼š</p>
      <pre><code>y_t^{(MC)} = r_t + Î³r_{t+1} + Î³^2r_{t+2} + ... + Î³^{T-t}r_T
</code></pre>

      <p>äºŒè€…ç‰¹ç‚¹å¦‚ä¸‹ï¼š</p>
      <ul>
        <li><strong>TDï¼š</strong> æ›´æ–°å¿«ã€ä½æ–¹å·®ï¼Œä½†ä»…çœ‹ä¸€æ­¥ï¼Œä¼ æ’­æ…¢ã€‚</li>
        <li><strong>Monte Carloï¼š</strong> åˆ©ç”¨å®Œæ•´ä¿¡æ¯ã€æ— åï¼Œä½†éœ€ç­‰æ•´å±€ç»“æŸã€æ–¹å·®å¤§ã€‚</li>
      </ul>

      <p><strong>N-step Learning</strong> ä»‹äºä¸¤è€…ä¹‹é—´ï¼Œé€šè¿‡å›ºå®šé•¿åº¦ N çš„æ»šåŠ¨çª—å£ï¼Œå°†æœªæ¥ N æ­¥çš„çœŸå®å¥–åŠ±çº³å…¥æ›´æ–°ç›®æ ‡ï¼š</p>
      <pre><code>y_t^{(N)} = r_t + Î³r_{t+1} + ... + Î³^{N-1}r_{t+N-1} + Î³^NÂ·max_a Q_target(s_{t+N}, a)
</code></pre>

      <p>è¿™æ ·å¯ä»¥åŒæ—¶ä¿ç•™ TD çš„åœ¨çº¿æ›´æ–°èƒ½åŠ›ï¼Œåˆå¼•å…¥ Monte Carlo çš„é•¿è¿œè§†è§’ï¼Œå…¼é¡¾ç¨³å®šæ€§ä¸é«˜æ•ˆæ€§ã€‚</p>

      <h4>2. å®ç°é€»è¾‘ï¼ˆæ»‘åŠ¨çª—å£æœºåˆ¶ï¼‰</h4>
      <ol>
        <li>ç»´æŠ¤é•¿åº¦ä¸º N çš„é˜Ÿåˆ— <code>n_step_buffer</code>ï¼Œæ¯æ¬¡ç¯å¢ƒäº¤äº’åå…¥é˜Ÿæ ·æœ¬ <code>(s, a, r, s_next, done)</code>ã€‚</li>
        <li>å½“é˜Ÿåˆ—æ»¡ N æ—¶ï¼Œè®¡ç®—ç´¯è®¡æŠ˜æ‰£å¥–åŠ±ï¼š
        <pre><code>R = Î£_{i=0}^{N-1} Î³^i Â· r_{t+i}</code></pre></li>
        <li>å–é˜Ÿé¦–çŠ¶æ€ <code>(s_t, a_t)</code> å’Œé˜Ÿå°¾ä¸‹ä¸€ä¸ªçŠ¶æ€ <code>s_{t+N}</code> æ„é€  N-step æ ·æœ¬ï¼š
        <pre><code>(s_t, a_t, R, s_{t+N}, done)</code></pre></li>
        <li>å­˜å…¥ç»éªŒå›æ”¾æ± ï¼Œè®­ç»ƒæ—¶ä»¥ç›®æ ‡ï¼š
        <pre><code>y_t^{(N)} = R + (Î³^N)Â·max_a Q_target(s_{t+N}, a)</code></pre></li>
      </ol>

      <h4>3. å‚æ•°ä¸æ³¨æ„äº‹é¡¹</h4>
      <ul>
        <li>N é€šå¸¸å– <strong>3~5</strong>ï¼ˆRainbow é»˜è®¤ N=3ï¼‰ã€‚</li>
        <li>è‹¥å›åˆæå‰ç»“æŸï¼ˆä¸è¶³ N æ­¥ï¼‰ï¼Œç›´æ¥ç”¨å·²è§‚æµ‹åˆ°çš„å¥–åŠ±ç´¯åŠ ã€‚</li>
        <li>ä¸ PERã€NoisyNetã€Dueling å¯æ— ç¼ç»“åˆï¼Œå¢å¼ºè®­ç»ƒæ•ˆç‡ä¸ç¨³å®šæ€§ã€‚</li>
      </ul>

      <hr>

      <h3>é¢„å¤‡çŸ¥è¯†ï¼šC51ï¼ˆCategorical DQNï¼Œåˆ†å¸ƒå¼ä»·å€¼å­¦ä¹ ï¼‰</h3>

      <p><strong>æ ¸å¿ƒåŠ¨æœºï¼š</strong> DQN åªå­¦ä¹ å›æŠ¥çš„æœŸæœ› Q(s,a)ï¼Œè€Œ C51 ç›´æ¥å­¦ä¹ å›æŠ¥åˆ†å¸ƒ Z(s,a)ï¼Œæ•æ‰é£é™©ä¸ä¸ç¡®å®šæ€§ï¼Œä½¿æ™ºèƒ½ä½“çš„ç­–ç•¥æ›´ç¨³å¥ã€æ›´å…·è¡¨è¾¾åŠ›ã€‚</p>

      <h4>1. å›ºå®šæ”¯æŒåŒºé—´ä¸ 51 ä¸ªç¦»æ•£åŸå­ï¼ˆatomsï¼‰</h4>
      <ul>
        <li>C51 å°†æœªæ¥å›æŠ¥çš„èŒƒå›´å›ºå®šåœ¨ <code>[V_min, V_max]</code> ä¸Šï¼Œå¹¶å‡åŒ€åˆ’åˆ†ä¸º 51 ä¸ªå–å€¼ç‚¹ï¼š</li>
        <pre><code>z_i = V_min + iÂ·(V_max - V_min)/50 , i = 0..50</code></pre>
        <li>æ¯ä¸ªåŠ¨ä½œ a è¾“å‡º 51 ä¸ªæ¦‚ç‡ <code>p_i(s,a)</code>ï¼Œè¡¨ç¤ºå›æŠ¥è½åœ¨ z_i å¤„çš„æ¦‚ç‡ã€‚</li>
      </ul>

      <h4>2. V<sub>min</sub> / V<sub>max</sub> çš„é€‰å–ä¾æ®</h4>
      <ul>
        <li>ç†è®ºä¸Šï¼šè‹¥æ¯æ­¥å¥–åŠ±èŒƒå›´ä¸º <code>[r_min, r_max]</code>ï¼ŒæŠ˜æ‰£å› å­ä¸º Î³ï¼Œåˆ™æœªæ¥ç´¯è®¡å›æŠ¥çš„ä¸Šä¸‹é™ä¸ºï¼š
        <pre><code>V_min = r_min / (1 - Î³)
V_max = r_max / (1 - Î³)</code></pre></li>
        <li>å®è·µä¸­å¯æ ¹æ®ä»»åŠ¡ç»éªŒé€‰å–æ›´ç´§åŒºé—´ã€‚ä¾‹å¦‚ï¼š
          <ul>
            <li>Atari ç¯å¢ƒï¼šé€šå¸¸è®¾ä¸º [-10, 10] æˆ– [-100, 100]</li>
            <li>è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼ˆå¦‚ MuJoCoï¼‰ï¼šæ ¹æ®å¥–åŠ±å°ºåº¦é€‚å½“ç¼©æ”¾</li>
          </ul>
        </li>
        <li>å›ºå®š <code>[V_min, V_max]</code> ä½¿ç½‘ç»œè¾“å‡ºç»´åº¦ç¨³å®šï¼Œä¾¿äºåˆ†å¸ƒæŠ•å½±ã€‚</li>
      </ul>

      <h4>3. ç½‘ç»œè¾“å‡ºä¸åŠ¨ä½œé€‰æ‹©</h4>
      <ul>
        <li>è¾“å‡ºï¼šæ¯ä¸ªåŠ¨ä½œå¯¹åº” 51 ç»´æ¦‚ç‡åˆ†å¸ƒï¼Œç» softmax å½’ä¸€åŒ–ã€‚</li>
        <li>è®­ç»ƒï¼šä¼˜åŒ–æ•´æ¡åˆ†å¸ƒï¼›æ‰§è¡ŒåŠ¨ä½œæ—¶å–æœŸæœ›ï¼š
        <pre><code>Q(s,a) = Î£_i p_i(s,a)Â·z_i</code></pre></li>
      </ul>

      <h4>4. åˆ†å¸ƒå¼ Bellman æ›´æ–°ä¸æŠ•å½±</h4>
      <ol>
        <li>ç”¨ Double DQN æ€æƒ³é€‰æ‹©ä¸‹ä¸€åŠ¨ä½œï¼š
        <pre><code>a* = argmax_a Î£_i p_i(s',a)Â·z_i</code></pre></li>
        <li>æ„é€ ç›®æ ‡åˆ†å¸ƒï¼š
        <pre><code>TZ = r + Î³Â·Z_target(s', a*)</code></pre></li>
        <li>å°†å…¶çº¿æ€§æŠ•å½±å›å›ºå®šæ”¯æŒåŒºé—´ï¼Œå¾—åˆ°ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒ m_iã€‚</li>
      </ol>

      <h4>5. æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µï¼ˆCross-Entropyï¼‰</h4>
      <p>è®­ç»ƒç›®æ ‡ï¼šè®©é¢„æµ‹åˆ†å¸ƒ <code>p</code> é€¼è¿‘ç›®æ ‡åˆ†å¸ƒ <code>m</code>ã€‚</p>
      <pre><code>L = - Î£_i m_i Â· log p_i</code></pre>

      <h5>äº¤å‰ç†µæœ€å°å€¼è¯æ˜ï¼ˆåŸºäº Jensen ä¸ç­‰å¼ï¼‰</h5>
      <p>å®šä¹‰äº¤å‰ç†µï¼š<code>H(p,q) = -Î£_i p_i ln q_i</code>ï¼Œè¯æ˜å½“ p=q æ—¶ H(p,q) æœ€å°ã€‚</p>
      <pre><code>H(p,q) - H(p,p)
= Î£_i p_i ln(q_i/p_i)

å›  ln(x) ä¸ºå‡¹å‡½æ•°ï¼ŒJensen ä¸ç­‰å¼ï¼š
Î£_i p_i ln(x_i) â‰¤ ln(Î£_i p_i x_i)ï¼Œå…¶ä¸­ Î£_i p_i=1

å– x_i = q_i / p_iï¼š
Î£_i p_i ln(q_i/p_i) â‰¤ ln(Î£_i q_i) = ln(1) = 0

â‡’ H(p,p) - H(p,q) â‰¤ 0
â‡’ H(p,p) â‰¤ H(p,q)

å½“ p=q æ—¶ç­‰å·æˆç«‹ã€‚
</code></pre>
      <p>å› æ­¤ï¼Œé¢„æµ‹åˆ†å¸ƒç­‰äºç›®æ ‡åˆ†å¸ƒæ—¶ï¼Œäº¤å‰ç†µå–å¾—æœ€å°å€¼ï¼Œè¿™å°±æ˜¯åˆ†å¸ƒå­¦ä¹ ç¨³å®šçš„ç†è®ºåŸºç¡€ã€‚</p>

      <h4>6. å°ç»“</h4>
      <ul>
        <li><strong>N-Stepï¼š</strong> æ˜¯ TD ä¸ Monte Carlo çš„ä¸­é—´å½¢æ€ï¼Œç»“åˆçŸ­æœŸç¨³å®šä¸é•¿æœŸè§†é‡ï¼ŒåŠ å¿«å¥–åŠ±ä¼ æ’­ã€‚</li>
        <li><strong>C51ï¼š</strong> ç›´æ¥å­¦ä¹ å›æŠ¥åˆ†å¸ƒï¼ˆ51 ä¸ªæ¦‚ç‡ï¼‰ï¼Œé€šè¿‡äº¤å‰ç†µä¼˜åŒ–é¢„æµ‹åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚</li>
        <li><strong>V<sub>min</sub>/V<sub>max</sub>ï¼š</strong> è¡¨ç¤ºç¯å¢ƒå¯èƒ½å›æŠ¥çš„å…¨å±€ä¸Šä¸‹ç•Œï¼Œé€šå¸¸è®¾ä¸º <code>[r_min/(1-Î³), r_max/(1-Î³)]</code> æˆ–ç»éªŒèŒƒå›´ã€‚</li>
        <li>ä¸¤è€…å‡ä¸º Rainbow DQN çš„å…³é”®ç»„æˆæ¨¡å—ï¼Œä¸ Doubleã€Duelingã€PERã€NoisyNet å…±åŒç»„æˆå®Œæ•´å¼ºåŒ–å­¦ä¹ å½©è™¹ä½“ç³»ã€‚</li>
      </ul>

      <hr>

      <h3>å¼ºåŒ–å­¦ä¹ è¿›é˜¶ï¼šRainbow DQN ğŸŒˆï¼ˆç»ˆæ DQN æ”¹è¿›ç‰ˆï¼‰</h3>

      <p><strong>è®ºæ–‡ï¼š</strong> Hessel et al., "<em>Rainbow: Combining Improvements in Deep Reinforcement Learning</em>", DeepMind, 2018</p>

      <p><strong>æ ¸å¿ƒæ€æƒ³ï¼š</strong> Rainbow å°†å…­å¤§å¼ºåŒ–å­¦ä¹ æ”¹è¿›æ¨¡å—æ•´åˆå…¥ DQN æ¡†æ¶ï¼Œä½¿æ™ºèƒ½ä½“åœ¨ç¨³å®šæ€§ã€æ ·æœ¬æ•ˆç‡ã€æ¢ç´¢èƒ½åŠ›ä¸è¡¨è¾¾èƒ½åŠ›ä¸Šå…¨é¢æå‡ã€‚</p>

      <h4>1. Rainbow çš„å…­å¤§ç»„æˆæ¨¡å—</h4>

      <table>
        <tr><th>æ¨¡å—</th><th>æ”¹è¿›æ–¹å‘</th><th>ä¸»è¦ä½œç”¨</th></tr>
        <tr><td><strong>Double DQN</strong></td><td>ç›®æ ‡è®¡ç®—</td><td>å‡å°‘ Q å€¼è¿‡ä¼°è®¡åå·®</td></tr>
        <tr><td><strong>Dueling DQN</strong></td><td>ç½‘ç»œç»“æ„</td><td>åˆ†ç¦»çŠ¶æ€ä»·å€¼ä¸åŠ¨ä½œä¼˜åŠ¿</td></tr>
        <tr><td><strong>PERï¼ˆPrioritized Experience Replayï¼‰</strong></td><td>æ•°æ®é‡‡æ ·</td><td>æé«˜å…³é”®æ ·æœ¬é‡‡æ ·é¢‘ç‡</td></tr>
        <tr><td><strong>NoisyNet</strong></td><td>æ¢ç´¢ç­–ç•¥</td><td>åœ¨å‚æ•°ä¸­æ³¨å…¥å¯å­¦ä¹ å™ªå£°ï¼Œæ›¿ä»£ Îµ-greedy</td></tr>
        <tr><td><strong>N-Step Learning</strong></td><td>å­¦ä¹ ä¿¡å·</td><td>åŠ é€Ÿå¥–åŠ±ä¼ æ’­ï¼Œå…¼é¡¾ TD ä¸ Monte Carlo</td></tr>
        <tr><td><strong>C51</strong></td><td>ä»·å€¼è¡¨ç¤º</td><td>é¢„æµ‹å›æŠ¥åˆ†å¸ƒè€Œéå•ä¸€æœŸæœ›å€¼</td></tr>
      </table>

      <p>è¿™å…­ä¸ªæ¨¡å—æ„æˆäº† DQN çš„"å½©è™¹"å¢å¼ºä½“ç³»ã€‚</p>

      <hr>

      <h4>2. ç½‘ç»œç»“æ„ï¼ˆDueling + NoisyNet + C51ï¼‰</h4>

      <ul>
        <li><strong>Dueling ç»“æ„ï¼š</strong> å…±äº«ç‰¹å¾æå–å±‚ååˆ†ä¸ºä¸¤æ”¯ï¼š
        <pre><code>Q(s,a) = V(s) + [A(s,a) - mean(A(s,Â·))]</code></pre>
        ä½¿ç½‘ç»œèƒ½åŒºåˆ†"çŠ¶æ€æœ¬èº«ä»·å€¼"ä¸"åŠ¨ä½œå¸¦æ¥çš„å¢ç›Š"ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚</li>

        <li><strong>NoisyNetï¼š</strong> å°†å…¨è¿æ¥å±‚æ›¿æ¢ä¸ºå¸¦å™ªå£°çº¿æ€§å±‚ï¼š
        <pre><code>W' = W + Ïƒ_W âŠ™ Îµ_W,   b' = b + Ïƒ_b âŠ™ Îµ_b</code></pre>
        æ¯æ¬¡å‰å‘ä¼ æ’­å‰é‡‡æ ·éšæœºå™ªå£° Îµï¼ŒÏƒ ä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œå½¢æˆå¯å­¦ä¹ æ¢ç´¢æœºåˆ¶ã€‚</li>

        <li><strong>C51 è¾“å‡ºå±‚ï¼š</strong> æ¯ä¸ªåŠ¨ä½œè¾“å‡º 51 ä¸ª logitsï¼Œç» softmax å¾—åˆ°åˆ†å¸ƒæ¦‚ç‡ï¼š
        <pre><code>p_i(s,a) = softmax(z_i)</code></pre>
        æœ€ç»ˆ Q å€¼ä¸ºæœŸæœ›ï¼š
        <pre><code>Q(s,a) = Î£_i p_i(s,a) Â· z_i</code></pre></li>
      </ul>

      <hr>

      <h4>3. æ•°æ®ä¸ç›®æ ‡æ„å»ºï¼ˆN-Step + Double + C51 Projectionï¼‰</h4>

      <h5>3.1 N-Step æ»‘åŠ¨çª—å£æœºåˆ¶</h5>
      <p>ç»´æŠ¤é•¿åº¦ä¸º N çš„é˜Ÿåˆ— <code>n_step_buffer</code>ï¼Œæ¯æ¬¡äº¤äº’åå­˜å…¥æ ·æœ¬ï¼š</p>
      <pre><code>(s_t, a_t, r_t, s_{t+1}, done_t)</code></pre>
      <p>å½“é˜Ÿåˆ—æ»¡ N æ­¥æ—¶ï¼Œè®¡ç®—ç´¯è®¡æŠ˜æ‰£å¥–åŠ±ï¼š</p>
      <pre><code>R = Î£_{i=0}^{N-1} Î³^i Â· r_{t+i}</code></pre>
      <p>å¹¶ç”Ÿæˆ N-Step æ ·æœ¬ <code>(s_t, a_t, R, s_{t+N}, done_{t+N})</code> å­˜å…¥å›æ”¾æ± ã€‚</p>

      <h5>3.2 Double DQN ç›®æ ‡åŠ¨ä½œé€‰æ‹©</h5>
      <ul>
        <li>åœ¨çº¿ç½‘ç»œï¼ˆQ<sub>online</sub>ï¼‰é€‰åŠ¨ä½œï¼š  
        <pre><code>a* = argmax_a Î£_i p_i(s',a)Â·z_i</code></pre></li>
        <li>ç›®æ ‡ç½‘ç»œï¼ˆQ<sub>target</sub>ï¼‰ç”¨æ¥è¯„ä¼°åˆ†å¸ƒï¼š  
        <pre><code>Z_target = Z_target(s', a*)</code></pre></li>
      </ul>

      <h5>3.3 C51 åˆ†å¸ƒå¼ Bellman æŠ•å½±</h5>
      <p>å¯¹ç›®æ ‡åˆ†å¸ƒçš„æ¯ä¸ªåŸå­ z<sub>j</sub> è®¡ç®—ä»¿å°„å¹³ç§»ï¼š</p>
      <pre><code>t_z = clip(r_acc + Î³^N Â· z_j, V_min, V_max)</code></pre>
      <p>å°†å…¶æŠ•å½±å›å›ºå®šæ”¯æŒç‚¹é›†åˆ <code>{z_i}</code> ä¸Šï¼ŒæŒ‰çº¿æ€§æƒé‡åˆ†æ‘Šæ¦‚ç‡è´¨é‡ï¼Œå¾—åˆ°ç›®æ ‡åˆ†å¸ƒ mã€‚</p>

      <hr>

      <h4>4. æŸå¤±å‡½æ•°ä¸ PER ä¼˜å…ˆé‡‡æ ·</h4>

      <h5>4.1 äº¤å‰ç†µä¸ KL æ•£åº¦çš„å…³ç³»</h5>

      <p>ç»™å®šç›®æ ‡åˆ†å¸ƒ <code>p</code>ï¼ˆçœŸå®ï¼‰ä¸é¢„æµ‹åˆ†å¸ƒ <code>q</code>ï¼ˆæ¨¡å‹è¾“å‡ºï¼‰ï¼Œæœ‰ï¼š</p>
      <pre><code>D_KL(p||q) = Î£ p(x) log(p(x)/q(x))
H(p,q) = -Î£ p(x) log q(x)
</code></pre>
      <p>å±•å¼€åå¯å¾—ï¼š</p>
      <pre><code>H(p,q) = H(p) + D_KL(p||q)</code></pre>
      <p>ç”±äº H(p) æ˜¯å¸¸æ•°ï¼Œæœ€å°åŒ–äº¤å‰ç†µ <code>H(p,q)</code> ç­‰ä»·äºæœ€å°åŒ– KL æ•£åº¦ <code>D_KL(p||q)</code>ã€‚</p>
      <p>å› æ­¤ï¼ŒC51 ä½¿ç”¨çš„äº¤å‰ç†µæŸå¤±ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯æœ€å°åŒ–ç›®æ ‡åˆ†å¸ƒä¸é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„ KL å·®å¼‚ã€‚</p>

      <h5>4.2 ä¸‰ç§å¸¸è§æŸå¤±å‡½æ•°è®¾è®¡æ–¹å¼</h5>

      <table>
        <tr><th>ç±»å‹</th><th>å®šä¹‰</th><th>è¯´æ˜</th></tr>
        <tr>
          <td><strong>(1) åˆ†å¸ƒäº¤å‰ç†µæŸå¤±</strong></td>
          <td><code>L = -Î£_i m_i Â· log p_i</code></td>
          <td>æœ€å¸¸ç”¨ï¼Œä¸ C51 åŸè®ºæ–‡ä¸€è‡´ï¼›ç›´æ¥åº¦é‡åˆ†å¸ƒå·®å¼‚ï¼Œç­‰ä»·äºæœ€å°åŒ– KL æ•£åº¦ã€‚</td>
        </tr>
        <tr>
          <td><strong>(2) æœŸæœ› TD è¯¯å·®æŸå¤±</strong></td>
          <td><code>L = (R + Î³^NÂ·Q_target - Q_online)^2</code></td>
          <td>å°†åˆ†å¸ƒå–æœŸæœ›åå›é€€åˆ°ä¼ ç»Ÿ TD å½¢å¼ï¼Œè®¡ç®—æ›´ç›´è§‚ï¼Œä½†ä¸¢å¤±åˆ†å¸ƒä¿¡æ¯ã€‚</td>
        </tr>
        <tr>
          <td><strong>(3) æ··åˆæŸå¤±</strong></td>
          <td><code>L = Î»Â·L_CE + (1âˆ’Î»)Â·L_TD</code></td>
          <td>èåˆäº¤å‰ç†µä¸ TD è¯¯å·®ä¿¡å·ï¼Œå…¼é¡¾åˆ†å¸ƒæ‹Ÿåˆä¸ç¨³å®šæ€§ï¼ˆÎ»â‰ˆ0.5 å¸¸ç”¨ï¼‰ã€‚</td>
        </tr>
      </table>

      <p>ä¸€èˆ¬æ¨èä½¿ç”¨åˆ†å¸ƒäº¤å‰ç†µæŸå¤±ï¼ˆæ–¹æ¡ˆ 1ï¼‰ï¼Œä¸ C51 ç†è®ºå®Œå…¨ä¸€è‡´ï¼›åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œä¸ºå¢å¼ºç¨³å®šæ€§å¯ä½¿ç”¨æ··åˆæŸå¤±ã€‚</p>

      <h5>4.3 PER ä¼˜å…ˆçº§è®¡ç®—ï¼ˆåˆ†å¸ƒå¼ç‰ˆæœ¬ï¼‰</h5>
      <ul>
        <li><strong>æ–¹æ³• 1ï¼š</strong> ä½¿ç”¨æ ·æœ¬äº¤å‰ç†µ <code>CE_i = -Î£ m_i log p_i</code> ä½œä¸ºä¼˜å…ˆçº§ã€‚</li>
        <li><strong>æ–¹æ³• 2ï¼š</strong> å–åˆ†å¸ƒæœŸæœ›è®¡ç®— TD è¯¯å·® <code>|Î´| = |y - Q|</code>ã€‚</li>
        <li><strong>æ–¹æ³• 3ï¼š</strong> æ··åˆä¼˜å…ˆçº§ <code>prio_i = Î»Â·CE_i + (1âˆ’Î»)Â·|Î´|</code>ã€‚</li>
      </ul>
      <p>é‡‡æ ·æ¦‚ç‡ä¸é‡è¦æ€§æƒé‡å¦‚ä¸‹ï¼š</p>
      <pre><code>P(i) = prio_i^Î± / Î£_j prio_j^Î±
w_i = (NÂ·P(i))^-Î²
</code></pre>
      <p>å…¶ä¸­ Î±â‰ˆ0.6 æ§åˆ¶é‡‡æ ·åå¥½å¼ºåº¦ï¼ŒÎ² ä» 0.4 çº¿æ€§ä¸Šå‡è‡³ 1.0 ç”¨äºæ ¡æ­£é‡‡æ ·åå·®ã€‚</p>

      <hr>

      <h4>5. ç›®æ ‡ç½‘ç»œæ›´æ–°ä¸å™ªå£°åˆ·æ–°</h4>
      <ul>
        <li>Target ç½‘ç»œå‚æ•°æ¯ K æ­¥"ç¡¬æ›´æ–°"ï¼š<code>Î¸^- â† Î¸</code>ï¼ˆä¾‹å¦‚æ¯ 2000 æ­¥ï¼‰ã€‚</li>
        <li>æˆ–é‡‡ç”¨è½¯æ›´æ–°ï¼š<code>Î¸^- â† Ï„Î¸ + (1âˆ’Ï„)Î¸^âˆ’</code>ï¼ŒÏ„â‰ˆ1eâˆ’3ã€‚</li>
        <li>æ¯æ¬¡å‰å‘ä¼ æ’­å‰ <code>sample_noise()</code>ï¼Œä»¥åˆ·æ–°æ¢ç´¢å™ªå£°ã€‚</li>
      </ul>

      <hr>

      <h4>6. è®­ç»ƒä¸»æµç¨‹ï¼ˆä¼ªä»£ç ï¼‰</h4>

<pre><code class="language-python"># Rainbow DQN ä¸»å¾ªç¯

Initialize Q_online, Q_target â† Q_online
Initialize prioritized replay buffer

for each episode:
    s â† env.reset()
    n_step_buffer â† deque(maxlen=N)

    while not done:
        # 1. ä½¿ç”¨ NoisyNet é€‰æ‹©åŠ¨ä½œ
        a â† argmax_a E[Z(s,a;Î¸)]
        s_next, r, done â† env.step(a)
        n_step_buffer.append((s, a, r, s_next, done))

        # 2. è‹¥ n_step_buffer æ»¡ N æ­¥ï¼Œç”Ÿæˆ N-Step æ ·æœ¬
        if len(n_step_buffer) == N:
            R â† Î£ Î³^iÂ·r_{t+i}
            store_transition(s, a, R, s_{t+N}, done)

        s â† s_next

        # 3. æ¯æ­¥è®­ç»ƒ
        batch â† replay_buffer.sample(B)
        compute target distribution m
        compute loss L = -Î£ m_iÂ·log p_i
        update Î¸ with weighted loss
        update priorities with per-sample loss
        periodically update Q_target
</code></pre>

      <hr>

      <h4>7. è¶…å‚æ•°å‚è€ƒ</h4>

      <table>
        <tr><th>å‚æ•°</th><th>å…¸å‹å€¼</th><th>è¯´æ˜</th></tr>
        <tr><td>æŠ˜æ‰£ Î³</td><td>0.99</td><td>é•¿æœŸå¥–åŠ±æŠ˜æ‰£</td></tr>
        <tr><td>N-Step</td><td>3</td><td>å¥–åŠ±ä¼ æ’­æ­¥æ•°</td></tr>
        <tr><td>Î±</td><td>0.6</td><td>PER é‡‡æ ·åå¥½å¼ºåº¦</td></tr>
        <tr><td>Î²</td><td>0.4 â†’ 1.0</td><td>é‡è¦æ€§æƒé‡ä¿®æ­£</td></tr>
        <tr><td>V_min, V_max</td><td>[-10, 10] æˆ– [-100, 100]</td><td>C51 æ”¯æŒåŒºé—´</td></tr>
        <tr><td>å­¦ä¹ ç‡</td><td>1eâˆ’4</td><td>Adam ä¼˜åŒ–å™¨</td></tr>
        <tr><td>æ‰¹é‡å¤§å°</td><td>32 / 64</td><td>è®­ç»ƒæ‰¹æ¬¡å¤§å°</td></tr>
      </table>

      <hr>

      <h4>8. æ€»ç»“</h4>

      <ul>
        <li><strong>Rainbow</strong> æ˜¯ DQN çš„å…¨é¢æ•´åˆç‰ˆæœ¬ï¼Œèåˆå…­å¤§æ¨¡å—ï¼š</li>
        <pre><code>Rainbow = Dueling + NoisyNet + PER + N-Step + Double + C51</code></pre>
        <li>åœ¨çº¿ç½‘ç»œé€‰åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°åˆ†å¸ƒï¼›N-Step å¥–åŠ±ä¼ æ’­æ›´å¿«ï¼›äº¤å‰ç†µæŸå¤±æœ€å°åŒ–é¢„æµ‹ä¸ç›®æ ‡åˆ†å¸ƒçš„ KL æ•£åº¦å·®è·ã€‚</li>
        <li>ç»“åˆæ‰€æœ‰æ”¹è¿›åï¼ŒRainbow åœ¨ Atari ç³»åˆ—ç¯å¢ƒä¸­å¤§å¹…è¶…è¶ŠåŸå§‹ DQN ä¸å„ç‹¬ç«‹å˜ä½“ï¼Œæˆä¸ºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é‡è¦åŸºçº¿ç®—æ³•ã€‚</li>
      </ul>
    </section>

    <section id="chapter18" class="chapter">
      <h2>ç¬¬åå…«ç« ï¼šDDPGï¼ˆDeep Deterministic Policy Gradientï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter19" class="chapter">
      <h2>ç¬¬åä¹ç« ï¼šTD3ï¼ˆTwin Delayed DDPGï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter20" class="chapter">
      <h2>ç¬¬äºŒåç« ï¼šTRPOï¼ˆTrust Region Policy Optimizationï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter21" class="chapter">
      <h2>ç¬¬äºŒåä¸€ç« ï¼šPPOï¼ˆProximal Policy Optimizationï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter22" class="chapter">
      <h2>ç¬¬äºŒåäºŒç« ï¼šSACï¼ˆSoft Actor-Criticï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter23" class="chapter">
      <h2>ç¬¬äºŒåä¸‰ç« ï¼šç­–ç•¥æ¢¯åº¦å¢å¼ºç»„åˆ</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter_summary" class="chapter">
      <h2>è¿›é˜¶æ€»ç»“ä¸å­¦ä¹ å»ºè®®</h2>
      <h3>1. èƒ½åŠ›åœ°å›¾</h3>
      <table>
        <tr><th>èƒ½åŠ›</th><th>ç›¸å…³ç®—æ³•</th><th>æå‡è·¯å¾„</th></tr>
        <tr><td>æ ·æœ¬æ•ˆç‡</td><td>PERã€Rainbowã€SAC</td><td>ä¼˜å…ˆé‡‡æ · â†’ å¤šæ­¥ç›®æ ‡ â†’ æœ€å¤§ç†µ</td></tr>
        <tr><td>ç¨³å®šæ€§</td><td>Duelingã€TD3ã€PPO</td><td>æ¶æ„æ”¹è¿› â†’ åŒç½‘ç»œ â†’ å‰ªåˆ‡æŸå¤±</td></tr>
        <tr><td>è¿ç»­æ§åˆ¶</td><td>DDPGã€TD3ã€SAC</td><td>ç¡®å®šæ€§ç­–ç•¥ â†’ åŒ Critic â†’ æ¸©åº¦è‡ªé€‚åº”</td></tr>
      </table>

      <h3>2. å®æˆ˜è·¯çº¿</h3>
      <ol>
        <li>å…ˆåœ¨ <code>MuJoCo</code> ç­‰ç»å…¸ç¯å¢ƒå¤ç° PPO/SACï¼›</li>
        <li>å¼•å…¥ TD3 æˆ– Rainbow å¯¹æ¯”æŒ‡æ ‡ï¼›</li>
        <li>åœ¨è‡ªå®šä¹‰ç¯å¢ƒä¸­è°ƒå‚ï¼Œè§‚å¯Ÿå™ªå£°ã€ç†µç›®æ ‡ã€å­¦ä¹ ç‡å½±å“ã€‚</li>
      </ol>

      <h3>3. æ¨èé˜…è¯»</h3>
      <ul>
        <li>"Rainbow: Combining Improvements in Deep Reinforcement Learning"</li>
        <li>"Continuous Control with Deep Reinforcement Learning" (DDPG)</li>
        <li>"Addressing Function Approximation Error in Actor-Critic Methods" (TD3)</li>
        <li>"Proximal Policy Optimization Algorithms" (PPO)</li>
        <li>"Soft Actor-Critic" ç³»åˆ—è®ºæ–‡</li>
      </ul>
    </section>
  </main>
</div>

<footer>
  <p>Â© 2025 <span>Haoyun Tang</span> | Reinforcement Learning Advanced Notes</p>
</footer>

<script>
document.addEventListener('DOMContentLoaded', () => {
  document.querySelectorAll('pre[class*="language-"]').forEach(pre => {
    if (!pre.classList.contains('line-numbers')) {
      pre.classList.add('line-numbers');
    }
  });
  if (window.Prism) {
    Prism.highlightAll();
  }
});

window.addEventListener('load', () => {
  if (window.Prism) {
    Prism.highlightAll();
  }
});

const navLinks = document.querySelectorAll('nav a');
const asideLinks = document.querySelectorAll('aside a');
const combined = [...navLinks, ...asideLinks];

window.addEventListener('scroll', () => {
  const offset = window.scrollY + 160;
  combined.forEach(link => {
    const section = document.querySelector(link.getAttribute('href'));
    if (!section) return;
    if (section.offsetTop <= offset && section.offsetTop + section.offsetHeight > offset) {
      link.classList.add('active');
    } else {
      link.classList.remove('active');
    }
  });
});
</script>
</body>
</html>