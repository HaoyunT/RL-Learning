<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>å¼ºåŒ–å­¦ä¹ è¿›é˜¶ç®—æ³• | Yun</title>
<!-- æ•°å­¦å…¬å¼æ¸²æŸ“ -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Prism.js æ ·å¼ä¸è„šæœ¬ -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-coy.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-javascript.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-bash.min.js"></script>
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>

<style>
body {
  margin: 0;
  font-family: "Segoe UI", "Roboto", -apple-system, sans-serif;
  background: linear-gradient(135deg, #121620 0%, #1b1f2b 40%, #221814 100%);
  color: #f5f5f5;
  line-height: 1.8;
  scroll-behavior: smooth;
}

header {
  position: sticky;
  top: 0;
  z-index: 100;
  text-align: center;
  background: rgba(20, 18, 24, 0.92);
  padding: 28px 20px 18px;
  backdrop-filter: blur(10px);
  border-bottom: 1px solid rgba(255, 140, 66, 0.35);
  box-shadow: 0 6px 22px rgba(0, 0, 0, 0.5);
}

header h1 {
  font-size: 2.3rem;
  color: #ff9740;
  margin: 0;
  font-weight: 700;
  letter-spacing: 1.5px;
}

nav ul {
  list-style: none;
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  gap: 10px;
  padding: 0;
  margin: 18px 0 0;
}

nav a {
  color: #f0dccc;
  text-decoration: none;
  font-weight: 600;
  font-size: 0.95rem;
  padding: 6px 16px;
  border-radius: 6px;
  transition: all 0.3s ease;
  background: rgba(255, 140, 66, 0.1);
  border: 1px solid rgba(255, 140, 66, 0.2);
}

nav a:hover, nav a.active {
  color: #ff9740;
  background: rgba(255, 140, 66, 0.2);
  border-color: rgba(255, 140, 66, 0.5);
  box-shadow: 0 0 14px rgba(255, 140, 66, 0.35);
}

.container {
  display: flex;
  position: relative;
  z-index: 5;
  max-width: 1400px;
  margin: 30px auto;
  gap: 30px;
  padding: 0 20px 40px;
}

aside {
  position: sticky;
  top: 130px;
  width: 260px;
  height: fit-content;
  background: rgba(28, 24, 32, 0.88);
  border: 1px solid rgba(255, 140, 66, 0.25);
  border-radius: 12px;
  padding: 22px;
  backdrop-filter: blur(12px);
  flex-shrink: 0;
}

aside h3 {
  color: #ff9740;
  font-size: 1.15rem;
  margin: 0 0 16px 0;
  padding-bottom: 10px;
  border-bottom: 2px solid rgba(255, 140, 66, 0.35);
}

aside ul {
  list-style: none;
  margin: 0;
  padding: 0;
  display: grid;
  gap: 6px;
}

aside a {
  color: #f0dccc;
  text-decoration: none;
  font-size: 0.9rem;
  padding: 8px 12px;
  border-radius: 6px;
  transition: all 0.2s ease;
  border-left: 3px solid transparent;
  background: transparent;
}

aside a:hover, aside a.active {
  color: #ff9740;
  background: rgba(255, 140, 66, 0.12);
  border-left-color: #ff9740;
  padding-left: 16px;
}

main {
  flex: 1;
  min-width: 0;
}

.chapter {
  background: rgba(28, 24, 32, 0.82);
  border: 1px solid rgba(255, 140, 66, 0.2);
  border-radius: 14px;
  padding: 36px;
  margin-bottom: 35px;
  transition: all 0.3s ease;
  box-shadow: 0 10px 30px rgba(0, 0, 0, 0.45);
}

.chapter:hover {
  border-color: rgba(255, 140, 66, 0.4);
  transform: translateY(-3px);
  box-shadow: 0 16px 38px rgba(0, 0, 0, 0.55);
}

.chapter h2 {
  color: #ff8c42;
  font-size: 1.85rem;
  margin: 0 0 24px 0;
  padding-bottom: 12px;
  border-bottom: 2px solid rgba(255, 140, 66, 0.3);
  font-weight: 700;
}

.chapter h3 {
  color: #ffb061;
  font-size: 1.25rem;
  margin-top: 26px;
  border-left: 4px solid rgba(255, 176, 97, 0.6);
  padding-left: 12px;
}

.chapter p, .chapter li {
  color: #f2e6dc;
}

.chapter strong {
  color: #ffb061;
}

.chapter code {
  background: rgba(255, 140, 66, 0.15);
  color: #ffd9b3;
  padding: 3px 6px;
  border-radius: 4px;
}

/* è§£å†³è¡Œå·é®æŒ¡ä»£ç é—®é¢˜ */
pre[class*="language-"] {
  background: linear-gradient(135deg, #1a0d1f 0%, #2d1b3d 100%) !important;
  padding: 20px !important;
  padding-left: 3.2em !important; /* ç»™ä»£ç æ­£æ–‡è®©å‡ºç©ºé—´ */
  border-radius: 10px !important;
  overflow-x: auto !important;
  font-size: 0.92rem !important;
  line-height: 1.6 !important;
  border: 1px solid rgba(156, 39, 176, 0.4) !important;
  position: relative;
  font-family: 'Fira Code', 'Courier New', 'Consolas', 'Monaco', monospace !important;
  box-shadow: 0 4px 20px rgba(156, 39, 176, 0.2) !important;
  text-rendering: optimizeLegibility !important;
  -webkit-font-smoothing: antialiased !important;
  -moz-osx-font-smoothing: grayscale !important;
}

/* ä»£ç è¡Œå· */
pre.line-numbers {
  padding-left: 60px !important;
}

.line-numbers .line-numbers-rows {
  left: 0 !important;
  width: 3em !important;
  background: transparent !important; /* å»æ‰èƒŒæ™¯é®ç½© */
  border-right: 1px solid #444 !important; /* åˆ†å‰²çº¿ */
}

.line-numbers-rows > span:before {
  color: rgba(186, 104, 200, 0.8) !important;
  font-weight: bold;
}

pre code {
  color: inherit !important;
  background: none !important;
  font-family: inherit !important;
  font-size: inherit !important;
  text-rendering: optimizeLegibility !important;
  -webkit-font-smoothing: antialiased !important;
  font-feature-settings: "liga" 0 !important;
}

/* ä»£ç å—å†…å„å…ƒç´ é«˜äº® */
.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
  color: #9c9c9c !important;
  font-style: italic;
}

.token.punctuation {
  color: #f8f8f2 !important;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
  color: #ff79c6 !important;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
  color: #50fa7b !important;
}

.token.operator,
.token.entity,
.token.url {
  color: #8be9fd !important;
}

.token.atrule,
.token.attr-value,
.token.keyword {
  color: #bd93f9 !important;
}

.token.function,
.token.class-name {
  color: #ffb86c !important;
}

.token.regex,
.token.important,
.token.variable {
  color: #f1fa8c !important;
}

/* ç¡®ä¿æ‰€æœ‰ä»£ç æ–‡æœ¬éƒ½æœ‰åŸºç¡€é¢œè‰² */
pre[class*="language-"] code,
pre[class*="language-"] {
  color: #f8f8f2 !important;
}

/* ä¿®å¤ç­‰å·å’Œç¬¦å·çš„æ¸²æŸ“é—®é¢˜ */
pre[class*="language-"] .token.operator,
pre[class*="language-"] .token.punctuation {
  background: none !important;
  text-shadow: none !important;
  font-weight: normal !important;
}

/* é˜²æ­¢å­—ä½“è¿å­—å½±å“ç¬¦å·æ˜¾ç¤º */
pre[class*="language-"] code {
  font-variant-ligatures: none !important;
  font-feature-settings: "liga" 0, "clig" 0 !important;
}

/* ä»£ç å—é¼ æ ‡æ‚¬åœæ•ˆæœ */
pre[class*="language-"]:hover {
  box-shadow: 
    0 12px 48px rgba(156, 39, 176, 0.3),
    inset 0 1px 0 rgba(255,255,255,0.1) !important;
  transform: translateY(-2px);
  transition: all 0.3s ease;
}

/* æ»šåŠ¨æ¡ç¾åŒ– */
pre[class*="language-"]::-webkit-scrollbar {
  height: 10px;
  background: rgba(156, 39, 176, 0.1);
}

pre[class*="language-"]::-webkit-scrollbar-thumb {
  background: rgba(156, 39, 176, 0.5);
  border-radius: 5px;
  transition: background 0.3s;
}

pre[class*="language-"]::-webkit-scrollbar-thumb:hover {
  background: rgba(156, 39, 176, 0.8);
}

table {
  width: 100%;
  border-collapse: collapse;
  margin: 18px 0;
  background: rgba(255, 140, 66, 0.06);
  border-radius: 10px;
  overflow: hidden;
  box-shadow: 0 6px 20px rgba(0, 0, 0, 0.35);
}

table th, table td {
  padding: 12px 16px;
  border-bottom: 1px solid rgba(255, 140, 66, 0.2);
  color: #f7ede2;
}

table th {
  background: rgba(255, 140, 66, 0.22);
  color: #fff1e6;
  font-size: 1.05rem;
}

table tr:last-child td {
  border-bottom: none;
}

/* é˜²æ­¢é®æŒ¡çš„é˜´å½±æˆ–æ¨¡ç³Šæ•ˆæœ */
pre[class*="language-"]::before,
pre[class*="language-"]::after {
  box-shadow: none !important;
  background: none !important;
}

footer {
  text-align: center;
  padding: 28px 20px;
  background: rgba(20, 18, 24, 0.9);
  border-top: 1px solid rgba(255, 140, 66, 0.3);
  color: #f0dccc;
  margin-top: 60px;
  font-size: 0.9rem;
}

footer span {
  color: #ff9740;
  font-weight: 600;
}

@media (max-width: 1024px) {
  .container {
    flex-direction: column;
  }
  aside {
    position: relative;
    top: auto;
    width: 100%;
  }
}

@media (max-width: 768px) {
  header h1 {
    font-size: 2rem;
  }
  nav a {
    font-size: 0.85rem;
    padding: 4px 12px;
  }
  .container {
    padding: 0 12px 30px;
  }
  .chapter {
    padding: 24px;
  }
}
/* æ•°å­¦å…¬å¼ï¼ˆç¬¬16-19ç« ï¼‰æ˜¾ç¤ºï¼šæ¢å¤ä¸ºâ€œæ­£å¸¸â€MathJaxï¼Œæ— èƒŒæ™¯/è¾¹æ¡†ï¼Œä»…ä¿ç•™é€‚åº¦é—´è· */
#chapter16 mjx-container[display="true"],
#chapter17 mjx-container[display="true"],
#chapter18 mjx-container[display="true"],
#chapter19 mjx-container[display="true"] {
  display: block;
  margin: 1.1em auto;       /* ä»…ç•™ç™½ï¼Œä¾¿äºé˜…è¯» */
  padding: 0;               /* æ— é¢å¤–å†…è¾¹è· */
  background: none !important;
  border: none !important;
  border-radius: 0;
  box-shadow: none;
  max-width: 100%;
}

/* è¡Œå†…å…¬å¼ï¼šä¿æŒé»˜è®¤ï¼Œä¸åšè‰²å—/è¾¹æ¡†ä¿®é¥° */
#chapter16 mjx-container:not([display="true"]),
#chapter17 mjx-container:not([display="true"]),
#chapter18 mjx-container:not([display="true"]),
#chapter19 mjx-container:not([display="true"]) {
  vertical-align: baseline;
  padding: 0;
  background: none;
}

/* equation è¾…åŠ©ç±»ï¼šä¸­æ€§æ ·å¼ï¼ˆæ— åº•è‰²/è¾¹æ¡†ï¼‰ */
.equation {
  margin: 1.1em auto;
  padding: 0;
  text-align: center;
  background: none;
  border: none;
  border-radius: 0;
  box-shadow: none;
  max-width: 100%;
}
.equation mjx-container { margin: 0 !important; }
</style>
</head>
<body>
<header>
  <h1>å¼ºåŒ–å­¦ä¹ ç¬”è®°ï¼ˆè¿›é˜¶ç¯‡ï¼‰</h1>
  <nav>
    <ul>
      <li><a href="index.html">é¦–é¡µ</a></li>
      <li><a href="home.html">åŸºç¡€ç†è®º</a></li>
      <li><a href="advanced.html" class="active">è¿›é˜¶ç®—æ³•</a></li>
      <li><a href="MARL.html">å¤šæ™ºèƒ½ä½“å­¦ä¹ </a></li>
      <li><a href="papers.html">è®ºæ–‡ç ”è¯»</a></li>
    </ul>
  </nav>
</header>

<div class="container">
  <aside>
    <h3>ğŸ”¥ è¿›é˜¶ç›®å½•</h3>
    <ul>
      <li><a href="#chapter14">ç¬¬åå››ç« ï¼šDueling DQN</a></li>
      <li><a href="#chapter15">ç¬¬åäº”ç« ï¼šPER</a></li>
      <li><a href="#chapter16">ç¬¬åå…­ç« ï¼šNoisyNet DQN</a></li>
      <li><a href="#chapter17">ç¬¬åä¸ƒç« ï¼šRainbow DQN</a></li>
      <li><a href="#chapter18">ç¬¬åå…«ç« ï¼šDPG & DDPG</a></li>
      <li><a href="#chapter19">ç¬¬åä¹ç« ï¼šTD3</a></li>
      <li><a href="#chapter20">ç¬¬äºŒåç« ï¼šTRPO</a></li>
      <li><a href="#chapter21">ç¬¬äºŒåä¸€ç« ï¼šPPO</a></li>
      <li><a href="#chapter22">ç¬¬äºŒåäºŒç« ï¼šSAC</a></li>
      <li><a href="#chapter23">ç¬¬äºŒåä¸‰ç« ï¼šç­–ç•¥æ¢¯åº¦å¢å¼ºç»„åˆ</a></li>
      <li><a href="#chapter_summary">è¿›é˜¶æ€»ç»“</a></li>
    </ul>
  </aside>

  <main>
  <section id="chapter14" class="chapter">
    <h2>ç¬¬åå››ç« ï¼šDueling DQNï¼ˆåŒæµæ¶æ„ï¼‰</h2>
    <p>Dueling DQN å°† Q ç½‘ç»œæ‹†åˆ†ä¸º<strong>çŠ¶æ€ä»·å€¼åˆ†æ”¯ V(s)</strong>ä¸<strong>åŠ¨ä½œä¼˜åŠ¿åˆ†æ”¯ A(s,a)</strong>ï¼Œç”¨ä¸¤æ¡å¹¶è¡Œçš„å­ç½‘ç»œåˆ†åˆ«ä¼°è®¡çŠ¶æ€çš„æ•´ä½“ä»·å€¼ä¸åŠ¨ä½œçš„ç›¸å¯¹ä¼˜åŠ¿ï¼Œå†å°†äºŒè€…ç»„åˆå¾—åˆ° Q å€¼ã€‚è¿™ç§è®¾è®¡åœ¨ä¸€äº›çŠ¶æ€ä¸‹åŠ¨ä½œå·®å¼‚è¾ƒå°ï¼ˆæˆ–åŠ¨ä½œæ— å…³ï¼‰çš„åœºæ™¯ä¸‹èƒ½æ˜¾è‘—æå‡ä¼°è®¡ç¨³å®šæ€§ä¸æ”¶æ•›é€Ÿåº¦ã€‚</p>

    <h3>1. æ ¸å¿ƒç»“æ„ä¸æ•°å­¦å½¢å¼</h3>
    <p>ä¸»å¹²ç½‘ç»œï¼ˆå·ç§¯æˆ–å‰é¦ˆï¼‰æå–å…±äº«ç‰¹å¾ååˆ†ä¸ºä¸¤æ¡åˆ†æ”¯ï¼š</p>
    <ul>
      <li><strong>Value Streamï¼š</strong>è¾“å‡ºæ ‡é‡ $V(s)$ï¼Œè¡¨ç¤ºçŠ¶æ€æœ¬èº«çš„ä»·å€¼ï¼›</li>
      <li><strong>Advantage Streamï¼š</strong>è¾“å‡ºå‘é‡ $A(s,a)$ï¼Œè¡¨ç¤ºåœ¨çŠ¶æ€ $s$ ä¸‹å„åŠ¨ä½œçš„ç›¸å¯¹ä¼˜åŠ¿ã€‚</li>
    </ul>
    <p>ä¸ºä½¿åˆ†è§£å¯è¾¨è¯†ï¼ˆidentifiableï¼‰ï¼Œé€šå¸¸ç”¨ä¸‹å¼é‡ç»„ Q å€¼ï¼š</p>
    <p>$$Q(s, a) = V(s) + \Big(A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a')\Big)$$</p>
    <p>å‡å»ä¼˜åŠ¿å‡å€¼èƒ½ä¿è¯ $\sum_a A(s,a)=0$ï¼Œä»è€Œé¿å…ä»»æ„å¸¸æ•°åœ¨ V ä¸ A ä¹‹é—´è½¬ç§»å¯¼è‡´çš„ä¸å¯è¾¨è¯†é—®é¢˜ï¼ˆè§ä¸‹é¢çš„å”¯ä¸€åˆ†è§£è¯æ˜ï¼‰ã€‚</p>

    <h3>2. ç›´è§‰ä¸ä¼˜ç‚¹</h3>
    <ul>
      <li>å½“æŸäº›çŠ¶æ€ä¸‹ä¸åŒåŠ¨ä½œæ”¶ç›Šæ¥è¿‘æ—¶ï¼Œä¸“é—¨å­¦ä¹  $V(s)$ ä¼šæ›´ç¨³å®šï¼›</li>
      <li>$A(s,a)$ èšç„¦åŠ¨ä½œé—´å·®å¼‚ï¼Œèƒ½æ›´å¿«æ•æ‰å¾®å°ç­–ç•¥ä¼˜åŠ¿ï¼›</li>
      <li>å¯ä¸ Double DQNã€PERã€NoisyNet ç­‰æ”¹è¿›æ–¹æ³•ç›´æ¥ç»“åˆï¼Œå¸¸è§äºå¼ºåŒ–å­¦ä¹ ç«æŠ€å¹³å°ï¼ˆAtariã€ProcGen ç­‰ï¼‰ã€‚</li>
    </ul>

    <h3>3. å”¯ä¸€åˆ†è§£ï¼ˆIdentifiabilityï¼‰è¯æ˜</h3>
    <p>é—®é¢˜ï¼šç»™å®š Q(s,a)ï¼Œå°†å…¶å†™æˆ $Q(s,a)=V(s)+A(s,a)$ æ˜¯å¦å”¯ä¸€ï¼Ÿç­”æ¡ˆï¼šåœ¨æ²¡æœ‰çº¦æŸæ—¶ä¸æ˜¯å”¯ä¸€çš„ï¼›è‹¥å¯¹æ¯ä¸ª s æ–½åŠ çº¦æŸ $\sum_a A(s,a)=0$ï¼ˆæˆ–ç­‰ä»·çš„å¸¸æ•°çº¦æŸï¼‰ï¼Œåˆ™åˆ†è§£å”¯ä¸€ã€‚</p>
    <p><strong>è¯æ˜æ­¥éª¤ï¼š</strong></p>
    <p>å‡è®¾å­˜åœ¨ä¸¤ç»„åˆ†è§£ $(V, A)$ ä¸ $(V', A')$ æ»¡è¶³å¯¹æ‰€æœ‰ $(s,a)$ï¼š</p>
    <p>$$Q(s,a)=V(s)+A(s,a)=V'(s)+A'(s,a)$$</p>
    <p>ä»¤ $D(s)=V(s)-V'(s)$ï¼Œ$E(s,a)=A(s,a)-A'(s,a)$ï¼Œåˆ™å¯¹æ‰€æœ‰ $(s,a)$ æœ‰</p>
    <p>$$D(s)+E(s,a)=0 \quad \Rightarrow \quad E(s,a) = -D(s)$$</p>
    <p>å¯¹åŠ¨ä½œé›†åˆæ±‚å’Œå¹¶ä½¿ç”¨çº¦æŸ $\sum_a E(s,a)=0$ï¼š</p>
    <p>$$\sum_a E(s,a) = -\sum_a D(s) = -|\mathcal{A}|\, D(s) = 0 \Rightarrow D(s)=0$$</p>
    <p>äºæ˜¯ $D(s)=0$ï¼Œè¿›è€Œ $E(s,a)=0$ï¼Œå³ $(V,A)$ ä¸ $(V',A')$ åœ¨æ–½åŠ å‡å€¼çº¦æŸä¸‹ç›¸åŒï¼Œåˆ†è§£å”¯ä¸€ã€‚</p>

    <h3>4. ç½‘ç»œæ¶æ„è¯¦è§£</h3>
    <p>Dueling DQN çš„ç½‘ç»œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š</p>
    <ol>
      <li><strong>å…±äº«ç‰¹å¾æå–å±‚</strong>ï¼šå·ç§¯æˆ–å…¨è¿æ¥å±‚ç”¨äºæå–çŠ¶æ€ç‰¹å¾ï¼ˆä½œä¸ºåä¸¤ä¸ªåˆ†æ”¯çš„è¾“å…¥ï¼‰ï¼›</li>
      <li><strong>Value åˆ†æ”¯</strong>ï¼šç‹¬ç«‹çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œæœ€ç»ˆè¾“å‡ºå•ä¸ªæ ‡é‡ $V(s)$ï¼›</li>
      <li><strong>Advantage åˆ†æ”¯</strong>ï¼šç‹¬ç«‹çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œæœ€ç»ˆè¾“å‡º $|\mathcal{A}|$ ç»´å‘é‡ $A(s,a)$ã€‚</li>
    </ol>
    <p>å‰å‘ä¼ æ’­æ—¶ï¼Œå¯¹ Advantage å±‚è¾“å‡ºè¿›è¡Œ<strong>å»å‡å€¼å¤„ç†</strong>åä¸ Value å±‚ç»„åˆï¼š</p>
    <p>$$Q(s,a) = V(s) + \Big(A(s,a) - \frac{1}{|\mathcal{A}|}\sum_a A(s,a)\Big)$$</p>

    <h3>5. åˆå§‹åŒ–ä¸è®­ç»ƒç»†èŠ‚</h3>
    <p>ä¸ºç¡®ä¿ç½‘ç»œç¨³å®šæ€§å’Œå¯è¾¨è¯†æ€§ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„åˆå§‹åŒ–ï¼š</p>
    <ul>
      <li><strong>æƒé‡åˆå§‹åŒ–</strong>ï¼šä½¿ç”¨ Xavier uniform æˆ– Kaiming åˆå§‹åŒ–ï¼›å¯¹æ‰€æœ‰çº¿æ€§å±‚çš„åç½®åˆå§‹åŒ–ä¸º 0ï¼›</li>
      <li><strong>ä¸ºä»€ä¹ˆéœ€è¦ç‰¹æ®Šåˆå§‹åŒ–</strong>ï¼šAdvantage åˆ†æ”¯åˆæœŸåº”è¾“å‡ºæ¥è¿‘ 0 çš„å€¼ï¼Œè¿™æ · $A - \text{mean}(A) \approx 0$ï¼Œç¡®ä¿ Q å€¼åˆæœŸä¸»è¦ç”± $V(s)$ ä¸»å¯¼ï¼Œé¿å…ä¼˜åŠ¿åˆ†æ”¯è¿‡å¤§å¯¼è‡´çš„æ•°å€¼ä¸ç¨³å®šï¼›</li>
      <li><strong>ä¸»è¦ä¼˜åŠ¿</strong>ï¼šå°åˆå§‹ä¼˜åŠ¿ä¿è¯äº†åˆ†è§£çš„å¯è¾¨è¯†æ€§ï¼Œä½¿ V ä¸ A ä¸ä¼šåœ¨å‚æ•°ç©ºé—´ä¸­äº’ç›¸æŠµæ¶ˆã€‚</li>
    </ul>

    <h3>6. å®ç°æ­¥éª¤ï¼ˆé€æ­¥è¯´æ˜ï¼‰</h3>
    <p>ä¸‹é¢æŠŠ Dueling DQN çš„è®­ç»ƒè¿‡ç¨‹æ‹†æˆå¯ç›´æ¥è½åœ°çš„æ­¥éª¤ï¼Œä¾¿äºæŠŠä»£ç æ˜ å°„åˆ°å…·ä½“å®ç°ï¼š</p>
    <ol>
      <li><strong>å®šä¹‰ç½‘ç»œ</strong>ï¼šå…±äº«ä¸»å¹²æå–ç‰¹å¾ï¼›åˆ†å‡ºä¸¤ä¸ªå¤´â€”â€”Value è¾“å‡ºæ ‡é‡ V(s)ï¼ŒAdvantage è¾“å‡ºå‘é‡ A(s,a)ã€‚å‰å‘æ—¶å¯¹ A å»å‡å€¼å†ä¸ V é‡æ„ Qã€‚</li>
      <li><strong>ç»éªŒæ”¶é›†</strong>ï¼šä½¿ç”¨ Îµ-greedy æˆ–å¯å­¦ä¹ å™ªå£°ç­–ç•¥é‡‡æ · transition $(s,a,r,s',\text{done})$ï¼Œå¹¶å­˜å…¥ç»éªŒå›æ”¾æ±  ğ’Ÿã€‚</li>
      <li><strong>å°æ‰¹é‡é‡‡æ ·</strong>ï¼šä» ğ’Ÿ ä¸­éšæœºé‡‡æ · batchï¼›è‹¥ä½¿ç”¨ PER è¯·ç”¨é‡è¦æ€§æƒé‡ä¿®æ­£æŸå¤±ã€‚</li>
      <li><strong>ç›®æ ‡è®¡ç®—</strong>ï¼šç”¨ç›®æ ‡ç½‘ç»œ $\theta^-$ å‰å‘å¾—åˆ° $V'(s')$ ä¸ $A'(s',\cdot)$ï¼Œå¯¹ $A'$ å»å‡å€¼å¹¶é‡æ„ Q'ï¼›ç›®æ ‡ $y = r + \gamma \cdot \max_{a'} Q'(s',a')$ï¼ˆè‹¥ done åˆ™ $y = r$ï¼‰ã€‚</li>
      <li><strong>æŸå¤±ä¸æ›´æ–°</strong>ï¼šè®¡ç®— $L = \text{MSE}(y, Q(s,a;\theta))$ æˆ– Huber Lossï¼Œåå‘ä¼ æ’­å¹¶ç”¨ä¼˜åŒ–å™¨ step æ›´æ–°ä¸»ç½‘ç»œ $\theta$ã€‚</li>
      <li><strong>ç›®æ ‡ç½‘ç»œåŒæ­¥</strong>ï¼šé‡‡ç”¨ç¡¬æ›´æ–°ï¼ˆæ¯ C æ­¥å¤åˆ¶ï¼‰æˆ–è½¯æ›´æ–°ï¼ˆ$\theta^- \leftarrow \tau\theta + (1-\tau)\theta^-$ï¼‰ã€‚</li>
    </ol>

    <h3>7. PyTorch ä»£ç ç¤ºä¾‹</h3>
  <pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import deque

class DuelingNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DuelingNetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        
        # Value åˆ†æ”¯ï¼šè¾“å‡ºå•ä¸ªæ ‡é‡
        self.value_stream = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # Advantage åˆ†æ”¯ï¼šè¾“å‡º action_dim ç»´å‘é‡
        self.advantage_stream = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
        
        # æƒé‡åˆå§‹åŒ–
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        # æå–å…±äº«ç‰¹å¾
        features = self.feature(state)
        
        # Value ä¸ Advantage ç‹¬ç«‹å‰å‘
        v = self.value_stream(features)
        a = self.advantage_stream(features)
        
        # é‡ç»„ Q å€¼ï¼ˆå»å‡å€¼ä¿è¯å¯è¾¨è¯†ï¼‰
        a_mean = a.mean(dim=1, keepdim=True)
        q = v + (a - a_mean)
        
        return q

class DuelingDQN:
    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, epsilon=0.1):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        
        # ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_net = DuelingNetwork(state_dim, action_dim)
        self.target_q_net = DuelingNetwork(state_dim, action_dim)
        self.target_q_net.load_state_dict(self.q_net.state_dict())
        
        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        # ç»éªŒå›æ”¾æ± 
        self.memory = deque(maxlen=10000)
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state):
        """Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            with torch.no_grad():
                q_values = self.q_net(torch.FloatTensor(state).unsqueeze(0))
            return q_values.max(1)[1].item()
    
    def train_batch(self, batch_size):
        """è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡"""
        if len(self.memory) < batch_size:
            return
        
        # éšæœºé‡‡æ ·
        indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        
        states = torch.FloatTensor(np.array([x[0] for x in batch]))
        actions = torch.LongTensor([x[1] for x in batch])
        rewards = torch.FloatTensor([x[2] for x in batch])
        next_states = torch.FloatTensor(np.array([x[3] for x in batch]))
        dones = torch.FloatTensor([x[4] for x in batch])
        
        # å½“å‰ Q å€¼
        q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # ç›®æ ‡ Q å€¼
        with torch.no_grad():
            max_next_q = self.target_q_net(next_states).max(1)[0]
            target_q = rewards + self.gamma * max_next_q * (1 - dones)
        
        # æŸå¤±å’Œåå‘ä¼ æ’­
        loss = self.loss_fn(q_values, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)
        self.optimizer.step()
        
        return loss.item()
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_q_net.load_state_dict(self.q_net.state_dict())

# è®­ç»ƒç¤ºä¾‹
if __name__ == "__main__":
    state_dim = 4
    action_dim = 2
    agent = DuelingDQN(state_dim, action_dim)
    
    # æ¨¡æ‹Ÿç¯å¢ƒäº¤äº’
    for episode in range(100):
        state = np.random.randn(state_dim)
        episode_reward = 0
        
        for step in range(50):
            action = agent.act(state)
            next_state = np.random.randn(state_dim)
            reward = np.random.randn()
            done = step == 49
            
            agent.remember(state, action, reward, next_state, float(done))
            agent.train_batch(batch_size=32)
            
            episode_reward += reward
            state = next_state
        
        if (episode + 1) % 20 == 0:
            agent.update_target_network()
            print(f"Episode {episode+1}, Reward: {episode_reward:.2f}")
  </code></pre>
    <table>
      <tr>
        <th>é—®é¢˜</th>
        <th>è§£ç­”</th>
      </tr>
      <tr>
        <td>ä¸ºä»€ä¹ˆè¦å¯¹ Advantage å»å‡å€¼ï¼Ÿ</td>
        <td>å»å‡å€¼ä¿è¯äº†åˆ†è§£çš„å”¯ä¸€æ€§ã€‚è‹¥ä¸å»å‡å€¼ï¼Œå¯ä»¥åœ¨ V ä¸ A é—´ä»»æ„è½¬ç§»å¸¸æ•°å€¼è€Œäº§ç”Ÿç›¸åŒ Qï¼Œå¯¼è‡´å‚æ•°ä¸å¯è¾¨è¯†ã€‚</td>
      </tr>
      <tr>
        <td>V ä¸ A æ˜¯å¦éœ€è¦åŒæ—¶è®­ç»ƒï¼Ÿ</td>
        <td>æ˜¯çš„ã€‚ä¸¤ä¸ªåˆ†æ”¯å…±äº«ä¸»å¹²ï¼Œåå‘ä¼ æ’­ä¼šåŒæ—¶æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚V å­¦ä¹ çŠ¶æ€ä»·å€¼ï¼ŒA å­¦ä¹ åŠ¨ä½œç›¸å¯¹ä¼˜åŠ¿ï¼Œä¸¤è€…åˆ†å·¥æ˜ç¡®ã€‚</td>
      </tr>
      <tr>
        <td>åˆå§‹åŒ–ä¸ºä»€ä¹ˆå¾ˆé‡è¦ï¼Ÿ</td>
        <td>åˆæœŸ Advantage åº”æ¥è¿‘ 0ï¼Œä½¿ Q â‰ˆ Vï¼Œç¡®ä¿ç½‘ç»œç¨³å®šå¯åŠ¨ã€‚è¿‡å¤§çš„åˆå§‹ A ä¼šå¯¼è‡´æ•°å€¼éœ‡è¡ï¼Œå½±å“æ”¶æ•›ã€‚</td>
      </tr>
      <tr>
        <td>å¦‚ä½•ä¸ Double DQN ç»“åˆï¼Ÿ</td>
        <td>ç›®æ ‡è®¡ç®—æ—¶ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°ä»·å€¼ï¼Œä¸æ ‡å‡† Double DQN ç±»ä¼¼ï¼Œåªæ˜¯ç½‘ç»œç»“æ„ä»å•ä¸ª Q æ”¹ä¸º V+Aã€‚</td>
      </tr>
    </table>

    <h3>9. å®è·µè¦ç‚¹ä¸ç»„åˆç­–ç•¥</h3>
    <ul>
      <li><strong>ä¸ Double DQN ç»“åˆ</strong>ï¼šç›®æ ‡è®¡ç®—ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°ä»·å€¼ï¼Œæ¶ˆé™¤è¿‡ä¼°è®¡åå·®ï¼›</li>
      <li><strong>ä¸ PER ç»“åˆ</strong>ï¼šPrioritized Experience Replay èƒ½æ›´å¿«èšç„¦é«˜ TD è¯¯å·®æ ·æœ¬ï¼Œä¸ Dueling æ¶æ„ç›¸è¾…ç›¸æˆï¼›</li>
      <li><strong>ç¦»æ•£ vs è¿ç»­åŠ¨ä½œ</strong>ï¼šDueling DQN åœ¨ç¦»æ•£åŠ¨ä½œé—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ï¼›è¿ç»­æ§åˆ¶éœ€ä¸ç¡®å®šæ€§ç­–ç•¥ï¼ˆå¦‚ DDPGï¼‰æˆ–ç­–ç•¥åˆ†æ”¯æ”¹é€ ï¼›</li>
      <li><strong>è°ƒå‚å»ºè®®</strong>ï¼šé¿å…ä¼˜åŠ¿åˆ†æ”¯è¾“å‡ºè¿‡å¤§å¯¼è‡´æ•°å€¼ä¸ç¨³ï¼ˆå¯åŠ  L2 æ­£åˆ™æˆ–æ¢¯åº¦è£å‰ªï¼‰ï¼›ç›‘æ§ V ä¸ A çš„å¤§å°æ¯”ä¾‹ã€‚</li>
    </ul>
  </section>

    <section id="chapter15" class="chapter">
      <h2>ç¬¬åäº”ç« ï¼šPER</h2>
      

      <h3>PERï¼šä¼˜å…ˆç»éªŒå›æ”¾</h3>
      
      <p><strong>é—®é¢˜ï¼š</strong>æ™®é€šç»éªŒå›æ”¾æ˜¯å‡åŒ€éšæœºé‡‡æ ·ï¼Œä½†æœ‰äº›æ ·æœ¬ä¿¡æ¯é‡å¤§ï¼ˆæ¯”å¦‚ TD è¯¯å·®å¾ˆå¤§ï¼‰ï¼Œæœ‰äº›æ²¡å•¥ç”¨ã€‚</p>
      
      <p><strong>è§£å†³ï¼š</strong>ç»™æ¯ä¸ªæ ·æœ¬æ‰“åˆ†ï¼ŒTD è¯¯å·®è¶Šå¤§ â†’ ä¼˜å…ˆçº§è¶Šé«˜ â†’ è¶Šå®¹æ˜“è¢«é‡‡æ ·ã€‚</p>

      <h4>é‡‡æ ·æ¦‚ç‡</h4>
      <p>$$P(i) = \frac{(|\delta_i| + \varepsilon)^\alpha}{\sum_k (|\delta_k| + \varepsilon)^\alpha}$$</p>
      
      <table>
        <tr><th>å‚æ•°</th><th>å«ä¹‰</th><th>å¸¸ç”¨å€¼</th></tr>
        <tr><td>Î±</td><td>æ§åˆ¶ä¼˜å…ˆçº§å½±å“åŠ›åº¦</td><td>0.6</td></tr>
        <tr><td>Îµ</td><td>é˜²æ­¢ä¼˜å…ˆçº§ä¸º 0</td><td>1e-6</td></tr>
      </table>

      <h4>é‡è¦æ€§æƒé‡</h4>
      <p>å› ä¸ºæ”¹å˜äº†é‡‡æ ·åˆ†å¸ƒï¼Œéœ€è¦ç”¨æƒé‡ä¿®æ­£ï¼Œé¿å…åå·®ï¼š</p>
      <p>$$w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta$$</p>
      
      <p>Î² ä» 0.4 é€æ¸å‡åˆ° 1.0ï¼Œè®­ç»ƒå‰æœŸå…è®¸æœ‰ç‚¹åå·®ï¼ŒåæœŸä¸¥æ ¼ä¿®æ­£ã€‚</p>

      <p><strong>æœ€ç»ˆæŸå¤±ï¼š</strong></p>
      <p>$$L = \text{mean}(w_i \cdot \delta_i^2)$$</p>

      <h3>ä¸‰ã€æ•´åˆæµç¨‹ï¼šDueling + PER + Double</h3>
      
      <ol>
        <li>åˆå§‹åŒ– online ç½‘ç»œï¼ˆDueling ç»“æ„ï¼‰ã€target ç½‘ç»œã€PER ç¼“å†²åŒº</li>
        <li>ä¸ç¯å¢ƒäº¤äº’ï¼Œå¾—åˆ° (s, a, r, s', done)</li>
        <li>è®¡ç®— TD è¯¯å·®ï¼š
          $$\delta = \big|r + \gamma \cdot Q_{\text{target}}\big(s', \arg\max_{a'} Q_{\text{online}}(s',a')\big) - Q_{\text{online}}(s,a)\big|$$
        </li>
        <li>å­˜å…¥ PERï¼Œä¼˜å…ˆçº§ = Î´ + Îµ</li>
        <li>æŒ‰ä¼˜å…ˆçº§é‡‡æ · batchï¼Œå¾—åˆ°æƒé‡ w</li>
        <li>è®¡ç®—åŠ æƒæŸå¤±ï¼Œåå‘ä¼ æ’­</li>
        <li>æ›´æ–° PER ä¸­å„æ ·æœ¬çš„ä¼˜å…ˆçº§</li>
        <li>å®šæœŸåŒæ­¥ target ç½‘ç»œ</li>
      </ol>

      <p><strong>æ³¨æ„ï¼š</strong>è¿™é‡Œç”¨çš„æ˜¯ Double DQN çš„ç›®æ ‡è®¡ç®—æ–¹å¼ï¼ˆonline é€‰åŠ¨ä½œï¼Œtarget è¯„ä¼°ï¼‰ï¼Œé˜²æ­¢ Q å€¼è¿‡ä¼°è®¡ã€‚</p>

      <h3>å››ã€å®Œæ•´ä»£ç å®ç°</h3>

      <h4>Dueling ç½‘ç»œ</h4>
<pre><code class="language-python">
import torch
import torch.nn as nn

class DuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # å…±äº«ç‰¹å¾å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        # åˆ†æ”¯ 1ï¼šçŠ¶æ€ä»·å€¼
        self.value = nn.Linear(128, 1)
        # åˆ†æ”¯ 2ï¼šåŠ¨ä½œä¼˜åŠ¿
        self.advantage = nn.Linear(128, action_dim)

    def forward(self, x):
        feat = self.feature(x)
        v = self.value(feat)
        a = self.advantage(feat)
        # å»å‡å€¼ç»„åˆ
        q = v + (a - a.mean(dim=1, keepdim=True))
        return q
</code></pre>

      <h4>PER ç¼“å†²åŒºï¼ˆSumTree å®ç°ï¼‰</h4>
<pre><code class="language-python">
import numpy as np
import random

class SumTree:
    """SumTree ç”¨äºå¿«é€Ÿé‡‡æ ·ï¼Œæ—¶é—´å¤æ‚åº¦ O(log N)"""
    def __init__(self, capacity):
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1)
        self.data = [None] * capacity
        self.write = 0
        self.n_entries = 0

    def _propagate(self, idx, change):
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)

    def update(self, idx, priority):
        change = priority - self.tree[idx]
        self.tree[idx] = priority
        self._propagate(idx, change)

    def add(self, priority, data):
        idx = self.write + self.capacity - 1
        self.data[self.write] = data
        self.update(idx, priority)
        self.write = (self.write + 1) % self.capacity
        self.n_entries = min(self.n_entries + 1, self.capacity)

    def get(self, s):
        idx = 0
        while True:
            left = 2 * idx + 1
            if left >= len(self.tree):
                break
            if s <= self.tree[left]:
                idx = left
            else:
                s -= self.tree[left]
                idx = left + 1
        data_idx = idx - self.capacity + 1
        return idx, self.tree[idx], self.data[data_idx]

    @property
    def total(self):
        return self.tree[0]

class PERBuffer:
    def __init__(self, capacity, alpha=0.6, eps=1e-6):
        self.tree = SumTree(capacity)
        self.alpha = alpha
        self.eps = eps

    def add(self, td_error, sample):
        priority = (abs(td_error) + self.eps) ** self.alpha
        self.tree.add(priority, sample)

    def sample(self, batch_size, beta=0.4):
        batch, idxs, priorities = [], [], []
        segment = self.tree.total / batch_size
        
        for i in range(batch_size):
            a, b = segment * i, segment * (i + 1)
            s = random.uniform(a, b)
            idx, p, data = self.tree.get(s)
            batch.append(data)
            idxs.append(idx)
            priorities.append(p)
        
        # è®¡ç®—é‡è¦æ€§æƒé‡
        probs = np.array(priorities) / self.tree.total
        weights = (self.tree.n_entries * probs) ** (-beta)
        weights /= weights.max()  # å½’ä¸€åŒ–
        
        return batch, idxs, torch.FloatTensor(weights)

    def update_priority(self, idx, td_error):
        priority = (abs(td_error) + self.eps) ** self.alpha
        self.tree.update(idx, priority)
</code></pre>

      <h4>è®­ç»ƒæµç¨‹</h4>
<pre><code class="language-python">
import torch.optim as optim
import torch.nn.functional as F

# åˆå§‹åŒ–
online_net = DuelingDQN(state_dim=4, action_dim=2)
target_net = DuelingDQN(state_dim=4, action_dim=2)
target_net.load_state_dict(online_net.state_dict())
optimizer = optim.Adam(online_net.parameters(), lr=1e-4)

buffer = PERBuffer(capacity=10000, alpha=0.6)
gamma = 0.99
beta = 0.4

for episode in range(1000):
    state = env.reset()
    done = False
    
    while not done:
        # Îµ-greedy é€‰åŠ¨ä½œ
        if random.random() < epsilon:
            action = random.randint(0, action_dim-1)
        else:
            with torch.no_grad():
                q = online_net(torch.FloatTensor(state).unsqueeze(0))
                action = q.argmax().item()
        
        next_state, reward, done, _ = env.step(action)
        
        # è®¡ç®— TD è¯¯å·®
        with torch.no_grad():
            q_next = target_net(torch.FloatTensor(next_state).unsqueeze(0))
            best_action = online_net(torch.FloatTensor(next_state).unsqueeze(0)).argmax()
            td_target = reward + (1 - done) * gamma * q_next[0, best_action]
            q_current = online_net(torch.FloatTensor(state).unsqueeze(0))[0, action]
            td_error = abs(td_target - q_current).item()
        
        # å­˜å…¥ PER
        buffer.add(td_error, (state, action, reward, next_state, done))
        state = next_state
        
        # è®­ç»ƒ
        if buffer.tree.n_entries > 64:
            batch, idxs, weights = buffer.sample(32, beta)
            
            states = torch.FloatTensor([x[0] for x in batch])
            actions = torch.LongTensor([x[1] for x in batch])
            rewards = torch.FloatTensor([x[2] for x in batch])
            next_states = torch.FloatTensor([x[3] for x in batch])
            dones = torch.FloatTensor([x[4] for x in batch])
            
            # Double DQN ç›®æ ‡
            q_eval = online_net(states).gather(1, actions.unsqueeze(1)).squeeze()
            with torch.no_grad():
                best_actions = online_net(next_states).argmax(1)
                q_next = target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze()
                q_target = rewards + (1 - dones) * gamma * q_next
            
            # åŠ æƒæŸå¤±
            td_errors = (q_target - q_eval).abs()
            loss = (weights * F.mse_loss(q_eval, q_target, reduction='none')).mean()
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # æ›´æ–°ä¼˜å…ˆçº§
            for idx, err in zip(idxs, td_errors.detach().numpy()):
                buffer.update_priority(idx, err)
    
    # å®šæœŸåŒæ­¥ target ç½‘ç»œ
    if episode % 10 == 0:
        target_net.load_state_dict(online_net.state_dict())
</code></pre>

      <h3>äº”ã€æ•ˆæœå¯¹æ¯”</h3>
      <ul>
        <li><strong>Dueling</strong>ï¼šç½‘ç»œæ›´é«˜æ•ˆï¼Œå°¤å…¶åœ¨åŠ¨ä½œå½±å“å°çš„çŠ¶æ€</li>
        <li><strong>PER</strong>ï¼šæ”¶æ•›æ›´å¿«ï¼Œæ ·æœ¬åˆ©ç”¨ç‡æ›´é«˜</li>
        <li><strong>Double</strong>ï¼šé˜²æ­¢ Q å€¼è¿‡ä¼°è®¡</li>
      </ul>

      <p>ä¸‰è€…ç»„åˆæ˜¯ Rainbow DQN çš„æ ¸å¿ƒï¼Œåœ¨ Atari æ¸¸æˆä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>

      <p><strong>æ”¹è¿›è·¯çº¿ï¼š</strong>DQN â†’ Double DQN â†’ Dueling DQN â†’ PER â†’ Rainbow DQN</p>
    </section>

    <section id="chapter16" class="chapter">
      <h2>ç¬¬åå…­ç« ï¼šNoisyNet DQNï¼ˆè‡ªé€‚åº”æ¢ç´¢ï¼‰</h2>

      <p>å‰é¢é€šè¿‡ Dueling å’Œ PER æ”¹è¿›äº†ç½‘ç»œç»“æ„å’Œé‡‡æ ·æ•ˆç‡ï¼Œä½†æ¢ç´¢é—®é¢˜è¿˜æ²¡è§£å†³å¥½ã€‚</p>

      <p>ä¼ ç»Ÿçš„ <strong>Îµ-greedy</strong> æ¢ç´¢é äººå·¥è®¾å®š Îµï¼Œå®ƒçš„éšæœºæ€§è·Ÿç½‘ç»œå­¦ä¹ æ²¡ä»€ä¹ˆå…³ç³»ï¼Œå¾ˆéš¾è‡ªé€‚åº”è°ƒæ•´æ¢ç´¢å¼ºåº¦ã€‚<br>
      <strong>NoisyNet DQN</strong>ï¼ˆFortunato et al., 2017ï¼‰æ¢äº†ä¸ªæ€è·¯ â€”â€” ç›´æ¥åœ¨ç½‘ç»œå‚æ•°é‡ŒåŠ å¯å­¦ä¹ çš„å™ªå£°ï¼Œè®©æ¢ç´¢å˜æˆç½‘ç»œå­¦ä¹ çš„ä¸€éƒ¨åˆ†ã€‚</p>

      <h3>ä¸€ã€æ ¸å¿ƒæƒ³æ³•</h3>
      <p>ç”¨å¸¦å™ªå£°çš„çº¿æ€§å±‚æ›¿ä»£æ™®é€šå…¨è¿æ¥å±‚ï¼š</p>

      <pre><code class="language-python">
y = (W + Ïƒ_W âŠ™ Îµ_W) Â· x + (b + Ïƒ_b âŠ™ Îµ_b)
      </code></pre>

      <ul>
        <li><code>W, b</code>ï¼šæ ‡å‡†æƒé‡å’Œåç½®</li>
        <li><code>Ïƒ_W, Ïƒ_b</code>ï¼šå¯å­¦ä¹ çš„å™ªå£°å¼ºåº¦ï¼ˆç½‘ç»œè®­ç»ƒæ—¶ä¼šè°ƒæ•´ï¼‰</li>
        <li><code>Îµ_W, Îµ_b</code>ï¼šæ¯æ¬¡å‰å‘ä¼ æ’­é‡æ–°é‡‡æ ·çš„é«˜æ–¯å™ªå£°</li>
      </ul>

      <p>è¿™æ ·ç½‘ç»œè¾“å‡ºè‡ªå¸¦éšæœºæ€§ï¼Œå™ªå£°å¤§å°éšè®­ç»ƒè‡ªåŠ¨è°ƒæ•´ã€‚ä¸ç”¨æ‰‹åŠ¨è°ƒ Îµ äº†ï¼Œæ¢ç´¢å¼ºåº¦ç½‘ç»œè‡ªå·±å­¦ã€‚</p>

      <h3>äºŒã€ä»£ç å®ç°</h3>
<pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class NoisyLinear(nn.Module):
    def __init__(self, in_features, out_features, std_init=0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        # å¯å­¦ä¹ å‚æ•°
        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.register_buffer("weight_eps", torch.empty(out_features, in_features))

        self.bias_mu = nn.Parameter(torch.empty(out_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))
        self.register_buffer("bias_eps", torch.empty(out_features))

        self.reset_parameters()
        self.sample_noise()

    def reset_parameters(self):
        mu_range = 1 / math.sqrt(self.in_features)
        self.weight_mu.data.uniform_(-mu_range, mu_range)
        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))
        self.bias_mu.data.uniform_(-mu_range, mu_range)
        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))

    def sample_noise(self):
        """é‡æ–°é‡‡æ ·å™ªå£°"""
        self.weight_eps.normal_()
        self.bias_eps.normal_()

    def forward(self, x):
        if self.training:
            # è®­ç»ƒæ—¶åŠ å™ªå£°
            w = self.weight_mu + self.weight_sigma * self.weight_eps
            b = self.bias_mu + self.bias_sigma * self.bias_eps
        else:
            # æµ‹è¯•æ—¶ä¸åŠ å™ªå£°
            w, b = self.weight_mu, self.bias_mu
        return F.linear(x, w, b)
</code></pre>

      <h3>ä¸‰ã€ä¸ Dueling DQN ç»“åˆ</h3>
<pre><code class="language-python">
class NoisyDuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # å…±äº«ç‰¹å¾å±‚ï¼ˆæ™®é€šå±‚ï¼‰
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        # ä»·å€¼æµå’Œä¼˜åŠ¿æµç”¨ NoisyLinear
        self.value = NoisyLinear(128, 1)
        self.advantage = NoisyLinear(128, action_dim)

    def forward(self, x):
        feat = self.feature(x)
        v = self.value(feat)
        a = self.advantage(feat)
        q = v + (a - a.mean(dim=1, keepdim=True))
        return q
    
    def sample_noise(self):
        """é‡é‡‡æ ·æ‰€æœ‰å™ªå£°å±‚"""
        self.value.sample_noise()
        self.advantage.sample_noise()
</code></pre>

      <h3>å››ã€è®­ç»ƒæµç¨‹</h3>
      <ol>
        <li>æŠŠ Dueling DQN çš„ <code>Linear</code> å±‚æ¢æˆ <code>NoisyLinear</code></li>
        <li><strong>å»æ‰ Îµ-greedy</strong>ï¼Œæ¢ç´¢å®Œå…¨é å™ªå£°é©±åŠ¨</li>
        <li>æ¯æ¬¡å‰å‘ä¼ æ’­å‰è°ƒç”¨ <code>sample_noise()</code> é‡æ–°é‡‡æ ·å™ªå£°</li>
        <li>æŸå¤±å‡½æ•°ã€TD ç›®æ ‡ã€target æ›´æ–°éƒ½è·Ÿ DQN ä¸€æ ·</li>
      </ol>

<pre><code class="language-python">
# è®­ç»ƒæ—¶æ— éœ€ Îµ-greedy
for episode in range(1000):
    state = env.reset()
    online_net.sample_noise()  # æ¯ä¸ª episode å¼€å§‹é‡é‡‡æ ·å™ªå£°
    
    while not done:
        # ç›´æ¥é€‰æœ€ä¼˜åŠ¨ä½œï¼Œå™ªå£°å·²ç»åœ¨ç½‘ç»œé‡Œäº†
        with torch.no_grad():
            q = online_net(torch.FloatTensor(state).unsqueeze(0))
            action = q.argmax().item()
        
        next_state, reward, done, _ = env.step(action)
        buffer.add((state, action, reward, next_state, done))
        state = next_state
        
        # è®­ç»ƒæ­¥éª¤
        if len(buffer) > batch_size:
            batch = buffer.sample(batch_size)
            online_net.sample_noise()  # è®­ç»ƒå‰é‡é‡‡æ ·
            target_net.sample_noise()
            
            # è®¡ç®—æŸå¤±å¹¶æ›´æ–°...
</code></pre>

      <h3>äº”ã€ä¼˜ç‚¹</h3>
      <ul>
        <li><strong>è‡ªé€‚åº”æ¢ç´¢</strong>ï¼šç½‘ç»œåœ¨ä¸ç¡®å®šçš„çŠ¶æ€è‡ªåŠ¨å¢å¤§å™ªå£°</li>
        <li><strong>å‡å°‘è¶…å‚</strong>ï¼šä¸ç”¨æ‰‹åŠ¨è°ƒ Îµ å’Œè¡°å‡ç­–ç•¥äº†</li>
        <li><strong>æ€§èƒ½æ›´å¥½</strong>ï¼šåœ¨ Atari å’Œ MuJoCo ä¸Šæ™®éä¼˜äº Îµ-greedy</li>
      </ul>

      <h3>å…­ã€å…¸å‹ç»„åˆ</h3>
      <p>NoisyNet ç»å¸¸å’Œè¿™äº›ä¸€èµ·ç”¨ï¼š</p>
      <ul>
        <li><strong>Dueling DQN</strong> â†’ æå‡ç‰¹å¾è¡¨è¾¾</li>
        <li><strong>Double DQN</strong> â†’ å‡å°‘è¿‡ä¼°è®¡</li>
        <li><strong>PER</strong> â†’ åŠ é€Ÿæ ·æœ¬åˆ©ç”¨</li>
      </ul>
      <p>ç»„åˆèµ·æ¥å°±æ˜¯ Rainbow DQN çš„ç¬¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚</p>

      <h3>ä¸ƒã€å°ç»“</h3>
      <p>NoisyNet æŠŠæ¢ç´¢æœºåˆ¶èå…¥ç½‘ç»œå‚æ•°ï¼Œè®©æ™ºèƒ½ä½“è‡ªå·±è°ƒèŠ‚æ¢ç´¢å¼ºåº¦ã€‚<br>
      ç›¸æ¯”ä¼ ç»Ÿ Îµ-greedyï¼Œè¿™æ‰æ˜¯çœŸæ­£çš„"å¯å­¦ä¹ æ¢ç´¢"ï¼Œåœ¨å¤æ‚ç¯å¢ƒé‡Œæ›´é«˜æ•ˆã€æ›´æ™ºèƒ½ã€‚</p>

      <p><strong>æ”¹è¿›è·¯çº¿ï¼š</strong>DQN â†’ Double DQN â†’ Dueling DQN â†’ PER â†’ <strong>NoisyNet DQN</strong> â†’ Rainbow DQN</p>
    </section>

    <section id="chapter17" class="chapter">
      <h2>ç¬¬åä¸ƒç« ï¼šRainbow DQNï¼ˆé›†å¤§æˆè€…ï¼‰</h2>

      <h3>é¢„å¤‡çŸ¥è¯†ï¼šN-Step Learningï¼ˆå¤šæ­¥æ—¶åºå·®åˆ†ï¼‰</h3>

      <p><strong>æ ¸å¿ƒåŠ¨æœºï¼š</strong> ç›¸æ¯” 1-step TD åªçœ‹ä¸‹ä¸€æ­¥å¥–åŠ±ï¼ŒN-step å°†æœªæ¥ N æ­¥çš„çœŸå®å¥–åŠ±ä¸€æ¬¡æ€§çº³å…¥ç›®æ ‡ï¼Œè®©å¥–åŠ±ä¿¡å·æ›´å¿«åœ°å›ä¼ åˆ°å½“å‰çŠ¶æ€ï¼Œä»è€ŒåŠ é€Ÿå­¦ä¹ ã€æé«˜æ ·æœ¬åˆ©ç”¨ç‡ã€‚</p>

      <h4>1. ç†è®ºèƒŒæ™¯ï¼šä»‹äº TD ä¸ Monte Carlo ä¹‹é—´çš„æŠ˜ä¸­</h4>
  <p>ä¼ ç»Ÿæ—¶åºå·®åˆ†ï¼ˆTDï¼‰æ¯æ­¥æ›´æ–°ä¸€æ¬¡ï¼Œåªåˆ©ç”¨ä¸‹ä¸€æ­¥å¥–åŠ±ï¼š</p>
  $$y_t^{(1)} = r_t + \gamma \max_{a} Q_{\text{target}}(s_{t+1}, a)$$
  <p>è€Œ Monte Carlo æ–¹æ³•ç­‰æ•´å±€ç»“æŸåç”¨å®Œæ•´å›æŠ¥æ›´æ–°ï¼š</p>
  $$y_t^{(MC)} = r_t + \gamma r_{t+1} + \gamma^{2} r_{t+2} + \dots + \gamma^{T-t} r_T$$

      <p>äºŒè€…ç‰¹ç‚¹å¦‚ä¸‹ï¼š</p>
      <ul>
        <li><strong>TDï¼š</strong> æ›´æ–°å¿«ã€ä½æ–¹å·®ï¼Œä½†ä»…çœ‹ä¸€æ­¥ï¼Œä¼ æ’­æ…¢ã€‚</li>
        <li><strong>Monte Carloï¼š</strong> åˆ©ç”¨å®Œæ•´ä¿¡æ¯ã€æ— åï¼Œä½†éœ€ç­‰æ•´å±€ç»“æŸã€æ–¹å·®å¤§ã€‚</li>
      </ul>

  <p><strong>N-step Learning</strong> ä»‹äºä¸¤è€…ä¹‹é—´ï¼Œé€šè¿‡å›ºå®šé•¿åº¦ N çš„æ»šåŠ¨çª—å£ï¼Œå°†æœªæ¥ N æ­¥çš„çœŸå®å¥–åŠ±çº³å…¥æ›´æ–°ç›®æ ‡ï¼š</p>
  $$y_t^{(N)} = \sum_{i=0}^{N-1} \gamma^{i} r_{t+i} + \gamma^{N} \max_{a} Q_{\text{target}}(s_{t+N}, a)$$

      <p>è¿™æ ·å¯ä»¥åŒæ—¶ä¿ç•™ TD çš„åœ¨çº¿æ›´æ–°èƒ½åŠ›ï¼Œåˆå¼•å…¥ Monte Carlo çš„é•¿è¿œè§†è§’ï¼Œå…¼é¡¾ç¨³å®šæ€§ä¸é«˜æ•ˆæ€§ã€‚</p>

      <h4>2. å®ç°é€»è¾‘ï¼ˆæ»‘åŠ¨çª—å£æœºåˆ¶ï¼‰</h4>
      <ol>
        <li>ç»´æŠ¤é•¿åº¦ä¸º N çš„é˜Ÿåˆ— <code>n_step_buffer</code>ï¼Œæ¯æ¬¡ç¯å¢ƒäº¤äº’åå…¥é˜Ÿæ ·æœ¬ <code>(s, a, r, s_next, done)</code>ã€‚</li>
        <li>å½“é˜Ÿåˆ—æ»¡ N æ—¶ï¼Œè®¡ç®—ç´¯è®¡æŠ˜æ‰£å¥–åŠ±ï¼š
  $$R = \sum_{i=0}^{N-1} \gamma^{i} r_{t+i}$$</li>
  <li>å–é˜Ÿé¦–çŠ¶æ€ <code>(s_t, a_t)</code> å’Œé˜Ÿå°¾ä¸‹ä¸€ä¸ªçŠ¶æ€ <code>s_{t+N}</code> æ„é€  N-step æ ·æœ¬ï¼š
  $$(s_t, a_t, R, s_{t+N}, done)$$</li>
  <li>å­˜å…¥ç»éªŒå›æ”¾æ± ï¼Œè®­ç»ƒæ—¶ä»¥ç›®æ ‡ï¼š
  $$y_t^{(N)} = R + \gamma^{N}\max_{a} Q_{\text{target}}(s_{t+N}, a)$$</li>
      </ol>

      <h4>3. å‚æ•°ä¸æ³¨æ„äº‹é¡¹</h4>
      <ul>
        <li>N é€šå¸¸å– <strong>3~5</strong>ï¼ˆRainbow é»˜è®¤ N=3ï¼‰ã€‚</li>
        <li>è‹¥å›åˆæå‰ç»“æŸï¼ˆä¸è¶³ N æ­¥ï¼‰ï¼Œç›´æ¥ç”¨å·²è§‚æµ‹åˆ°çš„å¥–åŠ±ç´¯åŠ ã€‚</li>
        <li>ä¸ PERã€NoisyNetã€Dueling å¯æ— ç¼ç»“åˆï¼Œå¢å¼ºè®­ç»ƒæ•ˆç‡ä¸ç¨³å®šæ€§ã€‚</li>
      </ul>

      <hr>

      <h3>é¢„å¤‡çŸ¥è¯†ï¼šC51ï¼ˆCategorical DQNï¼Œåˆ†å¸ƒå¼ä»·å€¼å­¦ä¹ ï¼‰</h3>

      <p><strong>æ ¸å¿ƒåŠ¨æœºï¼š</strong> DQN åªå­¦ä¹ å›æŠ¥çš„æœŸæœ› Q(s,a)ï¼Œè€Œ C51 ç›´æ¥å­¦ä¹ å›æŠ¥åˆ†å¸ƒ Z(s,a)ï¼Œæ•æ‰é£é™©ä¸ä¸ç¡®å®šæ€§ï¼Œä½¿æ™ºèƒ½ä½“çš„ç­–ç•¥æ›´ç¨³å¥ã€æ›´å…·è¡¨è¾¾åŠ›ã€‚</p>

      <h4>1. å›ºå®šæ”¯æŒåŒºé—´ä¸ 51 ä¸ªç¦»æ•£åŸå­ï¼ˆatomsï¼‰</h4>
      <ul>
        <li>C51 å°†æœªæ¥å›æŠ¥çš„èŒƒå›´å›ºå®šåœ¨ <code>[V_min, V_max]</code> ä¸Šï¼Œå¹¶å‡åŒ€åˆ’åˆ†ä¸º 51 ä¸ªå–å€¼ç‚¹ï¼š</li>
  $$z_i = V_{\min} + i\,\frac{V_{\max}-V_{\min}}{50},\quad i=0,1,\dots,50$$
        <li>æ¯ä¸ªåŠ¨ä½œ a è¾“å‡º 51 ä¸ªæ¦‚ç‡ <code>p_i(s,a)</code>ï¼Œè¡¨ç¤ºå›æŠ¥è½åœ¨ z_i å¤„çš„æ¦‚ç‡ã€‚</li>
      </ul>

      <h4>2. V<sub>min</sub> / V<sub>max</sub> çš„é€‰å–ä¾æ®</h4>
      <ul>
        <li>ç†è®ºä¸Šï¼šè‹¥æ¯æ­¥å¥–åŠ±èŒƒå›´ä¸º <code>[r_min, r_max]</code>ï¼ŒæŠ˜æ‰£å› å­ä¸º Î³ï¼Œåˆ™æœªæ¥ç´¯è®¡å›æŠ¥çš„ä¸Šä¸‹é™ä¸ºï¼š
  $$V_{\min} = \frac{r_{\min}}{1-\gamma},\quad V_{\max} = \frac{r_{\max}}{1-\gamma}$$</li>
        <li>å®è·µä¸­å¯æ ¹æ®ä»»åŠ¡ç»éªŒé€‰å–æ›´ç´§åŒºé—´ã€‚ä¾‹å¦‚ï¼š
          <ul>
            <li>Atari ç¯å¢ƒï¼šé€šå¸¸è®¾ä¸º [-10, 10] æˆ– [-100, 100]</li>
            <li>è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼ˆå¦‚ MuJoCoï¼‰ï¼šæ ¹æ®å¥–åŠ±å°ºåº¦é€‚å½“ç¼©æ”¾</li>
          </ul>
        </li>
        <li>å›ºå®š <code>[V_min, V_max]</code> ä½¿ç½‘ç»œè¾“å‡ºç»´åº¦ç¨³å®šï¼Œä¾¿äºåˆ†å¸ƒæŠ•å½±ã€‚</li>
      </ul>

      <h4>3. ç½‘ç»œè¾“å‡ºä¸åŠ¨ä½œé€‰æ‹©</h4>
      <ul>
        <li>è¾“å‡ºï¼šæ¯ä¸ªåŠ¨ä½œå¯¹åº” 51 ç»´æ¦‚ç‡åˆ†å¸ƒï¼Œç» softmax å½’ä¸€åŒ–ã€‚</li>
        <li>è®­ç»ƒï¼šä¼˜åŒ–æ•´æ¡åˆ†å¸ƒï¼›æ‰§è¡ŒåŠ¨ä½œæ—¶å–æœŸæœ›ï¼š
  $$Q(s,a) = \sum_i p_i(s,a) z_i$$</li>
      </ul>

      <h4>4. åˆ†å¸ƒå¼ Bellman æ›´æ–°ä¸æŠ•å½±</h4>
      <ol>
        <li>ç”¨ Double DQN æ€æƒ³é€‰æ‹©ä¸‹ä¸€åŠ¨ä½œï¼š
  $$a^* = \arg\max_{a} \sum_i p_i(s',a) z_i$$</li>
        <li>æ„é€ ç›®æ ‡åˆ†å¸ƒï¼š
  $$T Z = r + \gamma \; Z_{target}(s', a^*)$$</li>
        <li>å°†å…¶çº¿æ€§æŠ•å½±å›å›ºå®šæ”¯æŒåŒºé—´ï¼Œå¾—åˆ°ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒ m_iã€‚</li>
      </ol>

      <h4>5. æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µï¼ˆCross-Entropyï¼‰</h4>
      <p>è®­ç»ƒç›®æ ‡ï¼šè®©é¢„æµ‹åˆ†å¸ƒ <code>p</code> é€¼è¿‘ç›®æ ‡åˆ†å¸ƒ <code>m</code>ã€‚</p>
  $$L = - \sum_i m_i \log p_i$$

      <h5>äº¤å‰ç†µæœ€å°å€¼è¯æ˜ï¼ˆåŸºäº Jensen ä¸ç­‰å¼ï¼‰</h5>
  <p>å®šä¹‰äº¤å‰ç†µï¼š$$H(p,q) = -\sum_i p_i \ln q_i$$ ï¼Œè¯æ˜å½“ $p=q$ æ—¶ $H(p,q)$ æœ€å°ã€‚</p>
  $$H(p,q)-H(p,p)=\sum_i p_i \ln \frac{q_i}{p_i}$$
  <p>ç”±äº $\ln(x)$ ä¸ºå‡¹å‡½æ•°ï¼ŒJensen ä¸ç­‰å¼ï¼š</p>
  $$\sum_i p_i \ln x_i \le \ln\Big(\sum_i p_i x_i\Big),\; \sum_i p_i = 1$$
  <p>å– $x_i = q_i/p_i$ å¾—ï¼š</p>
  $$\sum_i p_i \ln \frac{q_i}{p_i} \le \ln\Big(\sum_i q_i\Big)=\ln 1=0$$
  $$\Rightarrow\; H(p,p) - H(p,q) \le 0 \;\Rightarrow\; H(p,p) \le H(p,q)$$
  <p>å½“ $p=q$ æ—¶ç­‰å·æˆç«‹ã€‚</p>
      <p>å› æ­¤ï¼Œé¢„æµ‹åˆ†å¸ƒç­‰äºç›®æ ‡åˆ†å¸ƒæ™‚ï¼Œäº¤å‰ç†µå–å¾—æœ€å°å€¼ï¼Œè¿™å°±æ˜¯åˆ†å¸ƒå­¦ä¹ ç¨³å®šçš„ç†è®ºåŸºç¡€ã€‚</p>

      <h4>6. å°ç»“</h4>
      <ul>
        <li><strong>N-Stepï¼š</strong> æ˜¯ TD ä¸ Monte Carlo çš„ä¸­é—´å½¢æ€ï¼Œç»“åˆçŸ­æœŸç¨³å®šä¸é•¿æœŸè§†é‡ï¼ŒåŠ å¿«å¥–åŠ±ä¼ æ’­ã€‚</li>
        <li><strong>C51ï¼š</strong> ç›´æ¥å­¦ä¹ å›æŠ¥åˆ†å¸ƒï¼ˆ51 ä¸ªæ¦‚ç‡ï¼‰ï¼Œé€šè¿‡äº¤å‰ç†µä¼˜åŒ–é¢„æµ‹åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚</li>
        <li><strong>V<sub>min</sub>/V<sub>max</sub>ï¼š</strong> è¡¨ç¤ºç¯å¢ƒå¯èƒ½å›æŠ¥çš„å…¨å±€ä¸Šä¸‹ç•Œï¼Œé€šå¸¸è®¾ä¸º <code>[r_min/(1-Î³), r_max/(1-Î³)]</code> æˆ–ç»éªŒèŒƒå›´ã€‚</li>
        <li>ä¸¤è€…å‡ä¸º Rainbow DQN çš„å…³é”®ç»„æˆæ¨¡å—ï¼Œä¸ Doubleã€Duelingã€PERã€NoisyNet å…±åŒç»„æˆå®Œæ•´å¼ºåŒ–å­¦ä¹ å½©è™¹ä½“ç³»ã€‚</li>
      </ul>

      <hr>

      <h3>å¼ºåŒ–å­¦ä¹ è¿›é˜¶ï¼šRainbow DQN ğŸŒˆï¼ˆç»ˆæ DQN æ”¹è¿›ç‰ˆï¼‰</h3>

      <p><strong>è®ºæ–‡ï¼š</strong> Hessel et al., "<em>Rainbow: Combining Improvements in Deep Reinforcement Learning</em>", DeepMind, 2018</p>

      <p><strong>æ ¸å¿ƒæ€æƒ³ï¼š</strong> Rainbow å°†å…­å¤§å¼ºåŒ–å­¦ä¹ æ”¹è¿›æ¨¡å—æ•´åˆå…¥ DQN æ¡†æ¶ï¼Œä½¿æ™ºèƒ½ä½“åœ¨ç¨³å®šæ€§ã€æ ·æœ¬æ•ˆç‡ã€æ¢ç´¢èƒ½åŠ›ä¸è¡¨è¾¾èƒ½åŠ›ä¸Šå…¨é¢æå‡ã€‚</p>

      <h4>1. Rainbow çš„å…­å¤§ç»„æˆæ¨¡å—</h4>

      <table>
        <tr><th>æ¨¡å—</th><th>æ”¹è¿›æ–¹å‘</th><th>ä¸»è¦ä½œç”¨</th></tr>
        <tr><td><strong>Double DQN</strong></td><td>ç›®æ ‡è®¡ç®—</td><td>å‡å°‘ Q å€¼è¿‡ä¼°è®¡åå·®</td></tr>
        <tr><td><strong>Dueling DQN</strong></td><td>ç½‘ç»œç»“æ„</td><td>åˆ†ç¦»çŠ¶æ€ä»·å€¼ä¸åŠ¨ä½œä¼˜åŠ¿</td></tr>
        <tr><td><strong>PERï¼ˆPrioritized Experience Replayï¼‰</strong></td><td>æ•°æ®é‡‡æ ·</td><td>æé«˜å…³é”®æ ·æœ¬é‡‡æ ·é¢‘ç‡</td></tr>
        <tr><td><strong>NoisyNet</strong></td><td>æ¢ç´¢ç­–ç•¥</td><td>åœ¨å‚æ•°ä¸­æ³¨å…¥å¯å­¦ä¹ å™ªå£°ï¼Œæ›¿ä»£ Îµ-greedy</td></tr>
        <tr><td><strong>N-Step Learning</strong></td><td>å­¦ä¹ ä¿¡å·</td><td>åŠ é€Ÿå¥–åŠ±ä¼ æ’­ï¼Œå…¼é¡¾ TD ä¸ Monte Carlo</td></tr>
        <tr><td><strong>C51</strong></td><td>ä»·å€¼è¡¨ç¤º</td><td>é¢„æµ‹å›æŠ¥åˆ†å¸ƒè€Œéå•ä¸€æœŸæœ›å€¼</td></tr>
      </table>

      <p>è¿™å…­ä¸ªæ¨¡å—æ„æˆäº† DQN çš„"å½©è™¹"å¢å¼ºä½“ç³»ã€‚</p>

      <hr>

      <h4>2. ç½‘ç»œç»“æ„ï¼ˆDueling + NoisyNet + C51ï¼‰</h4>

      <ul>
        <li><strong>Dueling ç»“æ„ï¼š</strong> å…±äº«ç‰¹å¾æå–å±‚ååˆ†ä¸ºä¸¤æ”¯ï¼š
        <pre><code>Q(s,a) = V(s) + [A(s,a) - mean(A(s,Â·))]</code></pre>
        ä½¿ç½‘ç»œèƒ½åŒºåˆ†"çŠ¶æ€æœ¬èº«ä»·å€¼"ä¸"åŠ¨ä½œå¸¦æ¥çš„å¢ç›Š"ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚</li>

        <li><strong>NoisyNetï¼š</strong> å°†å…¨è¿æ¥å±‚æ›¿æ¢ä¸ºå¸¦å™ªå£°çº¿æ€§å±‚ï¼š
        <pre><code>W' = W + Ïƒ_W âŠ™ Îµ_W,   b' = b + Ïƒ_b âŠ™ Îµ_b</code></pre>
        æ¯æ¬¡å‰å‘ä¼ æ’­å‰é‡‡æ ·éšæœºå™ªå£° Îµï¼ŒÏƒ ä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œå½¢æˆå¯å­¦ä¹ æ¢ç´¢æœºåˆ¶ã€‚</li>

        <li><strong>C51 è¾“å‡ºå±‚ï¼š</strong> æ¯ä¸ªåŠ¨ä½œè¾“å‡º 51 ä¸ª logitsï¼Œç» softmax å¾—åˆ°åˆ†å¸ƒæ¦‚ç‡ï¼š
        <pre><code>p_i(s,a) = softmax(z_i)</code></pre>
  æœ€ç»ˆ Q å€¼ä¸ºæœŸæœ›ï¼š
  <div class="equation">$$Q(s,a) = \sum_i p_i(s,a)\, z_i$$</div></li>
      </ul>

      <hr>

      <h4>3. æ•°æ®ä¸ç›®æ ‡æ„å»ºï¼ˆN-Step + Double + C51 Projectionï¼‰</h4>

      <h5>3.1 N-Step æ»‘åŠ¨çª—å£æœºåˆ¶</h5>
      <p>ç»´æŠ¤é•¿åº¦ä¸º N çš„é˜Ÿåˆ— <code>n_step_buffer</code>ï¼Œæ¯æ¬¡äº¤äº’åå­˜å…¥æ ·æœ¬ï¼š</p>
      <pre><code>(s_t, a_t, r_t, s_{t+1}, done_t)</code></pre>
      <p>å½“é˜Ÿåˆ—æ»¡ N æ­¥æ—¶ï¼Œè®¡ç®—ç´¯è®¡æŠ˜æ‰£å¥–åŠ±ï¼š</p>
  <div class="equation">$$R = \sum_{i=0}^{N-1} \gamma^{i} r_{t+i}$$</div>
      <p>å¹¶ç”Ÿæˆ N-Step æ ·æœ¬ <code>(s_t, a_t, R, s_{t+N}, done_{t+N})</code> å­˜å…¥å›æ”¾æ± ã€‚</p>

      <h5>3.2 Double DQN ç›®æ ‡åŠ¨ä½œé€‰æ‹©</h5>
      <ul>
        <li>åœ¨çº¿ç½‘ç»œï¼ˆQ<sub>online</sub>ï¼‰é€‰åŠ¨ä½œï¼š  
  $$a^* = \arg\max_{a} \sum_i p_i(s',a) z_i$$</li>
        <li>ç›®æ ‡ç½‘ç»œï¼ˆQ<sub>target</sub>ï¼‰ç”¨æ¥è¯„ä¼°åˆ†å¸ƒï¼š  
        <pre><code>Z_target = Z_target(s', a*)</code></pre></li>
      </ul>

      <h5>3.3 C51 åˆ†å¸ƒå¼ Bellman æŠ•å½±</h5>
      <p>å¯¹ç›®æ ‡åˆ†å¸ƒçš„æ¯ä¸ªåŸå­ z<sub>j</sub> è®¡ç®—ä»¿å°„å¹³ç§»ï¼š</p>
  $$t_z = \operatorname{clip}(r_{acc} + \gamma^{N} z_j, V_{\min}, V_{\max})$$
      <p>å°†å…¶æŠ•å½±å›å›ºå®šæ”¯æŒç‚¹é›†åˆ <code>{z_i}</code> ä¸Šï¼ŒæŒ‰çº¿æ€§æƒé‡åˆ†æ‘Šæ¦‚ç‡è´¨é‡ï¼Œå¾—åˆ°ç›®æ ‡åˆ†å¸ƒ mã€‚</p>

      <hr>

      <h4>4. æŸå¤±å‡½æ•°ä¸ PER ä¼˜å…ˆé‡‡æ ·</h4>

      <h5>4.1 äº¤å‰ç†µä¸ KL æ•£åº¦çš„å…³ç³»</h5>

      <p>ç»™å®šç›®æ ‡åˆ†å¸ƒ <code>p</code>ï¼ˆçœŸå®ï¼‰ä¸é¢„æµ‹åˆ†å¸ƒ <code>q</code>ï¼ˆæ¨¡å‹è¾“å‡ºï¼‰ï¼Œæœ‰ï¼š</p>
  $$D_{KL}(p\Vert q) = \sum_x p(x) \log \frac{p(x)}{q(x)},\quad H(p,q) = -\sum_x p(x) \log q(x)$$
      <p>å±•å¼€åå¯å¾—ï¼š</p>
  $$H(p,q)=H(p)+D_{KL}(p\Vert q)$$
      <p>ç”±äº H(p) æ˜¯å¸¸æ•°ï¼Œæœ€å°åŒ–äº¤å‰ç†µ <code>H(p,q)</code> ç­‰ä»·äºæœ€å°åŒ– KL æ•£åº¦ <code>D_KL(p||q)</code>ã€‚</p>
      <p>å› æ­¤ï¼ŒC51 ä½¿ç”¨çš„äº¤å‰ç†µæŸå¤±ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯æœ€å°åŒ–ç›®æ ‡åˆ†å¸ƒä¸é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„ KL å·®å¼‚ã€‚</p>

      <h5>4.2 ä¸‰ç§å¸¸è§æŸå¤±å‡½æ•°è®¾è®¡æ–¹å¼</h5>

      <table>
        <tr><th>ç±»å‹</th><th>å®šä¹‰</th><th>è¯´æ˜</th></tr>
        <tr>
          <td><strong>(1) åˆ†å¸ƒäº¤å‰ç†µæŸå¤±</strong></td>
          <td>$$L = -\sum_i m_i \log p_i$$</td>
          <td>æœ€å¸¸ç”¨ï¼Œä¸ C51 åŸè®ºæ–‡ä¸€è‡´ï¼›ç›´æ¥åº¦é‡åˆ†å¸ƒå·®å¼‚ï¼Œç­‰ä»·äºæœ€å°åŒ– KL æ•£åº¦ã€‚</td>
        </tr>
        <tr>
          <td><strong>(2) æœŸæœ› TD è¯¯å·®æŸå¤±</strong></td>
          <td><code>L = (R + Î³^NÂ·Q_target - Q_online)^2</code></td>
          <td>å°†åˆ†å¸ƒå–æœŸæœ›åå›é€€åˆ°ä¼ ç»Ÿ TD å½¢å¼ï¼Œè®¡ç®—æ›´ç›´è§‚ï¼Œä½†ä¸¢å¤±åˆ†å¸ƒä¿¡æ¯ã€‚</td>
        </tr>
        <tr>
          <td><strong>(3) æ··åˆæŸå¤±</strong></td>
          <td><code>L = Î»Â·L_CE + (1âˆ’Î»)Â·L_TD</code></td>
          <td>èåˆäº¤å‰ç†µä¸ TD è¯¯å·®ä¿¡å·ï¼Œå…¼é¡¾åˆ†å¸ƒæ‹Ÿåˆä¸ç¨³å®šæ€§ï¼ˆÎ»â‰ˆ0.5 å¸¸ç”¨ï¼‰ã€‚</td>
        </tr>
      </table>

      <p>ä¸€èˆ¬æ¨èä½¿ç”¨åˆ†å¸ƒäº¤å‰ç†µæŸå¤±ï¼ˆæ–¹æ¡ˆ 1ï¼‰ï¼Œä¸ C51 ç†è®ºå®Œå…¨ä¸€è‡´ï¼›åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œä¸ºå¢å¼ºç¨³å®šæ€§å¯ä½¿ç”¨æ··åˆæŸå¤±ã€‚</p>

      <h5>4.3 PER ä¼˜å…ˆçº§è®¡ç®—ï¼ˆåˆ†å¸ƒå¼ç‰ˆæœ¬ï¼‰</h5>
      <ul>
  <li><strong>æ–¹æ³• 1ï¼š</strong> ä½¿ç”¨æ ·æœ¬äº¤å‰ç†µ $$CE_i = -\sum_i m_i \log p_i$$ ä½œä¸ºä¼˜å…ˆçº§ã€‚</li>
        <li><strong>æ–¹æ³• 2ï¼š</strong> å–åˆ†å¸ƒæœŸæœ›è®¡ç®— TD è¯¯å·® <code>|Î´| = |y - Q|</code>ã€‚</li>
        <li><strong>æ–¹æ³• 3ï¼š</strong> æ··åˆä¼˜å…ˆçº§ <code>prio_i = Î»Â·CE_i + (1âˆ’Î»)Â·|Î´|</code>ã€‚</li>
      </ul>
      <p>é‡‡æ ·æ¦‚ç‡ä¸é‡è¦æ€§æƒé‡å¦‚ä¸‹ï¼š</p>
      <pre><code>P(i) = prio_i^Î± / Î£_j prio_j^Î±
w_i = (NÂ·P(i))^-Î²
</code></pre>
      <p>å…¶ä¸­ Î±â‰ˆ0.6 æ§åˆ¶é‡‡æ ·åå¥½å¼ºåº¦ï¼ŒÎ² ä» 0.4 çº¿æ€§ä¸Šå‡è‡³ 1.0 ç”¨äºæ ¡æ­£é‡‡æ ·åå·®ã€‚</p>

      <hr>

      <h4>5. ç›®æ ‡ç½‘ç»œæ›´æ–°ä¸å™ªå£°åˆ·æ–°</h4>
      <ul>
        <li>Target ç½‘ç»œå‚æ•°æ¯ K æ­¥"ç¡¬æ›´æ–°"ï¼š<code>Î¸^- â† Î¸</code>ï¼ˆä¾‹å¦‚æ¯ 2000 æ­¥ï¼‰ã€‚</li>
        <li>æˆ–é‡‡ç”¨è½¯æ›´æ–°ï¼š<code>Î¸^- â† Ï„Î¸ + (1âˆ’Ï„)Î¸^âˆ’</code>ï¼ŒÏ„â‰ˆ1eâˆ’3ã€‚</li>
        <li>æ¯æ¬¡å‰å‘ä¼ æ’­å‰ <code>sample_noise()</code>ï¼Œä»¥åˆ·æ–°æ¢ç´¢å™ªå£°ã€‚</li>
      </ul>

      <hr>

      <h4>6. è®­ç»ƒä¸»æµç¨‹ï¼ˆä¼ªä»£ç ï¼‰</h4>

<pre><code class="language-python"># Rainbow DQN ä¸»å¾ªç¯

Initialize Q_online, Q_target â† Q_online
Initialize prioritized replay buffer

for each episode:
    s â† env.reset()
    n_step_buffer â† deque(maxlen=N)

    while not done:
        # 1. ä½¿ç”¨ NoisyNet é€‰æ‹©åŠ¨ä½œ
        a â† argmax_a E[Z(s,a;Î¸)]
        s_next, r, done â† env.step(a)
        n_step_buffer.append((s, a, r, s_next, done))

        # 2. è‹¥ n_step_buffer æ»¡ N æ­¥ï¼Œç”Ÿæˆ N-Step æ ·æœ¬
        if len(n_step_buffer) == N:
            R â† Î£ Î³^iÂ·r_{t+i}
            store_transition(s, a, R, s_{t+N}, done)

        s â† s_next

        # 3. æ¯æ­¥è®­ç»ƒ
        batch â† replay_buffer.sample(B)
        compute target distribution m
  compute loss  \(L = -\sum_i m_i \log p_i\)
        update Î¸ with weighted loss
        update priorities with per-sample loss
        periodically update Q_target
</code></pre>

      <hr>

      <h4>7. è¶…å‚æ•°å‚è€ƒ</h4>

      <table>
        <tr><th>å‚æ•°</th><th>å…¸å‹å€¼</th><th>è¯´æ˜</th></tr>
        <tr><td>æŠ˜æ‰£ Î³</td><td>0.99</td><td>é•¿æœŸå¥–åŠ±æŠ˜æ‰£</td></tr>
        <tr><td>N-Step</td><td>3</td><td>å¥–åŠ±ä¼ æ’­æ­¥æ•°</td></tr>
        <tr><td>Î±</td><td>0.6</td><td>PER é‡‡æ ·åå¥½å¼ºåº¦</td></tr>
        <tr><td>Î²</td><td>0.4 â†’ 1.0</td><td>é‡è¦æ€§æƒé‡ä¿®æ­£</td></tr>
        <tr><td>V_min, V_max</td><td>[-10, 10] æˆ– [-100, 100]</td><td>C51 æ”¯æŒåŒºé—´</td></tr>
        <tr><td>å­¦ä¹ ç‡</td><td>1eâˆ’4</td><td>Adam ä¼˜åŒ–å™¨</td></tr>
        <tr><td>æ‰¹é‡å¤§å°</td><td>32 / 64</td><td>è®­ç»ƒæ‰¹æ¬¡å¤§å°</td></tr>
      </table>

      <hr>

      <h4>8. æ€»ç»“</h4>

      <ul>
        <li><strong>Rainbow</strong> æ˜¯ DQN çš„å…¨é¢æ•´åˆç‰ˆæœ¬ï¼Œèåˆå…­å¤§æ¨¡å—ï¼š</li>
        <pre><code>Rainbow = Dueling + NoisyNet + PER + N-Step + Double + C51</code></pre>
        <li>åœ¨çº¿ç½‘ç»œé€‰åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°åˆ†å¸ƒï¼›N-Step å¥–åŠ±ä¼ æ’­æ›´å¿«ï¼›äº¤å‰ç†µæŸå¤±æœ€å°åŒ–é¢„æµ‹ä¸ç›®æ ‡åˆ†å¸ƒçš„ KL æ•£åº¦å·®è·ã€‚</li>
        <li>ç»“åˆæ‰€æœ‰æ”¹è¿›åï¼ŒRainbow åœ¨ Atari ç³»åˆ—ç¯å¢ƒä¸­å¤§å¹…è¶…è¶ŠåŸå§‹ DQN ä¸å„ç‹¬ç«‹å˜ä½“ï¼Œæˆä¸ºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é‡è¦åŸºçº¿ç®—æ³•ã€‚</li>
      </ul>
    </section>

    <section id="chapter18" class="chapter">
      <h2>ç¬¬åå…«ç« ï¼šDPG & DDPGï¼ˆä»ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦åˆ°æ·±åº¦å®ç°ï¼‰</h2>

      <p><strong> æ·±å…¥ç†è§£ DPGï¼ˆDeterministic Policy Gradientï¼‰ï¼šä»éšæœºåˆ°ç¡®å®šæ€§ç­–ç•¥çš„æ¡¥æ¢</strong></p>

      <p>å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆPolicy Gradientï¼‰å®¶æ—ä¸­ï¼Œä»æœ€æ—©çš„ REINFORCE åˆ° Actor-Criticï¼ˆACï¼‰ã€A2C/A3C å†åˆ° DPG/DDPG/TD3/SACï¼Œå…¶æ ¸å¿ƒæ€æƒ³ä¸€ç›´å›´ç»•ä¸€ä¸ªé—®é¢˜ï¼š</p>
      <blockquote>

      <p><strong>å¦‚ä½•ç›´æ¥ä¼˜åŒ–ä¸€ä¸ªå‚æ•°åŒ–çš„ç­–ç•¥ï¼Œä½¿å¾—é•¿æœŸå›æŠ¥æœ€å¤§ï¼Ÿ</strong></p>
      </blockquote>

      <p>DPGï¼ˆDeterministic Policy Gradientï¼‰æ˜¯å…¶ä¸­çš„é‡è¦åˆ†æ”¯ï¼Œå®ƒé€šè¿‡å°†éšæœºç­–ç•¥ç®€åŒ–ä¸ºç¡®å®šæ€§å‡½æ•°ï¼Œå¤§å¹…é™ä½æ–¹å·®ã€æé«˜å­¦ä¹ æ•ˆç‡ï¼Œæˆä¸ºè¿ç»­åŠ¨ä½œæ§åˆ¶ä»»åŠ¡ï¼ˆå¦‚æœºæ¢°è‡‚ã€æ— äººé©¾é©¶ã€ä»¿çœŸæ§åˆ¶ç­‰ï¼‰çš„å…³é”®ç®—æ³•ã€‚</p>

      <hr>

      <h3>ä¸€ã€ä»éšæœºåˆ°ç¡®å®šæ€§ï¼šDPG çš„è¯ç”ŸåŠ¨æœº</h3>

      <p>åœ¨ç»å…¸çš„ Actor-Critic (AC) ç®—æ³•ä¸­ï¼Œç­–ç•¥æ˜¯<strong>éšæœºçš„</strong>ï¼š</p>
      <pre><code>Ï€_Î¸(a|s)</code></pre>
      <p>ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šçŠ¶æ€ $s$ï¼ŒActor è¾“å‡ºä¸€ä¸ªåŠ¨ä½œåˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯é«˜æ–¯åˆ†å¸ƒï¼‰ï¼Œå†ä»ä¸­é‡‡æ ·åŠ¨ä½œ $a$ã€‚</p>

      <p>è¿™æ ·çš„å¥½å¤„æ˜¯â€”â€”å¯ä»¥è‡ªç„¶åœ°å®ç°æ¢ç´¢ã€‚ä½†é—®é¢˜ä¹Ÿå¾ˆæ˜æ˜¾ï¼š</p>
      <ul>
        <li><strong>æ–¹å·®é«˜</strong>ï¼šæ¢¯åº¦ä¼°è®¡ä¾èµ–é‡‡æ ·ï¼Œæ›´æ–°æ–¹å‘å™ªå£°å¤§ï¼›</li>
        <li><strong>æ•ˆç‡ä½</strong>ï¼šè¿ç»­åŠ¨ä½œç©ºé—´ä¸‹çš„ç§¯åˆ†æéš¾ä¼°è®¡ï¼›</li>
        <li><strong>ä¸ç¨³å®š</strong>ï¼šåŠ¨ä½œé‡‡æ ·å¼•å…¥çš„éšæœºæ€§å¢åŠ äº†ç­–ç•¥å­¦ä¹ çš„ä¸ç¡®å®šæ€§ã€‚</li>
      </ul>

      <p>äºæ˜¯ï¼ŒSilver ç­‰äººåœ¨ 2014 å¹´æå‡ºäº† <strong>DPG</strong>ï¼ˆè®ºæ–‡ï¼š<em>Deterministic Policy Gradient Algorithms</em>, ICML 2014ï¼‰ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š</p>
      <blockquote>
        <p>åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è®©ç­–ç•¥ç›´æ¥è¾“å‡ºä¸€ä¸ª<strong>ç¡®å®šæ€§çš„åŠ¨ä½œ</strong> $a = \mu_\theta(s)$ï¼Œè€Œä¸æ˜¯åŠ¨ä½œåˆ†å¸ƒã€‚</p>
      </blockquote>

      <hr>

      <h3>äºŒã€DPG å®šç†ï¼ˆDeterministic Policy Gradient Theoremï¼‰</h3>

      <p>DPG ç†è®ºçš„æ ¹åŸºå°±æ˜¯è¿™ä¸ªå®šç†ï¼š</p>

      <div style="background: rgba(255, 140, 66, 0.1); padding: 15px; border-left: 4px solid #ff8c42; margin: 20px 0;">
        <p><strong>Theorem (Deterministic Policy Gradient Theorem)</strong></p>
        <p>è®¾ç­–ç•¥ä¸ºç¡®å®šæ€§å‡½æ•° $\mu_\theta(s)$ï¼Œå…¶æ€§èƒ½ç›®æ ‡ä¸ºï¼š</p>
        <p>$$
        J(\mu_\theta) = \mathbb{E}_{s \sim \rho^\mu}[Q^\mu(s, \mu_\theta(s))]
        $$</p>
        <p>å…¶ä¸­ $\rho^\mu$ æ˜¯ç­–ç•¥ $\mu$ ä¸‹çš„æŠ˜æ‰£çŠ¶æ€åˆ†å¸ƒã€‚</p>
        <p>åˆ™åœ¨æ»¡è¶³å¯å¯¼æ¡ä»¶ä¸‹ï¼Œæœ‰ï¼š</p>
        <p>$$
        \nabla_\theta J(\mu_\theta) = \mathbb{E}_{s \sim \rho^\beta}\left[\nabla_\theta \mu_\theta(s) \, \nabla_a Q^\mu(s,a)\Big|_{a=\mu_\theta(s)}\right]
        $$</p>
        <p>å…¶ä¸­ $\rho^\beta$ ä¸ºä»»æ„è¡Œä¸ºç­–ç•¥çš„çŠ¶æ€åˆ†å¸ƒï¼Œå› æ­¤è¯¥æ¢¯åº¦å¯<strong>ç¦»ç­–ç•¥ï¼ˆoff-policyï¼‰</strong>ä¼°è®¡ã€‚</p>
      </div>

      <p><em>ï¼ˆè¯¦ç»†è¯æ˜è§è®ºæ–‡ï¼šSilver et al., 2014, ICMLï¼‰</em></p>

      <h4>ğŸ“˜ å®šç†è¦ç‚¹è§£è¯»</h4>
      <ul>
        <li>ä¸å†éœ€è¦ $\log \pi_\theta(a|s)$ï¼›</li>
        <li>æ¢¯åº¦é€šè¿‡é“¾å¼æ³•åˆ™ç›´æ¥è®¡ç®—ï¼š$\nabla_\theta \mu_\theta(s) \times \nabla_a Q(s,a)$ï¼›</li>
        <li>ä¸éœ€è¦å¯¹åŠ¨ä½œç©ºé—´ç§¯åˆ†ï¼›</li>
        <li>è®¡ç®—æ–¹å·®å¤§å¤§é™ä½ï¼›</li>
        <li>å¯ä»¥ç¦»ç­–ç•¥è®­ç»ƒï¼ˆä½¿ç”¨ç»éªŒå›æ”¾ï¼‰ã€‚</li>
      </ul>

      <hr>

      <h3>ä¸‰ã€ä¸ºä»€ä¹ˆå¯ä»¥ç¡®å®šæ€§è¾“å‡º Î¼(s)ï¼Ÿ</h3>

  <p>åœ¨éšæœºç­–ç•¥ä¸‹ï¼š</p>
  $$a \sim \pi_\theta(a\mid s)$$

      <p>åœ¨è¿ç»­åŠ¨ä½œä»»åŠ¡ä¸­ï¼Œé€šå¸¸å‡è®¾ $\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma^2 I)$ã€‚</p>
  <p>å¦‚æœæˆ‘ä»¬è®© $\sigma \rightarrow 0$ï¼Œåˆ™åˆ†å¸ƒæ”¶æ•›ä¸º $\delta$ åˆ†å¸ƒï¼š</p>
  $$\pi_\theta(a\mid s) \;\Rightarrow\; \delta\big(a - \mu_\theta(s)\big)$$

  <p>æ­¤æ—¶æ¢¯åº¦ä»</p>
  $$\nabla_\theta J = \mathbb{E}\big[\nabla_\theta \log \pi_\theta(a\mid s)\; Q(s,a)\big]$$
  <p>è‡ªç„¶è¿‡æ¸¡åˆ°</p>
  $$\nabla_\theta J = \mathbb{E}\big[\nabla_\theta \mu_\theta(s)\; \nabla_a Q(s,a)\big]$$

      <p>è¿™æ­£æ˜¯ DPG çš„æ¢¯åº¦å½¢å¼ã€‚</p>

      <p><strong>å› æ­¤ï¼šDPG æ˜¯éšæœºç­–ç•¥æ¢¯åº¦çš„é›¶æ–¹å·®æé™å½¢å¼ã€‚</strong></p>

      <hr>

      <h3>å››ã€ä¸ºä»€ä¹ˆ DPG å¯ä»¥ off-policyï¼Ÿ</h3>

      <p>DPG ä¹‹æ‰€ä»¥å¯ä»¥ off-policyï¼Œæ˜¯å› ä¸ºå®ƒçš„æ¢¯åº¦å½¢å¼<strong>ä¸ä¾èµ–åŠ¨ä½œåˆ†å¸ƒ</strong>ï¼Œåªä¾èµ–çŠ¶æ€åˆ†å¸ƒï¼›è€ŒçŠ¶æ€åˆ†å¸ƒçš„åå·®å¯ä»¥ç”¨ç»éªŒå›æ”¾è¿‘ä¼¼è¡¥é½ã€‚</p>

      <p>å…·ä½“æ¥è¯´ï¼š</p>
      <ul>
        <li>éšæœºç­–ç•¥æ¢¯åº¦ä¾èµ– $\pi(a|s)$ï¼Œå¿…é¡»æ˜¯å½“å‰ç­–ç•¥é‡‡æ ·çš„æ•°æ®ï¼›</li>
        <li>ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦åªä¾èµ– $Q(s, \mu(s))$ çš„æ¢¯åº¦æ–¹å‘ï¼Œä¸é‡‡æ ·ç­–ç•¥æ— å…³ï¼›</li>
        <li>åªè¦çŠ¶æ€åˆ†å¸ƒ $\rho^\beta$ è¦†ç›–äº† $\rho^\mu$ï¼Œæ¢¯åº¦ä¼°è®¡å°±æ˜¯æ— åçš„ï¼›</li>
        <li>ç»éªŒå›æ”¾æ± ä¸­çš„å†å²æ•°æ®å¯ä»¥è¿‘ä¼¼æä¾›è¿™ç§è¦†ç›–ã€‚</li>
      </ul>

      <hr>

      <h3>äº”ã€ä»ç†è®ºä¸Šå¦‚ä½•éªŒè¯"ç¡®å®šæ€§ç­–ç•¥å‡è®¾"æ˜¯åˆç†çš„ï¼Ÿ</h3>

      <p>ä½ å¯èƒ½ä¼šé—®ï¼š<strong>ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯ä»¥æŠŠç­–ç•¥"é™å®š"ä¸ºç¡®å®šæ€§å‡½æ•° $\mu_\theta(s)$ï¼Ÿè¿™ä¸ªå‡è®¾æ˜¯å¦ä¸¢å¤±äº†æœ€ä¼˜è§£ï¼Ÿ</strong></p>

      <p>éªŒè¯å®ƒ"æœ‰ç”¨"ä¸”"æ­£ç¡®"ï¼Œå…¶å®è¦çœ‹ä¸‰å±‚é€»è¾‘ï¼š</p>

      <h4>å±‚æ¬¡ â‘  å­˜åœ¨æ€§å±‚é¢ï¼šæœ€ä¼˜ç­–ç•¥æ˜¯å¦å¯ä»¥æ˜¯ç¡®å®šæ€§çš„ï¼Ÿ</h4>

      <div style="background: rgba(76, 175, 80, 0.1); padding: 15px; border-left: 4px solid #4CAF50; margin: 20px 0;">
        <p><strong>ç»“è®ºï¼šâœ… æ˜¯çš„ï¼ˆç”± Bellman æœ€ä¼˜æ€§åŸç†ä¿è¯ï¼‰</strong></p>
        <p><strong>ç†è®ºä¾æ®ï¼š</strong></p>
        <p>åœ¨ MDP æ¡†æ¶ä¸‹ï¼Œå¯¹äºä»»ä½•éšæœºç­–ç•¥ $\pi(a|s)$ï¼Œæ€»å­˜åœ¨ä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥ $\mu(s)$ ä½¿å¾—ï¼š</p>
        <p>$$
        V^{\mu}(s) \geq V^{\pi}(s), \quad \forall s
        $$</p>
        <p>è¿™æ˜¯å› ä¸ºæœ€ä¼˜ç­–ç•¥å¯ä»¥é€šè¿‡ Bellman æœ€ä¼˜æ–¹ç¨‹ç›´æ¥æ„é€ ï¼š</p>
        <p>$$
        \mu^*(s) = \arg\max_a Q^*(s, a)
        $$</p>
        <p>ä¹Ÿå°±æ˜¯è¯´ï¼Œ<strong>è‡³å°‘å­˜åœ¨ä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥æ˜¯æœ€ä¼˜çš„</strong>ã€‚</p>
      </div>

      <p><strong>ç›´è§‰è§£é‡Šï¼š</strong></p>
      <ul>
        <li>å¦‚æœæŸä¸ªçŠ¶æ€ä¸‹ï¼ŒåŠ¨ä½œ $a_1$ çš„ Q å€¼æœ€é«˜ï¼Œé‚£ä¹ˆæ€»æ˜¯é€‰ $a_1$ å°±æ˜¯æœ€ä¼˜çš„ï¼›</li>
        <li>æ²¡æœ‰å¿…è¦æŒ‰æ¦‚ç‡"æ·éª°å­"åœ¨ $a_1$ å’Œæ¬¡ä¼˜åŠ¨ä½œä¹‹é—´éšæœºé€‰æ‹©ï¼›</li>
        <li>éšæœºæ€§åªä¼šé™ä½æœŸæœ›å›æŠ¥ï¼ˆé™¤éæ˜¯ä¸ºäº†æ¢ç´¢ï¼‰ã€‚</li>
      </ul>

      <h4>å±‚æ¬¡ â‘¡ å¯ä¼˜åŒ–æ€§å±‚é¢ï¼šåœ¨ç¡®å®šæ€§ç­–ç•¥ç±»ä¸­ï¼Œæ€§èƒ½ç›®æ ‡ $J(\mu_\theta)$ æ˜¯å¦å¯å¾®ã€å¯ä¼˜åŒ–ï¼Ÿ</h4>

      <div style="background: rgba(76, 175, 80, 0.1); padding: 15px; border-left: 4px solid #4CAF50; margin: 20px 0;">
        <p><strong>ç»“è®ºï¼šâœ… æ˜¯çš„ï¼ˆDPG å®šç†ç»™å‡ºæ¢¯åº¦è¡¨è¾¾å¼ï¼‰</strong></p>
        <p><strong>ç†è®ºä¾æ®ï¼š</strong></p>
        <p>Silver et al. (2014) è¯æ˜äº†ï¼Œåœ¨ç¡®å®šæ€§ç­–ç•¥ä¸‹ï¼Œæ€§èƒ½ç›®æ ‡å…³äºå‚æ•° $\theta$ çš„æ¢¯åº¦ä¸ºï¼š</p>
        <p>$$
        \nabla_\theta J(\mu_\theta) = \mathbb{E}_{s \sim \rho}\left[\nabla_\theta \mu_\theta(s) \, \nabla_a Q(s,a)\Big|_{a=\mu_\theta(s)}\right]
        $$</p>
        <p>è¿™ä¸ªæ¢¯åº¦ï¼š</p>
        <ul>
          <li>âœ… <strong>å¯è®¡ç®—</strong>ï¼šé€šè¿‡é“¾å¼æ³•åˆ™ä»ç¥ç»ç½‘ç»œåå‘ä¼ æ’­ï¼›</li>
          <li>âœ… <strong>æ— å</strong>ï¼šæœŸæœ›ä¸çœŸå®æ¢¯åº¦æ–¹å‘ä¸€è‡´ï¼›</li>
          <li>âœ… <strong>ä½æ–¹å·®</strong>ï¼šä¸éœ€è¦é‡‡æ ·åŠ¨ä½œï¼Œæ–¹å·®è¿œä½äºéšæœºç­–ç•¥æ¢¯åº¦ã€‚</li>
        </ul>
      </div>

      <p><strong>ç›´è§‰è§£é‡Šï¼š</strong></p>
      <ul>
        <li>æˆ‘ä»¬å¯ä»¥æŠŠ $J(\mu_\theta)$ çœ‹ä½œæ˜¯ä¸€ä¸ªå…³äº $\theta$ çš„å‡½æ•°ï¼›</li>
        <li>åªè¦ $\mu_\theta(s)$ å’Œ $Q(s,a)$ å¯å¾®ï¼Œ$J$ å°±å¯å¾®ï¼›</li>
        <li>å› æ­¤å¯ä»¥ç”¨æ¢¯åº¦ä¸Šå‡æ³•ä¼˜åŒ–å®ƒã€‚</li>
      </ul>

      <h4>å±‚æ¬¡ â‘¢ é€¼è¿‘æ€§å±‚é¢ï¼šå‚æ•°åŒ–å‡½æ•° $\mu_\theta$ æ˜¯å¦è¶³å¤Ÿè¡¨è¾¾ä¸°å¯Œç­–ç•¥ï¼Ÿ</h4>

      <div style="background: rgba(76, 175, 80, 0.1); padding: 15px; border-left: 4px solid #4CAF50; margin: 20px 0;">
        <p><strong>ç»“è®ºï¼šâœ… è‹¥ç¥ç»ç½‘ç»œè¶³å¤Ÿå¤§/éçº¿æ€§è¶³å¤Ÿå¼ºï¼Œåˆ™å¯é€¼è¿‘ä»»æ„ç¡®å®šæ€§ç­–ç•¥</strong></p>
        <p><strong>ç†è®ºä¾æ®ï¼š</strong></p>
        <p>æ ¹æ®<strong>ä¸‡èƒ½é€¼è¿‘å®šç†ï¼ˆUniversal Approximation Theoremï¼‰</strong>ï¼š</p>
        <p>ä¸€ä¸ªè¶³å¤Ÿå®½çš„å•å±‚ç¥ç»ç½‘ç»œï¼ˆæˆ–è¶³å¤Ÿæ·±çš„å¤šå±‚ç½‘ç»œï¼‰å¯ä»¥é€¼è¿‘ä»»æ„è¿ç»­å‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}^m$ã€‚</p>
        <p>å› æ­¤ï¼Œå¯¹äºä»»æ„ç¡®å®šæ€§ç­–ç•¥ $\mu^*(s)$ï¼Œåªè¦å®ƒæ˜¯è¿ç»­çš„ï¼ˆæˆ–åˆ†æ®µè¿ç»­ï¼‰ï¼Œå°±å­˜åœ¨å‚æ•° $\theta$ ä½¿å¾—ï¼š</p>
        <p>$$
        \mu_\theta(s) \approx \mu^*(s)
        $$</p>
      </div>

      <p><strong>ç›´è§‰è§£é‡Šï¼š</strong></p>
      <ul>
        <li>ç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ª"å‡½æ•°é€¼è¿‘å™¨"ï¼›</li>
        <li>åªè¦ç½‘ç»œå¤Ÿå¤§ã€è®­ç»ƒè¶³å¤Ÿï¼Œå®ƒå¯ä»¥å­¦åˆ°å¤æ‚çš„ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼›</li>
        <li>å®è·µä¸­ï¼Œè¿™ä¸ªå‡è®¾åœ¨é«˜ç»´è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸Šè¡¨ç°å¾ˆå¥½ï¼ˆå¦‚ MuJoCoã€æœºå™¨äººæ§åˆ¶ï¼‰ã€‚</li>
      </ul>

      <hr>

      <h4>ğŸ“Š ä¸‰å±‚éªŒè¯æ€»ç»“è¡¨</h4>

      <table>
        <tr><th>å±‚æ¬¡</th><th>æ£€éªŒå†…å®¹</th><th>ç»“è®º</th></tr>
        <tr>
          <td><strong>â‘  å­˜åœ¨æ€§å±‚é¢</strong></td>
          <td>æœ€ä¼˜ç­–ç•¥æ˜¯å¦å¯ä»¥æ˜¯ç¡®å®šæ€§çš„ï¼Ÿ</td>
          <td>âœ… æ˜¯ï¼ˆç”± Bellman åŸç†ä¿è¯ï¼‰</td>
        </tr>
        <tr>
          <td><strong>â‘¡ å¯ä¼˜åŒ–æ€§å±‚é¢</strong></td>
          <td>åœ¨ç¡®å®šæ€§ç­–ç•¥ç±»ä¸­ï¼Œæ€§èƒ½ç›®æ ‡ $J(\mu_\theta)$ æ˜¯å¦å¯å¾®ã€å¯ä¼˜åŒ–ï¼Ÿ</td>
          <td>âœ… æ˜¯ï¼ˆDPG å®šç†ç»™å‡ºæ¢¯åº¦è¡¨è¾¾å¼ï¼‰</td>
        </tr>
        <tr>
          <td><strong>â‘¢ é€¼è¿‘æ€§å±‚é¢</strong></td>
          <td>å‚æ•°åŒ–å‡½æ•° $\mu_\theta$ æ˜¯å¦è¶³å¤Ÿè¡¨è¾¾ä¸°å¯Œç­–ç•¥ï¼Ÿ</td>
          <td>âœ… è‹¥ç¥ç»ç½‘ç»œè¶³å¤Ÿå¤§/éçº¿æ€§è¶³å¤Ÿå¼ºï¼Œåˆ™å¯é€¼è¿‘ä»»æ„ç¡®å®šæ€§ç­–ç•¥</td>
        </tr>
      </table>

      <p><strong>ç»“è®ºï¼šç¡®å®šæ€§ç­–ç•¥å‡è®¾æ˜¯ç†è®ºä¸Šåˆç†ã€å®è·µä¸Šå¯è¡Œçš„ã€‚</strong></p>

      <p>è¿™ä¸‰å±‚éªŒè¯å‘Šè¯‰æˆ‘ä»¬ï¼š</p>
      <ul>
        <li>æˆ‘ä»¬<strong>æ²¡æœ‰ä¸¢å¤±æœ€ä¼˜è§£</strong>ï¼ˆè‡³å°‘å­˜åœ¨ä¸€ä¸ªç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥ï¼‰ï¼›</li>
        <li>æˆ‘ä»¬<strong>å¯ä»¥æ‰¾åˆ°å®ƒ</strong>ï¼ˆæ¢¯åº¦å¯è®¡ç®—ã€æ–¹å‘æ­£ç¡®ï¼‰ï¼›</li>
        <li>æˆ‘ä»¬<strong>å¯ä»¥ç”¨ç¥ç»ç½‘ç»œè¡¨ç¤ºå®ƒ</strong>ï¼ˆä¸‡èƒ½é€¼è¿‘å®šç†ä¿è¯ï¼‰ã€‚</li>
      </ul>

      <hr>

      <h3>å…­ã€DPG ä¸ AC çš„å¯¹æ¯”</h3>

      <table>
        <tr><th>ç‰¹å¾</th><th>AC / A2C / A3C</th><th>DPG / DDPG</th></tr>
        <tr><td>ç­–ç•¥ç±»å‹</td><td>éšæœºç­–ç•¥ $\pi(a|s)$</td><td>ç¡®å®šæ€§ç­–ç•¥ $\mu(s)$</td></tr>
        <tr><td>åŠ¨ä½œé€‰æ‹©</td><td>é‡‡æ · $a \sim \pi(a|s)$</td><td>ç›´æ¥è¾“å‡º $a = \mu(s)$</td></tr>
        <tr><td>æ›´æ–°å½¢å¼</td><td>$\nabla_\theta \log \pi(a|s) A(s,a)$</td><td>$\nabla_\theta \mu(s) \nabla_a Q(s,a)$</td></tr>
        <tr><td>æ–¹å·®</td><td>é«˜</td><td>ä½</td></tr>
        <tr><td>æ•°æ®ç±»å‹</td><td>on-policy</td><td>off-policy</td></tr>
        <tr><td>æ–¹å·®æŠ‘åˆ¶</td><td>å€¼å‡½æ•°åŸºçº¿</td><td>ç›®æ ‡ç½‘ç»œ + å›æ”¾æ± </td></tr>
        <tr><td>æ¢ç´¢æ¥æº</td><td>ç­–ç•¥é‡‡æ ·</td><td>å¤–éƒ¨å™ªå£°</td></tr>
        <tr><td>ç¨³å®šæœºåˆ¶</td><td>Advantage + å¤šçº¿ç¨‹</td><td>Target Net + Replay Buffer</td></tr>
        <tr><td>å…¸å‹ä»»åŠ¡</td><td>ç¦»æ•£/ä½ç»´è¿ç»­</td><td>é«˜ç»´è¿ç»­æ§åˆ¶</td></tr>
        <tr><td>æ ¸å¿ƒç†è®º</td><td>éšæœºç­–ç•¥æ¢¯åº¦å®šç†</td><td>ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦å®šç†</td></tr>
      </table>

      <hr>

      <h3>ä¸ƒã€DDPG ç®—æ³•æµç¨‹ï¼ˆå·¥ç¨‹ç‰ˆï¼‰</h3>

      <p>DPG ç†è®ºå¾ˆä¼˜é›…ï¼Œä½†åœ¨å®è·µä¸­é€šå¸¸ä¸ DQN çš„ç¨³å®šåŒ–æŠ€å·§ç»“åˆï¼Œå½¢æˆ <strong>DDPGï¼ˆDeep DPGï¼‰</strong>ã€‚ä»¥ä¸‹æµç¨‹æ˜¯å¸¦ç›®æ ‡ç½‘ç»œå’Œå›æ”¾æ± çš„å¯ç”¨ç‰ˆã€‚</p>

      <h4>0ï¸âƒ£ åˆå§‹åŒ–</h4>
      <ul>
        <li>éšæœºåˆå§‹åŒ– Actor $\mu_\theta$ã€Critic $Q_\phi$</li>
        <li>å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œ $\mu_{\theta'}, Q_{\phi'}$</li>
        <li>å»ºç«‹ç»éªŒå›æ”¾æ±  $\mathcal{D}$</li>
        <li>è®¾å®šè¶…å‚æ•°ï¼šå­¦ä¹ ç‡ã€$\gamma$ã€$\tau$ã€æ‰¹å¤§å°ã€å™ªå£°æ ‡å‡†å·®ç­‰</li>
      </ul>

      <h4>1ï¸âƒ£ äº¤äº’é‡‡æ ·ï¼ˆå¸¦æ¢ç´¢å™ªå£°ï¼‰</h4>
      <p>åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼š</p>
  $$a_t = \operatorname{clip}\big(\mu_\theta(s_t) + \epsilon_t,\; \text{bounds}\big),\quad \epsilon_t \sim \mathcal{N}(0,\, \sigma_{\text{explore}}^{2} I)$$
      <p>æ‰§è¡Œ $a_t$ï¼Œæ”¶é›† $(s_t, a_t, r_t, s_{t+1}, d_t)$ï¼ŒåŠ å…¥å›æ”¾æ± ã€‚</p>

      <h4>2ï¸âƒ£ é‡‡æ ·æ‰¹æ¬¡</h4>
      <p>ä» $\mathcal{D}$ ä¸­éšæœºé‡‡æ · $N$ æ¡ç»éªŒï¼š</p>
  $$(s_i, a_i, r_i, s'_i, d_i)$$

      <h4>3ï¸âƒ£ Critic æ›´æ–°</h4>
  <p>ç›®æ ‡å€¼ï¼ˆTD ç›®æ ‡ï¼‰ï¼š</p>
  $$y_i = r_i + \gamma\,(1-d_i)\, Q_{\phi'}\!\big(s'_i,\, \mu_{\theta'}(s'_i)\big)$$
  <p>Critic æŸå¤±ï¼š</p>
  $$L_Q = \frac{1}{N} \sum_i \big(Q_{\phi}(s_i,a_i) - y_i\big)^2$$
  <p>ä¼˜åŒ–ï¼š</p>
  <div class="equation">$$\phi \leftarrow \phi - \eta_Q \, \nabla_{\phi} L_Q$$</div>

      <h4>4ï¸âƒ£ Actor æ›´æ–°ï¼ˆç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼‰</h4>
  $$\nabla_{\theta} J \;\approx\; \frac{1}{N} \sum_i \nabla_{\theta} \mu_{\theta}(s_i)\; \nabla_{a} Q_{\phi}(s_i, a)\big|_{a=\mu_{\theta}(s_i)}$$
  <p>é€šå¸¸é€šè¿‡æœ€å°åŒ–ï¼š</p>
  $$L_\pi = -\frac{1}{N} \sum_i Q_{\phi}\big(s_i, \mu_{\theta}(s_i)\big)$$
  <p>ä¼˜åŒ–ï¼š</p>
  $$\theta \leftarrow \theta - \eta_\pi \, \nabla_{\theta} L_\pi$$

      <h4>5ï¸âƒ£ ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°</h4>
  $$\theta' \leftarrow \tau\,\theta + (1-\tau)\,\theta',\quad \phi' \leftarrow \tau\,\phi + (1-\tau)\,\phi'$$

      <h4>6ï¸âƒ£ é‡å¤ç›´åˆ°æ”¶æ•›</h4>
      <p>å®šæœŸåœ¨æ— å™ªå£°æ¡ä»¶ä¸‹è¯„ä¼°ç­–ç•¥æ€§èƒ½ã€‚</p>

      <hr>

      <h3>å…«ã€DDPG ä¼ªä»£ç </h3>

<pre><code class="language-python"># DDPG ç®—æ³•ä¼ªä»£ç 

Initialize Actor Î¼_Î¸, Critic Q_Ï†
Initialize target networks Î¼_Î¸' â† Î¼_Î¸, Q_Ï†' â† Q_Ï†
Initialize replay buffer D
Set hyperparameters: Î³, Ï„, Ïƒ, batch_size, max_episodes

for episode = 1 to max_episodes:
    s â† env.reset()
    done â† False
    
    while not done:
        # 1. é€‰æ‹©åŠ¨ä½œï¼ˆå¸¦æ¢ç´¢å™ªå£°ï¼‰
        a â† clip(Î¼_Î¸(s) + Îµ),  Îµ âˆ¼ N(0, ÏƒÂ²)
        
        # 2. æ‰§è¡ŒåŠ¨ä½œ
        s', r, done â† env.step(a)
        
        # 3. å­˜å‚¨ç»éªŒ
        D.store((s, a, r, s', done))
        
        # 4. é‡‡æ ·æ‰¹æ¬¡è®­ç»ƒ
        if |D| â‰¥ batch_size:
            batch â† D.sample(batch_size)
            
            # 5. è®¡ç®— Critic ç›®æ ‡
            for (s_i, a_i, r_i, s'_i, d_i) in batch:
                y_i â† r_i + Î³(1 - d_i) * Q_Ï†'ï¼ˆs'_i, Î¼_Î¸'ï¼ˆs'_iï¼‰)
            
            # 6. æ›´æ–° Critic
            L_Q â† mean((Q_Ï†(s_i, a_i) - y_i)Â²)
            Ï† â† Ï† - Î·_Q * âˆ‡_Ï† L_Q
            
            # 7. æ›´æ–° Actor
            L_Ï€ â† -mean(Q_Ï†(s_i, Î¼_Î¸(s_i)))
            Î¸ â† Î¸ - Î·_Ï€ * âˆ‡_Î¸ L_Ï€
            
            # 8. è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            Î¸' â† Ï„Î¸ + (1-Ï„)Î¸'
            Ï†' â† Ï„Ï† + (1-Ï„)Ï†'
        
        s â† s'
</code></pre>

      <hr>

      <h3>ä¹ã€ç›´è§‰æ€»ç»“</h3>

      <p><strong>åœ¨ AC ä¸­ï¼š</strong></p>
      <ul>
        <li>Actor å­¦ä¹ ä¸€ä¸ªåˆ†å¸ƒï¼ŒCritic è¯„ä¼°åŠ¨ä½œçš„å¥½åã€‚</li>
        <li>Actor æ›´æ–°ä¾èµ– $\log \pi$ï¼Œå­˜åœ¨é‡‡æ ·æ–¹å·®ã€‚</li>
      </ul>

      <p><strong>åœ¨ DPG ä¸­ï¼š</strong></p>
      <ul>
        <li>Actor ç›´æ¥ç»™å‡ºåŠ¨ä½œï¼ŒCritic é€šè¿‡ $Q(s,a)$ æ›²é¢å‘Šè¯‰ Actor å“ªä¸ªæ–¹å‘æ›´å¥½ã€‚</li>
        <li>Actor é¡ºç€ $\nabla_a Q$ çš„ä¸Šå‡æ–¹å‘æ›´æ–°ï¼Œåƒæ˜¯åœ¨çˆ¬ä»·å€¼å‡½æ•°çš„"å±±ä¸˜"ã€‚</li>
      </ul>

      <blockquote>
        <p><strong>ä¸€å¥è¯è®°ä½ï¼š</strong></p>
        <p>AC åœ¨æ·éª°å­å­¦ç­–ç•¥ï¼›<br>
        DPG ç›´æ¥çˆ¬å±±æ‰¾æœ€ä¼˜ï¼›<br>
        ä¸¤è€…çš„åŒºåˆ«ï¼Œå°±æ˜¯ä»"æœŸæœ›æ„ä¹‰çš„éšæœºä¸Šå‡"å˜ä¸º"ç¡®å®šæ€§æ–¹å‘çš„ç²¾ç¡®ä¸Šå‡"ã€‚</p>
      </blockquote>

      <hr>

      <h3>åã€å¸¸è§é—®é¢˜ä¸æ‰©å±•</h3>

      <table>
        <tr><th>é—®é¢˜</th><th>è§£å†³æ–¹å¼</th></tr>
        <tr><td>è¿‡ä¼°è®¡ Q</td><td>ç”¨ TD3ï¼šåŒ Q ç½‘ç»œå–æœ€å°å€¼</td></tr>
        <tr><td>æ¢ç´¢ä¸è¶³</td><td>è°ƒæ•´å™ªå£°å¼ºåº¦æˆ–ä½¿ç”¨å‚æ•°å™ªå£°</td></tr>
        <tr><td>å‘æ•£</td><td>é™ä½å­¦ä¹ ç‡ã€ä½¿ç”¨ç›®æ ‡å¹³æ»‘</td></tr>
        <tr><td>è®­ç»ƒæ…¢</td><td>å¢å¤§æ‰¹å¤§å°ã€å½’ä¸€åŒ–çŠ¶æ€/å¥–åŠ±</td></tr>
      </table>

      <hr>

      <h3>åä¸€ã€æ€»ç»“</h3>

      <table>
        <tr><th>æ¨¡å—</th><th>å˜åŒ–è¦ç‚¹</th></tr>
        <tr><td>ç­–ç•¥å½¢å¼</td><td>ä»éšæœºåˆ†å¸ƒ â†’ ç¡®å®šæ€§å‡½æ•°</td></tr>
        <tr><td>å­¦ä¹ ä¿¡å·</td><td>ä» log Ï€ â†’ é“¾å¼æ³•åˆ™ (Î¼, Q)</td></tr>
        <tr><td>ç¨³å®šæ€§æ¥æº</td><td>ä» Advantage â†’ ç›®æ ‡ç½‘ç»œ/å›æ”¾æ± </td></tr>
        <tr><td>æ•°æ®ç±»å‹</td><td>ä» on-policy â†’ off-policy</td></tr>
        <tr><td>æ ¸å¿ƒä¼˜ç‚¹</td><td>ä½æ–¹å·®ã€é«˜æ•°æ®æ•ˆç‡ã€é€‚åˆè¿ç»­æ§åˆ¶</td></tr>
        <tr><td>ç†è®ºæ”¯æ’‘</td><td>Deterministic Policy Gradient Theorem (Silver et al., 2014)</td></tr>
      </table>

      <p><strong> ç»“è¯­</strong></p>
      <p>DPG æ˜¯"ä»éšæœºåˆ°ç¡®å®šæ€§"çš„é‡è¦æ¡¥æ¢ã€‚å®ƒè®©å¼ºåŒ–å­¦ä¹ åœ¨è¿ç»­æ§åˆ¶é¢†åŸŸæ‹¥æœ‰äº†å®ç”¨å¯è¡Œçš„è·¯å¾„ï¼Œå¹¶æˆä¸ºåç»­ DDPGã€TD3ã€SAC ç­‰ç®—æ³•çš„ç†è®ºåŸºç¡€ã€‚</p>
    </section>

    <section id="chapter19" class="chapter">
      <h2>ç¬¬åä¹ç« ï¼šTD3ï¼ˆTwin Delayed DDPGï¼‰</h2>
      <h3>0. ä¸€å¥è¯ä¸å®šä½</h3>
      <p><strong>TD3 = DDPG çš„ä¸‰ä»¶å¥—ç¨³æ€å‡çº§ï¼š</strong></p>


      <ul>
        <li><strong>Clipped Double Qï¼ˆåŒ Q å–æœ€å°ï¼‰</strong>ï¼šæŠ‘åˆ¶ Q çš„ç³»ç»Ÿæ€§è¿‡ä¼°è®¡</li>
        <li><strong>Delayed Policy Updateï¼ˆå»¶è¿Ÿç­–ç•¥æ›´æ–°ï¼‰</strong>ï¼šå…ˆç¨³ Critic å†åŠ¨ Actor</li>
        <li><strong>Target Policy Smoothingï¼ˆç›®æ ‡ç­–ç•¥å¹³æ»‘ï¼‰</strong>ï¼šè®© TD ç›®æ ‡å¯¹å°–é” Q å³°ä¸æ•æ„Ÿ</li>
      </ul>
      <p>åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼ˆMuJoCoã€æœºæ¢°è‡‚ç­‰ï¼‰é‡Œï¼ŒTD3 é€šå¸¸æ¯” DDPG æ˜æ˜¾æ›´ç¨³ã€æ›´é«˜æ•ˆã€‚</p>

      <h3>1. èƒŒæ™¯ï¼šä¸ºä»€ä¹ˆéœ€è¦ TD3ï¼Ÿ</h3>
      <h4>1.1 DDPG çš„ä¸‰å¤§ç—›ç‚¹</h4>
      <ul>
        <li><strong>è¿‡ä¼°è®¡åå·®</strong>ï¼šå• Critic + â€œæœ€å¤§åŒ– Qâ€ çš„ç»“æ„ï¼Œä¼šæŠŠå™ªå£°å½“ä¼˜åŠ¿æ”¾å¤§ï¼Œé•¿æœŸåä¹è§‚ã€‚</li>
        <li><strong>ç­–ç•¥â€”å€¼å‡½æ•°æ­¥è°ƒå¤±è¡¡</strong>ï¼šCritic è¿˜æ²¡å­¦ç¨³ï¼ŒActor å°±è·Ÿç€ä¸ç¨³å®šçš„ä¿¡å·ç§»åŠ¨ï¼Œæ˜“å‘æ•£ã€‚</li>
        <li><strong>ç›®æ ‡å€¼å¯¹åŠ¨ä½œå¾®æ‰°ææ•æ„Ÿ</strong>ï¼šç›®æ ‡ç”¨ Q(s', Î¼'(s'))ï¼Œåœ¨å°–å³°å¤„å¯¹å¾®å°åŠ¨ä½œå˜åŒ–å‰§çƒˆï¼Œè¿‡æ‹Ÿåˆâ€œè„†å¼±å³°â€ã€‚</li>
      </ul>

      <h4>1.2 TD3 çš„ä¸‰å‰‚è¯</h4>
      <ul>
        <li><strong>åŒ Q å–æœ€å°</strong>ï¼šç”¨ä¸¤å¥—ç‹¬ç«‹çš„ç›®æ ‡ Qï¼ŒTD ç›®æ ‡é‡Œå– min(Q1', Q2')ï¼Œæ•°å€¼ä¸Šâ€œå‘ä¸‹è£å‰ªâ€æ‰é«˜ä¼°ã€‚</li>
        <li><strong>å»¶è¿Ÿç­–ç•¥æ›´æ–°</strong>ï¼šCritic æ¯æ­¥éƒ½æ›´ï¼›Actor/ç›®æ ‡ç½‘æ¯ d æ­¥ï¼ˆå¸¸ä¸º 2ï¼‰æ‰æ›´ï¼Œé¿å…ç­–ç•¥è¿½é€æœªæ”¶æ•›çš„ Qã€‚</li>
        <li><strong>ç›®æ ‡å¹³æ»‘</strong>ï¼šåœ¨ç›®æ ‡ç«¯ç»™ Î¼'(s') åŠ å°é«˜æ–¯å™ªå£°å¹¶è£å‰ªï¼Œå†é€å…¥ Q'ï¼Œé™ä½å¯¹å°–é”å³°å€¼çš„ä¾èµ–ã€‚</li>
      </ul>

      <h3>2. ç®—æ³•ç»†èŠ‚ä¸å…¬å¼</h3>
      <h4>2.1 ç»„ä»¶</h4>
      <ul>
        <li>Actorï¼ˆç¡®å®šæ€§ç­–ç•¥ï¼‰ï¼ša = Î¼Î¸(s)ï¼Œè¾“å‡ºç¼©æ”¾åˆ°åŠ¨ä½œè¾¹ç•Œã€‚</li>
        <li>ä¸¤ä¸ª Criticï¼šQÏ•1(s,a), QÏ•2(s,a)ï¼›ä»¥åŠå„è‡ªçš„ç›®æ ‡ç½‘ç»œ QÏ•1', QÏ•2'ã€‚</li>
        <li>ç›®æ ‡ Actorï¼šÎ¼Î¸'ã€‚</li>
        <li>å›æ”¾æ±  D å­˜ (s,a,r,s',d)ã€‚</li>
      </ul>

      <h4>2.2 è¡Œä¸ºç­–ç•¥ï¼ˆæ¢ç´¢ï¼‰</h4>
  <p>è®­ç»ƒäº¤äº’æ—¶æ‰§è¡Œï¼š</p>
  $$a_t = \operatorname{clip}\big(\mu_{\theta}(s_t) + \epsilon_t,\; \text{bounds}\big),\quad \epsilon_t \sim \mathcal{N}(0,\sigma_{\text{explore}}^2 I)$$
      <p>è¯„ä¼°/æµ‹è¯•æ—¶å»æ‰å™ªå£°ï¼Œåªç”¨ Î¼Î¸(s)ã€‚</p>

      <h4>2.3 TD ç›®æ ‡ï¼ˆæ ¸å¿ƒä¸‰ä»¶å¥—ï¼‰</h4>
      <p>ç›®æ ‡åŠ¨ä½œå¹³æ»‘ï¼š</p>
  $$\tilde a' = \operatorname{clip}\big(\mu_{\theta'}(s') + \epsilon,\; \text{bounds}\big),\quad \epsilon \sim \operatorname{clip}\big(\mathcal{N}(0,\sigma_{\text{targ}}^2),\,-c,\,c\big)$$
      <p>Clipped Double Qï¼š</p>
  $$y = r + \gamma(1-d)\, \min\!\big(Q_{\phi_1'}(s', \tilde a'),\; Q_{\phi_2'}(s', \tilde a')\big)$$
  <p>Critic æŸå¤±ï¼š</p>
  $$L_{Q_j} = \frac{1}{N} \sum_i \Big(Q_{\phi_j}(s_i, a_i) - y_i\Big)^2,\quad j\in\{1,2\}$$
  <p>Actor æŸå¤±ï¼ˆå»¶è¿Ÿæ›´æ–°ï¼‰ï¼š</p>
  $$L_\pi = -\frac{1}{N} \sum_i Q_{\phi_1}(s_i, \mu_{\theta}(s_i))$$
  <p>è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆä¸ Actor åŒæ­¥å»¶è¿Ÿï¼‰ï¼š</p>
  $$\theta' \leftarrow \tau\,\theta + (1-\tau)\,\theta',\quad \phi_j' \leftarrow \tau\,\phi_j + (1-\tau)\,\phi_j'$$

      <h3>3. å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆå·¥ç¨‹åŒ–ç¬”è®°ï¼‰</h3>
      <ol>
        <li>åˆå§‹åŒ–ï¼šéšæœºåˆå§‹åŒ– Î¼Î¸, QÏ•1, QÏ•2ï¼›å¤åˆ¶åˆ°ç›®æ ‡ç½‘ Î¸', Ï•j'ï¼›å»ºå›æ”¾æ±  Dï¼›è®¾è¶…å‚ Î³, Ï„, Ïƒ_explore, Ïƒ_targ, c, dã€‚</li>
        <li>äº¤äº’ & å­˜å‚¨ï¼šç”¨ a_t = clip(Î¼Î¸(s_t) + Ïµ_t) ä¸ç¯å¢ƒäº¤äº’ï¼Œå­˜ (s_t, a_t, r_t, s_{t+1}, d_t) åˆ° Dã€‚</li>
        <li>é‡‡æ ·æ‰¹æ¬¡ï¼šä» D å‡åŒ€é‡‡æ · N æ¡æ ·æœ¬ã€‚</li>
        <li>ç›®æ ‡åŠ¨ä½œå¹³æ»‘ & æ„é€  TD ç›®æ ‡ï¼šæŒ‰ 2.3ã€‚</li>
        <li>æ›´æ–°ä¸¤ä¸ª Criticï¼šæœ€å°åŒ– L_Q1, L_Q2ã€‚</li>
        <li>æ¯éš” d æ­¥ï¼šæ›´æ–° Actorï¼ˆæœ€å¤§åŒ– Q1ï¼‰ï¼›è½¯æ›´æ–° Î¸', Ï•1', Ï•2'ã€‚</li>
        <li>è¯„ä¼°ï¼šå®šæœŸåœ¨æ— å™ªå£°æ¡ä»¶ä¸‹è¿è¡Œè‹¥å¹²å›åˆæ±‚å¹³å‡å›æŠ¥ã€ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚</li>
        <li>å¾ªç¯è‡³æ”¶æ•›ã€‚</li>
      </ol>

      <h3>4. TD3 ä¼ªä»£ç ï¼ˆé«˜å¯†åº¦å·¥ç¨‹ç‰ˆï¼‰</h3>
<pre><code class="language-python"># TD3 ç®—æ³•ä¼ªä»£ç 
init Î¼Î¸, QÏ†1, QÏ†2; targets Î¼Î¸', QÏ†1', QÏ†2'; replay D
for t in 1..T:
  a = clip( Î¼Î¸(s) + N(0, Ïƒ_explore^2), bounds )
  s', r, d = env.step(a); D.add(s,a,r,s',d); s â† (s' if not d else reset)

  if len(D) < batch or t < warmup: continue
  S,A,R,SN,Dn = sample(D, N)

  Îµ = clip(N(0, Ïƒ_targ^2), -c, c)
  a_targ = clip( Î¼Î¸'(SN) + Îµ, bounds )

  y = R + Î³(1-Dn) * min( QÏ†1'(SN, a_targ), QÏ†2'(SN, a_targ) )
  update Ï†1, Ï†2 to minimize MSE(QÏ†j(S,A), y)

  if t % policy_delay == 0:
     update Î¸ to maximize mean QÏ†1(S, Î¼Î¸(S))
     soft_update(Î¸', Î¸, Ï„); soft_update(Ï†1', Ï†1, Ï„); soft_update(Ï†2', Ï†2, Ï„)
</code></pre>

      <h3>5. æ€»ç»“ä¸å·¥ç¨‹å»ºè®®</h3>
      <ul>
        <li>TD3 çš„ä¸‰å¤§æŠ€å·§æœ¬è´¨éƒ½æ˜¯â€œç¨³ä½Criticï¼Œæ…¢æ…¢åŠ¨Actorâ€ï¼Œè®©ç­–ç•¥æ›´æ–°æ›´å¯é ã€‚</li>
        <li>å®é™…ç”¨ TD3ï¼Œå»ºè®®å…ˆç”¨é»˜è®¤å‚æ•°ï¼Œä¼˜å…ˆå…³æ³¨ Q ä¼°è®¡å’Œç­–ç•¥æ”¶æ•›æ›²çº¿ã€‚</li>
        <li>å¦‚é‡è®­ç»ƒä¸ç¨³å®šï¼Œä¼˜å…ˆæ£€æŸ¥å™ªå£°ã€ç›®æ ‡å¹³æ»‘ã€å»¶è¿Ÿæ­¥æ•°ç­‰è¶…å‚ã€‚</li>
        <li>TD3 é€‚åˆå¤§å¤šæ•°è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼Œæ˜¯ DDPG çš„å·¥ç¨‹å‡çº§ç‰ˆã€‚</li>
      </ul>
    </section>

    <section id="chapter20" class="chapter">
      <h2>ç¬¬äºŒåç« ï¼šTRPOï¼ˆTrust Region Policy Optimizationï¼‰</h2>
      <h3>ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ TRPOï¼Ÿ</h3>
      <p>åœ¨ç­–ç•¥æ¢¯åº¦ç±»ç®—æ³•ï¼ˆå¦‚ REINFORCEã€A2Cã€A3Cï¼‰ä¸­ï¼Œæ¯æ¬¡æ›´æ–°éƒ½ä¼šå¯¼è‡´ç­–ç•¥åˆ†å¸ƒå˜åŒ–ã€‚ç¥ç»ç½‘ç»œçš„éçº¿æ€§ç‰¹æ€§ä½¿å¾—ä¸€æ¬¡â€œåˆç†â€çš„æ¢¯åº¦æ­¥å¯èƒ½å¯¼è‡´ç­–ç•¥è¾“å‡ºå®Œå…¨å´©æºƒï¼ˆpolicy collapseï¼‰ã€‚å› æ­¤ï¼ŒTRPO çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š<strong>æ—¢è¦è®©ç­–ç•¥æ”¹è¿›ï¼Œåˆè¦ä¿è¯å®ƒä¸è¦ç¦»æ—§ç­–ç•¥å¤ªè¿œ</strong>ï¼Œå³â€œä¿¡èµ–åŸŸä¼˜åŒ–â€ã€‚</p>

      <h3>äºŒã€æ€§èƒ½å·®åˆ†å…¬å¼</h3>
      <p>TRPO æ¨å¯¼å§‹äº Performance Difference Lemmaï¼š</p>
      $$
      J(\pi_{\text{new}}) - J(\pi_{\text{old}}) = \frac{1}{1-\gamma} \, \mathbb{E}_{s \sim d_{\pi_{\text{new}}}, a \sim \pi_{\text{new}}} [A_{\pi_{\text{old}}}(s, a)]
      $$
      <p>ç­–ç•¥æ”¹è¿›é‡å–å†³äºæ–°ç­–ç•¥åœ¨æ—§ç­–ç•¥ä¼˜åŠ¿å‡½æ•°ä¸‹çš„æœŸæœ›ã€‚</p>

      <h3>ä¸‰ã€é‡è¦æ€§é‡‡æ ·ä¸æ›¿ä»£ç›®æ ‡</h3>
      <p>å®é™…é‡‡æ ·æ¥è‡ªæ—§ç­–ç•¥ï¼Œéœ€ç”¨é‡è¦æ€§é‡‡æ ·ï¼š</p>
      $$
      \mathbb{E}_{a \sim \pi_{\text{new}}}[f(a)] = \mathbb{E}_{a \sim \pi_{\text{old}}} \left[ \frac{\pi_{\text{new}}(a)}{\pi_{\text{old}}(a)} f(a) \right]
      $$
      <p>äºæ˜¯ï¼š</p>
      $$
      J(\pi_{\text{new}}) - J(\pi_{\text{old}}) \approx \frac{1}{1-\gamma} \, \mathbb{E}_{s,a \sim \pi_{\text{old}}} [r_\theta(s,a) A_{\pi_{\text{old}}}(s,a)]
      $$
      $$
      r_\theta(s,a) = \frac{\pi_\theta(a|s)}{\pi_{\text{old}}(a|s)}
      $$
      <p>å‡è®¾çŠ¶æ€åˆ†å¸ƒå˜åŒ–å°ï¼Œå¾—åˆ° TRPO çš„æ ¸å¿ƒæ›¿ä»£ç›®æ ‡ï¼š</p>
      $$
      L(\theta) = \mathbb{E}_{s,a \sim \pi_{\text{old}}} [r_\theta(s,a) A_{\pi_{\text{old}}}(s,a)]
      $$

      <h3>å››ã€çº¦æŸç­–ç•¥æ›´æ–°å¹…åº¦ï¼šKL æ•£åº¦</h3>
      <p>ä¸ºé˜²æ­¢ç­–ç•¥åç¦»ï¼ŒTRPOå¼•å…¥çº¦æŸï¼š</p>
      $$
      \mathbb{E}_{s \sim \pi_{\text{old}}} [KL(\pi_{\text{old}}(\cdot|s) \| \pi_\theta(\cdot|s))] \leq \delta
      $$
      <p>ç›®æ ‡å˜ä¸ºï¼š</p>
      $$
      \max_{\theta} L(\theta) \quad \text{s.t.} \quad \bar{D}_{KL}(\pi_{\text{old}}, \pi_\theta) \leq \delta
      $$

      <h3>äº”ã€äºŒé˜¶è¿‘ä¼¼ä¸ Fisher ä¿¡æ¯çŸ©é˜µ</h3>
      <p>å¯¹ KL æ•£åº¦åšäºŒé˜¶æ³°å‹’å±•å¼€ï¼š</p>
      $$
      \bar{D}_{KL}(\pi_{\text{old}}, \pi_\theta) \approx \frac{1}{2} (\theta - \theta_{\text{old}})^\top F (\theta - \theta_{\text{old}})
      $$
      $$
      F = \mathbb{E}_{s,a \sim \pi_{\text{old}}} [\nabla_\theta \log \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)^\top]
      $$
      <p>å³ Fisher ä¿¡æ¯çŸ©é˜µã€‚</p>

      <h3>å…­ã€è‡ªç„¶æ¢¯åº¦æ–¹å‘ä¸è§£æè§£</h3>
      <p>æ„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š</p>
      $$
      L(\Delta\theta, \lambda) = g^\top \Delta\theta - \lambda \left( \frac{1}{2} \Delta\theta^\top F \Delta\theta - \delta \right)
      $$
      <p>å¯¹ \(\Delta\theta\) æ±‚å¯¼å¹¶ä»¤å…¶ä¸º 0ï¼š</p>
      $$
      g - \lambda F \Delta\theta = 0 \Rightarrow \Delta\theta = \frac{1}{\lambda} F^{-1} g
      $$
      <p>ä»£å›çº¦æŸè¾¹ç•Œï¼š</p>
      $$
      \lambda = \sqrt{\frac{g^\top F^{-1} g}{2\delta}}
      $$
      <p>æœ€ç»ˆè§£ï¼š</p>
      $$
      \Delta\theta = \sqrt{\frac{2\delta}{g^\top F^{-1} g}} F^{-1} g
      $$
      <p>è¿™å°±æ˜¯è‘—åçš„ <strong>è‡ªç„¶æ¢¯åº¦ï¼ˆNatural Gradientï¼‰</strong>ã€‚</p>

      <h3>ä¸ƒã€çº¿æœç´¢ä¸å®é™…æ›´æ–°</h3>
      <p>å®é™…ç›®æ ‡å’Œ KL éƒ½æ˜¯éçº¿æ€§çš„ï¼Œéœ€æ²¿è‡ªç„¶æ¢¯åº¦æ–¹å‘åš backtracking line searchï¼š</p>
      <ol>
        <li>ä» \(\alpha=1\) å¼€å§‹ï¼Œæ¯æ¬¡å‡åŠï¼ˆ1, 1/2, 1/4, ...ï¼‰ï¼›</li>
        <li>ç›´åˆ° surrogate ç›®æ ‡æå‡ä¸”å®é™… KL æœªè¶…è¿‡é˜ˆå€¼ï¼š</li>
      </ol>
      $$
      L(\theta_{\text{new}}) > L(\theta_{\text{old}}), \quad \hat{D}_{KL}(\pi_{\text{old}} \| \pi_{\text{new}}) \leq \delta
      $$

      <h3>å…«ã€æ•°å€¼å®ç°è¦ç‚¹</h3>
      <ol>
        <li><strong>å…±è½­æ¢¯åº¦ï¼ˆConjugate Gradient, CGï¼‰</strong>ï¼šè¿­ä»£è¿‘ä¼¼æ±‚è§£ \(F^{-1}g\)ã€‚</li>
        <li><strong>Fisher å‘é‡ç§¯ï¼ˆFVPï¼‰</strong>ï¼šç”¨ Pearlmutter trick è®¡ç®— FVPï¼Œæ— éœ€æ˜¾å¼çŸ©é˜µã€‚</li>
        <li><strong>çº¿æœç´¢ï¼ˆBacktrackingï¼‰</strong>ï¼šåœ¨çœŸå®ç›®æ ‡ä¸ŠéªŒè¯æ­¥é•¿å®‰å…¨æ€§ã€‚</li>
      </ol>
      <p>æœ€ç»ˆå‚æ•°æ›´æ–°ï¼š</p>
      $$
      	heta_{\text{new}} = \theta_{\text{old}} + \alpha \Delta\theta, \quad \alpha \in \{1, 1/2, 1/4, ...\}
      $$

      <h3>ä¹ã€TRPO ç®—æ³•æµç¨‹</h3>
      <ol>
        <li>é‡‡æ ·ï¼ˆon-policyï¼‰ï¼šç”¨å½“å‰ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†è½¨è¿¹ã€‚</li>
        <li>è®¡ç®—ä¼˜åŠ¿ä¸å›æŠ¥ç›®æ ‡ï¼ˆGAEï¼‰ã€‚</li>
        <li>å€¼å‡½æ•°å›å½’ï¼ˆå¤šè½® mini-batchï¼‰ã€‚</li>
        <li>ç­–ç•¥æ›´æ–°ï¼ˆå•æ­¥å¤§æ›´æ–° + çº¿æœç´¢ï¼‰ï¼š<br> 1ï¼‰æ„é€ æ›¿ä»£ç›®æ ‡ä¸æ¢¯åº¦ï¼›<br> 2ï¼‰Fisher å‘é‡ç§¯ï¼›<br> 3ï¼‰å…±è½­æ¢¯åº¦è¿‘ä¼¼ï¼›<br> 4ï¼‰è®¡ç®—ç†æƒ³æ­¥é•¿æ–¹å‘ï¼›<br> 5ï¼‰å›æº¯çº¿æœç´¢ã€‚</li>
        <li>æ›¿æ¢ç­–ç•¥ï¼Œè¿›å…¥ä¸‹ä¸€è½®ã€‚</li>
      </ol>
      <p>TRPO é€šè¿‡ä¿¡èµ–åŸŸçº¦æŸï¼Œä¿è¯æ¯æ¬¡ç­–ç•¥æ›´æ–°éƒ½å®‰å…¨ä¸”é«˜æ•ˆï¼Œæ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ç¨³å®šæ€§æé«˜çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ã€‚</p>
    </section>

    <section id="chapter21" class="chapter">
      <h2>ç¬¬äºŒåä¸€ç« ï¼šPPOï¼ˆProximal Policy Optimizationï¼‰</h2>

      <p>â€œTRPO ç»™äº†æˆ‘ä»¬ç†è®ºä¸Šçš„å®‰å…¨æ„Ÿï¼ŒPPO æŠŠå®ƒå˜æˆäº†èƒ½è·‘åœ¨æ˜¾å¡ä¸Šçš„ç°å®ã€‚â€</p>

      <h3>ä¸€ã€ä»ç­–ç•¥æ¢¯åº¦è¯´èµ·</h3>
      <p>ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ç›®æ ‡æ˜¯ç›´æ¥ä¼˜åŒ–å‚æ•°åŒ–ç­–ç•¥ $\pi_{\theta}(a|s)$ï¼Œæœ€å¤§åŒ–æœŸæœ›å›æŠ¥ï¼š</p>
      <p class="equation">$$J(\theta)=\mathbb{E}_{\pi_{\theta}}\Big[\sum_{t} \gamma^{t} r_t\Big].$$</p>
      <p>åŸºæœ¬çš„æ¢¯åº¦ä¼°è®¡ä¸ºï¼š</p>
      <p class="equation">$$\nabla_{\theta} J(\theta)=\mathbb{E}\big[\nabla_{\theta} \log \pi_{\theta}(a|s)\; A^{\pi}(s,a)\big],$$</p>
      <p>å…¶ä¸­ $A^{\pi}(s,a)$ æ˜¯ä¼˜åŠ¿å‡½æ•°ï¼Œè¡¨ç¤ºæŸä¸ªåŠ¨ä½œç›¸æ¯”å¹³å‡å†³ç­–çš„å¢ç›Šã€‚</p>

      <h3>äºŒã€ä» TRPO åˆ° PPO çš„åŠ¨æœº</h3>
      <p>TRPO é€šè¿‡å¯¹å¹³å‡ KL æ•£åº¦æ–½åŠ çº¦æŸï¼Œä¿è¯ç­–ç•¥ä¸â€œè·³å¾—å¤ªè¿œâ€ã€‚ç„¶è€Œ TRPO çš„å®ç°ä¾èµ–äºŒé˜¶ä¿¡æ¯ï¼ˆFisher çŸ©é˜µï¼‰ã€å…±è½­æ¢¯åº¦ä¸çº¿æœç´¢ï¼Œå·¥ç¨‹ä¸Šå¤æ‚ä¸”è®¡ç®—å¼€é”€å¤§ã€‚</p>
      <p>PPO çš„è®¾è®¡åŸåˆ™æ˜¯ä¿ç•™ TRPO çš„â€œé™åˆ¶æ›´æ–°å¹…åº¦â€æ€æƒ³ï¼Œä½†ç”¨ç®€å•å¯é çš„ä¸€é˜¶æ–¹æ³•æ›¿ä»£å¤æ‚çš„äºŒé˜¶æ­¥éª¤ï¼Œä»è€Œæ˜“å®ç°ä¸”é«˜æ•ˆã€‚</p>

      <h3>ä¸‰ã€PPO çš„æ ¸å¿ƒï¼šå‰ªåˆ‡æ›¿ä»£ç›®æ ‡ï¼ˆClipped Surrogate Objectiveï¼‰</h3>
      <p>å®šä¹‰æ¦‚ç‡æ¯”ç‡ï¼š</p>
      <p class="equation">$$r_{\theta}=\dfrac{\pi_{\theta}(a|s)}{\pi_{\text{old}}(a|s)}.$$</p>
      <p>å‰ªåˆ‡ç›®æ ‡ä¸ºï¼š</p>
      <p class="equation">$$L_{\text{CLIP}}(\theta)=\mathbb{E}\Big[\min\big(r_{\theta} A,\; \operatorname{clip}(r_{\theta},1-\epsilon,1+\epsilon) A\big)\Big].$$</p>
      <p>ç›´è§‚ä¸Šï¼Œå¯¹äº $A>0$ æˆ‘ä»¬å¸Œæœ›å¢å¤§ $r_{\theta}$ï¼›å¯¹äº $A<0$ åˆ™å¸Œæœ›å‡å° $r_{\theta}$ã€‚ä½†å½“æ¯”ç‡å˜åŒ–è¿‡å¤§ï¼Œclip ä¼šæŠŠå¢ç›Šæˆªæ–­åˆ° $[1-\epsilon,1+\epsilon]$ï¼Œä»è€Œé˜²æ­¢è¿‡å¤§çš„æ›´æ–°ã€‚</p>

      <h3>å››ã€å®Œæ•´è®­ç»ƒç›®æ ‡ï¼ˆç­–ç•¥ + å€¼å‡½æ•° + ç†µæ­£åˆ™ï¼‰</h3>
      <p>å®é™…è®­ç»ƒä¸­ï¼ŒPPO åŒæ—¶ä¼˜åŒ–ç­–ç•¥å‚æ•° $\theta$ ä¸å€¼å‡½æ•°å‚æ•° $\psi$ï¼Œæ€»æŸå¤±å¸¸å†™ä¸ºï¼š</p>
      <p class="equation">$$L(\theta,\psi)=L_{\text{CLIP}}(\theta)-c_v\,\mathbb{E}\big[(V_{\psi}(s)-\hat{R})^2\big]+c_e\,\mathbb{E}\big[\mathcal{H}(\pi_{\theta}(\cdot|s))\big],$$</p>
      <p>å…¶ä¸­ $c_v$ã€$c_e$ åˆ†åˆ«æ§åˆ¶å€¼å‡½æ•°å›å½’é¡¹å’Œç†µé¡¹çš„æƒé‡ï¼Œ$\hat R$ ä¸ºå›æŠ¥æˆ– target å€¼ã€‚</p>

      <h3>äº”ã€è®­ç»ƒæµç¨‹è¦ç‚¹</h3>
      <ol>
        <li><strong>é‡‡æ ·ï¼š</strong>ç”¨å½“å‰ç­–ç•¥ $\pi_{\text{old}}$ ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ä¸€æ‰¹è½¨è¿¹ $(s_t,a_t,r_t,s_{t+1},d_t,\log\pi_{\text{old}}(a_t|s_t))$ï¼›</li>
        <li><strong>ä¼˜åŠ¿ä¼°è®¡ï¼š</strong>å¸¸ç”¨ GAEï¼ˆGeneralized Advantage Estimationï¼‰ï¼š</li>
      </ol>
      <p class="equation">$$\delta_t=r_t+\gamma(1-d_t)V(s_{t+1})-V(s_t),\\
      \hat A_t=\sum_{l=0}^{\infty}(\gamma\lambda)^l \delta_{t+l}.$$</p>
      <ol start="3">
        <li>å¯¹ $\hat A_t$ åšæ ‡å‡†åŒ–ï¼ˆmean 0, std 1ï¼‰ä»¥ç¨³å®šè®­ç»ƒï¼›</li>
        <li><strong>å¤šè½®å°æ‰¹é‡ SGDï¼š</strong>å°†è½¨è¿¹æ‰“ä¹±ä¸º minibatchesï¼Œè¿›è¡Œå¤šè½® epochï¼ˆå¸¸è§ 3â€“10ï¼‰ï¼Œä½¿ç”¨ Adam è¿›è¡Œä¼˜åŒ–ï¼›</li>
        <li>å¦‚ç›‘æ§åˆ°å¹³å‡ KL è¶…è¿‡é˜ˆå€¼ï¼ˆå¦‚ 0.02ï¼‰ï¼Œå¯æå‰åœæ­¢å½“å‰ epochï¼›</li>
        <li>æ›´æ–° $\pi_{\text{old}}\leftarrow\pi_{\theta}$ï¼Œé‡å¤é‡‡æ ·ã€‚</li>
      </ol>

      <h3>å…­ã€å…³é”®è¶…å‚æ•°å»ºè®®</h3>
      <table>
        <tr><th>å‚æ•°</th><th>å«ä¹‰</th><th>æ¨èå€¼</th></tr>
        <tr><td>$\gamma$</td><td>æŠ˜æ‰£å› å­</td><td>0.99</td></tr>
        <tr><td>$\lambda$</td><td>GAE è¡°å‡</td><td>0.95</td></tr>
        <tr><td>$\epsilon$</td><td>Clip è¾¹ç•Œ</td><td>0.1â€“0.2</td></tr>
        <tr><td>$c_v$</td><td>å€¼å‡½æ•°ç³»æ•°</td><td>0.5</td></tr>
        <tr><td>$c_e$</td><td>ç†µç³»æ•°</td><td>0.01 (continuous æ›´å¤§)</td></tr>
        <tr><td>å­¦ä¹ ç‡ï¼ˆAdamï¼‰</td><td>å­¦ä¹ ç‡</td><td>3e-4ï¼ˆä»»åŠ¡ä¾èµ–ï¼‰</td></tr>
        <tr><td>T</td><td>æ¯è½®é‡‡æ ·æ­¥æ•°</td><td>2048 æˆ–æ›´å¤š</td></tr>
        <tr><td>Epochs</td><td>æ¯è½®è®­ç»ƒè½®æ•°</td><td>3â€“10</td></tr>
        <tr><td>Minibatch</td><td>å°æ‰¹å¤§å°</td><td>64â€“256</td></tr>
      </table>

      <h3>ä¸ƒã€å¸¸è§é—®é¢˜ä¸å·¥ç¨‹å»ºè®®</h3>
      <ul>
        <li><strong>è®­ç»ƒå‘æ•£ï¼š</strong>æ£€æŸ¥å­¦ä¹ ç‡ä¸ $\epsilon$ï¼Œå°è¯•å‡å°äºŒè€…ï¼›</li>
        <li><strong>æå‡ååˆé€€æ­¥ï¼š</strong>å¯èƒ½æ˜¯ epoch å¤ªå¤šæˆ–è¿‡æ‹Ÿåˆå½“å‰é‡‡æ ·ï¼Œå‡å°‘ epoch æˆ–ç›‘æ§ KLï¼›</li>
        <li><strong>ç­–ç•¥å¡Œç¼©ï¼š</strong>å¢å¤§ç†µç³»æ•°æˆ–ä½¿ç”¨ç†µç›®æ ‡è¡°å‡ç­–ç•¥ï¼›</li>
        <li><strong>å€¼å‡½æ•°ä¸å‡†ç¡®ï¼š</strong>å°è¯• value clipping ä¸æ›´ç¨³å®šçš„ç›®æ ‡ï¼ˆå¦‚åŒç½‘ç»œï¼‰ï¼Œå‡å°å€¼å‡½æ•°å­¦ä¹ ç‡ï¼›</li>
        <li><strong>GAE è°ƒå‚ï¼š</strong>è¾ƒé«˜çš„ $\lambda$ æ›´åå‘ä½æ–¹å·®é«˜åå·®ï¼Œå¸¸å– $0.92\sim0.98$ï¼›</li>
        <li><strong>æ ·æœ¬å¤ç”¨ï¼š</strong>PPO çš„å¤š epoch æ›´æ–°æé«˜äº†æ ·æœ¬åˆ©ç”¨ç‡ï¼Œä½†ä¹Ÿå¢åŠ äº†è¿‡æ‹Ÿåˆé£é™©ï¼Œå®è·µä¸­éœ€è¦æŠ˜ä¸­é€‰æ‹©ã€‚</li>
      </ul>

      <h3>å…«ã€PPO ä¸ TRPO çš„å…³ç³»</h3>
      <p>ç®€è¦å¯¹æ¯”ï¼š</p>
      <table>
        <tr><th>æ–¹é¢</th><th>TRPO</th><th>PPO</th></tr>
        <tr><td>ç­–ç•¥çº¦æŸ</td><td>ç¡¬ KL çº¦æŸ</td><td>æ¯”ç‡å‰ªåˆ‡ / KL æƒ©ç½š</td></tr>
        <tr><td>ä¼˜åŒ–æ–¹æ³•</td><td>äºŒé˜¶ï¼ˆFisherï¼‰</td><td>ä¸€é˜¶ï¼ˆAdam / SGDï¼‰</td></tr>
        <tr><td>å®ç°å¤æ‚åº¦</td><td>é«˜</td><td>ä½</td></tr>
        <tr><td>æ ·æœ¬åˆ©ç”¨ç‡</td><td>ä½</td><td>é«˜ï¼ˆå¤šè½®å¤ç”¨ï¼‰</td></tr>
      </table>

      <h3>ä¹ã€å°ç»“ï¼ˆBlog é£æ ¼è¡¥å……ï¼‰</h3>
      <p>PPO ä¹‹æ‰€ä»¥è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä¸ä»…å› ä¸ºå®ƒåœ¨å¤šæ•°åŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œæ›´é‡è¦çš„æ˜¯å®ƒæä¾›äº†ä¸€æ¡å·¥ç¨‹å¯è¡Œçš„è·¯å¾„ï¼š
      ç”¨ç®€å•çš„å‰ªåˆ‡ä¸å¤šè½®å°æ‰¹è®­ç»ƒæ›¿ä»£å¤æ‚çš„äºŒé˜¶æ±‚è§£ï¼Œä½¿å¾—ç­–ç•¥ä¼˜åŒ–å¯ä»¥ç›´æ¥å—ç›Šäºæ·±åº¦å­¦ä¹ ç°æœ‰çš„ä¸€é˜¶ä¼˜åŒ–å™¨ä¸å¹¶è¡ŒåŒ–å®ç°ã€‚
      åœ¨å·¥ç¨‹å®è·µä¸­ï¼Œæˆ‘çš„ç»éªŒæ˜¯ï¼š</p>
      <ul>
        <li>å…ˆç”¨æ¨èçš„è¶…å‚æ•°è·‘é€šï¼ˆä¾‹å¦‚ Adam lr=3e-4ï¼Œ$\epsilon=0.1$ï¼ŒT=2048ï¼ŒEpochs=4ï¼‰ï¼Œè§‚å¯Ÿå­¦ä¹ æ›²çº¿ï¼›</li>
        <li>è‹¥å‘æ•£ï¼Œå…ˆè°ƒå° lrï¼Œå…¶æ¬¡å‡å° $\epsilon$ï¼›</li>
        <li>åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ä¿ç•™ç†µæ­£åˆ™å¹¶é€æ­¥è¡°å‡ï¼›</li>
        <li>å€ŸåŠ©ç°æˆå®ç°ï¼ˆOpenAI Spinning Upã€Stable-Baselines3ï¼‰ä½œä¸ºåŸºçº¿å†åšæ”¹è¿›ã€‚</li>
      </ul>

      <p>å‚è€ƒå®ç°ï¼šOpenAI Spinning Upã€OpenAI åŸå§‹å®ç°ä¸ Stable-Baselines3 çš„ PPO æ¨¡å—å‡æ˜¯å¾ˆå¥½çš„å·¥ç¨‹æ¨¡æ¿ã€‚</p>

    </section>

    <section id="chapter22" class="chapter">
      <h2>ç¬¬äºŒåäºŒç« ï¼šSACï¼ˆSoft Actor-Criticï¼‰</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter23" class="chapter">
      <h2>ç¬¬äºŒåä¸‰ç« ï¼šç­–ç•¥æ¢¯åº¦å¢å¼ºç»„åˆ</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter_summary" class="chapter">
      <h2>è¿›é˜¶æ€»ç»“ä¸å­¦ä¹ å»ºè®®</h2>
      <h3>1. èƒ½åŠ›åœ°å›¾</h3>
      <table>
        <tr><th>èƒ½åŠ›</th><th>ç›¸å…³ç®—æ³•</th><th>æå‡è·¯å¾„</th></tr>
        <tr><td>æ ·æœ¬æ•ˆç‡</td><td>PERã€Rainbowã€SAC</td><td>ä¼˜å…ˆé‡‡æ · â†’ å¤šæ­¥ç›®æ ‡ â†’ æœ€å¤§ç†µ</td></tr>
        <tr><td>ç¨³å®šæ€§</td><td>Duelingã€TD3ã€PPO</td><td>æ¶æ„æ”¹è¿› â†’ åŒç½‘ç»œ â†’ å‰ªåˆ‡æŸå¤±</td></tr>
        <tr><td>è¿ç»­æ§åˆ¶</td><td>DDPGã€TD3ã€SAC</td><td>ç¡®å®šæ€§ç­–ç•¥ â†’ åŒ Critic â†’ æ¸©åº¦è‡ªé€‚åº”</td></tr>
      </table>

      <h3>2. å®æˆ˜è·¯çº¿</h3>
      <ol>
        <li>å…ˆåœ¨ <code>MuJoCo</code> ç­‰ç»å…¸ç¯å¢ƒå¤ç° PPO/SACï¼›</li>
        <li>å¼•å…¥ TD3 æˆ– Rainbow å¯¹æ¯”æŒ‡æ ‡ï¼›</li>
        <li>åœ¨è‡ªå®šä¹‰ç¯å¢ƒä¸­è°ƒå‚ï¼Œè§‚å¯Ÿå™ªå£°ã€ç†µç›®æ ‡ã€å­¦ä¹ ç‡å½±å“ã€‚</li>
      </ol>

      <h3>3. æ¨èé˜…è¯»</h3>
      <ul>
        <li>"Rainbow: Combining Improvements in Deep Reinforcement Learning"</li>
        <li>"Continuous Control with Deep Reinforcement Learning" (DDPG)</li>
        <li>"Addressing Function Approximation Error in Actor-Critic Methods" (TD3)</li>
        <li>"Proximal Policy Optimization Algorithms" (PPO)</li>
        <li>"Soft Actor-Critic" ç³»åˆ—è®ºæ–‡</li>
      </ul>
    </section>
  </main>
</div>

<footer>
  <p>Â© 2025 <span>Haoyun Tang</span> | Reinforcement Learning Advanced Notes</p>
</footer>

<script>
document.addEventListener('DOMContentLoaded', () => {
  document.querySelectorAll('pre[class*="language-"]').forEach(pre => {
    if (!pre.classList.contains('line-numbers')) {
      pre.classList.add('line-numbers');
    }
  });
  if (window.Prism) {
    Prism.highlightAll();
  }
});

window.addEventListener('load', () => {
  if (window.Prism) {
    Prism.highlightAll();
  }
});

const navLinks = document.querySelectorAll('nav a');
const asideLinks = document.querySelectorAll('aside a');
const combined = [...navLinks, ...asideLinks];

window.addEventListener('scroll', () => {
  const offset = window.scrollY + 160;
  combined.forEach(link => {
    const section = document.querySelector(link.getAttribute('href'));
    if (!section) return;
    if (section.offsetTop <= offset && section.offsetTop + section.offsetHeight > offset) {
      link.classList.add('active');
    } else {
      link.classList.remove('active');
    }
  });
});
</script>
</body>
</html>