<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>å¼ºåŒ–å­¦ä¹ è¿›é˜¶ç®—æ³• | Yun</title>
<!-- æ•°å­¦å…¬å¼æ¸²æŸ“ -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Prism.js æ ·å¼ä¸è„šæœ¬ -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-coy.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-javascript.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-bash.min.js"></script>
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>

<style>
body {
  margin: 0;
  font-family: "Segoe UI", "Roboto", -apple-system, sans-serif;
  background: linear-gradient(135deg, #121620 0%, #1b1f2b 40%, #221814 100%);
  color: #f5f5f5;
  line-height: 1.8;
  scroll-behavior: smooth;
}

header {
  position: sticky;
  top: 0;
  z-index: 100;
  text-align: center;
  background: rgba(20, 18, 24, 0.92);
  padding: 28px 20px 18px;
  backdrop-filter: blur(10px);
  border-bottom: 1px solid rgba(255, 140, 66, 0.35);
  box-shadow: 0 6px 22px rgba(0, 0, 0, 0.5);
}

header h1 {
  font-size: 2.3rem;
  color: #ff9740;
  margin: 0;
  font-weight: 700;
  letter-spacing: 1.5px;
}

nav ul {
  list-style: none;
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  gap: 10px;
  padding: 0;
  margin: 18px 0 0;
}

nav a {
  color: #f0dccc;
  text-decoration: none;
  font-weight: 600;
  font-size: 0.95rem;
  padding: 6px 16px;
  border-radius: 6px;
  transition: all 0.3s ease;
  background: rgba(255, 140, 66, 0.1);
  border: 1px solid rgba(255, 140, 66, 0.2);
}

nav a:hover, nav a.active {
  color: #ff9740;
  background: rgba(255, 140, 66, 0.2);
  border-color: rgba(255, 140, 66, 0.5);
  box-shadow: 0 0 14px rgba(255, 140, 66, 0.35);
}

.container {
  display: flex;
  position: relative;
  z-index: 5;
  max-width: 1400px;
  margin: 30px auto;
  gap: 30px;
  padding: 0 20px 40px;
}

aside {
  position: sticky;
  top: 130px;
  width: 260px;
  height: fit-content;
  background: rgba(28, 24, 32, 0.88);
  border: 1px solid rgba(255, 140, 66, 0.25);
  border-radius: 12px;
  padding: 22px;
  backdrop-filter: blur(12px);
  flex-shrink: 0;
}

aside h3 {
  color: #ff9740;
  font-size: 1.15rem;
  margin: 0 0 16px 0;
  padding-bottom: 10px;
  border-bottom: 2px solid rgba(255, 140, 66, 0.35);
}

aside ul {
  list-style: none;
  margin: 0;
  padding: 0;
  display: grid;
  gap: 6px;
}

aside a {
  color: #f0dccc;
  text-decoration: none;
  font-size: 0.9rem;
  padding: 8px 12px;
  border-radius: 6px;
  transition: all 0.2s ease;
  border-left: 3px solid transparent;
  background: transparent;
}

aside a:hover, aside a.active {
  color: #ff9740;
  background: rgba(255, 140, 66, 0.12);
  border-left-color: #ff9740;
  padding-left: 16px;
}

main {
  flex: 1;
  min-width: 0;
}

.chapter {
  background: rgba(28, 24, 32, 0.82);
  border: 1px solid rgba(255, 140, 66, 0.2);
  border-radius: 14px;
  padding: 36px;
  margin-bottom: 35px;
  transition: all 0.3s ease;
  box-shadow: 0 10px 30px rgba(0, 0, 0, 0.45);
}

.chapter:hover {
  border-color: rgba(255, 140, 66, 0.4);
  transform: translateY(-3px);
  box-shadow: 0 16px 38px rgba(0, 0, 0, 0.55);
}

.chapter h2 {
  color: #ff8c42;
  font-size: 1.85rem;
  margin: 0 0 24px 0;
  padding-bottom: 12px;
  border-bottom: 2px solid rgba(255, 140, 66, 0.3);
  font-weight: 700;
}

.chapter h3 {
  color: #ffb061;
  font-size: 1.25rem;
  margin-top: 26px;
  border-left: 4px solid rgba(255, 176, 97, 0.6);
  padding-left: 12px;
}

.chapter p, .chapter li {
  color: #f2e6dc;
}

.chapter strong {
  color: #ffb061;
}

.chapter code {
  background: rgba(255, 140, 66, 0.15);
  color: #ffd9b3;
  padding: 3px 6px;
  border-radius: 4px;
}

/* è§£å†³è¡Œå·é®æŒ¡ä»£ç é—®é¢˜ */
pre[class*="language-"] {
  background: linear-gradient(135deg, #1a0d1f 0%, #2d1b3d 100%) !important;
  padding: 20px !important;
  padding-left: 3.2em !important; /* ç»™ä»£ç æ­£æ–‡è®©å‡ºç©ºé—´ */
  border-radius: 10px !important;
  overflow-x: auto !important;
  font-size: 0.92rem !important;
  line-height: 1.6 !important;
  border: 1px solid rgba(156, 39, 176, 0.4) !important;
  position: relative;
  font-family: 'Fira Code', 'Courier New', 'Consolas', 'Monaco', monospace !important;
  box-shadow: 0 4px 20px rgba(156, 39, 176, 0.2) !important;
  text-rendering: optimizeLegibility !important;
  -webkit-font-smoothing: antialiased !important;
  -moz-osx-font-smoothing: grayscale !important;
}

/* ä»£ç è¡Œå· */
pre.line-numbers {
  padding-left: 60px !important;
}

.line-numbers .line-numbers-rows {
  left: 0 !important;
  width: 3em !important;
  background: transparent !important; /* å»æ‰èƒŒæ™¯é®ç½© */
  border-right: 1px solid #444 !important; /* åˆ†å‰²çº¿ */
}

.line-numbers-rows > span:before {
  color: rgba(186, 104, 200, 0.8) !important;
  font-weight: bold;
}

pre code {
  color: inherit !important;
  background: none !important;
  font-family: inherit !important;
  font-size: inherit !important;
  text-rendering: optimizeLegibility !important;
  -webkit-font-smoothing: antialiased !important;
  font-feature-settings: "liga" 0 !important;
}

/* ä»£ç å—å†…å„å…ƒç´ é«˜äº® */
.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
  color: #9c9c9c !important;
  font-style: italic;
}

.token.punctuation {
  color: #f8f8f2 !important;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
  color: #ff79c6 !important;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
  color: #50fa7b !important;
}

.token.operator,
.token.entity,
.token.url {
  color: #8be9fd !important;
}

.token.atrule,
.token.attr-value,
.token.keyword {
  color: #bd93f9 !important;
}

.token.function,
.token.class-name {
  color: #ffb86c !important;
}

.token.regex,
.token.important,
.token.variable {
  color: #f1fa8c !important;
}

/* ç¡®ä¿æ‰€æœ‰ä»£ç æ–‡æœ¬éƒ½æœ‰åŸºç¡€é¢œè‰² */
pre[class*="language-"] code,
pre[class*="language-"] {
  color: #f8f8f2 !important;
}

/* ä¿®å¤ç­‰å·å’Œç¬¦å·çš„æ¸²æŸ“é—®é¢˜ */
pre[class*="language-"] .token.operator,
pre[class*="language-"] .token.punctuation {
  background: none !important;
  text-shadow: none !important;
  font-weight: normal !important;
}

/* é˜²æ­¢å­—ä½“è¿å­—å½±å“ç¬¦å·æ˜¾ç¤º */
pre[class*="language-"] code {
  font-variant-ligatures: none !important;
  font-feature-settings: "liga" 0, "clig" 0 !important;
}

/* ä»£ç å—é¼ æ ‡æ‚¬åœæ•ˆæœ */
pre[class*="language-"]:hover {
  box-shadow: 
    0 12px 48px rgba(156, 39, 176, 0.3),
    inset 0 1px 0 rgba(255,255,255,0.1) !important;
  transform: translateY(-2px);
  transition: all 0.3s ease;
}

/* æ»šåŠ¨æ¡ç¾åŒ– */
pre[class*="language-"]::-webkit-scrollbar {
  height: 10px;
  background: rgba(156, 39, 176, 0.1);
}

pre[class*="language-"]::-webkit-scrollbar-thumb {
  background: rgba(156, 39, 176, 0.5);
  border-radius: 5px;
  transition: background 0.3s;
}

pre[class*="language-"]::-webkit-scrollbar-thumb:hover {
  background: rgba(156, 39, 176, 0.8);
}

table {
  width: 100%;
  border-collapse: collapse;
  margin: 18px 0;
  background: rgba(255, 140, 66, 0.06);
  border-radius: 10px;
  overflow: hidden;
  box-shadow: 0 6px 20px rgba(0, 0, 0, 0.35);
}

table th, table td {
  padding: 12px 16px;
  border-bottom: 1px solid rgba(255, 140, 66, 0.2);
  color: #f7ede2;
}

table th {
  background: rgba(255, 140, 66, 0.22);
  color: #fff1e6;
  font-size: 1.05rem;
}

table tr:last-child td {
  border-bottom: none;
}

/* é˜²æ­¢é®æŒ¡çš„é˜´å½±æˆ–æ¨¡ç³Šæ•ˆæœ */
pre[class*="language-"]::before,
pre[class*="language-"]::after {
  box-shadow: none !important;
  background: none !important;
}

footer {
  text-align: center;
  padding: 28px 20px;
  background: rgba(20, 18, 24, 0.9);
  border-top: 1px solid rgba(255, 140, 66, 0.3);
  color: #f0dccc;
  margin-top: 60px;
  font-size: 0.9rem;
}

footer span {
  color: #ff9740;
  font-weight: 600;
}

@media (max-width: 1024px) {
  .container {
    flex-direction: column;
  }
  aside {
    position: relative;
    top: auto;
    width: 100%;
  }
}

@media (max-width: 768px) {
  header h1 {
    font-size: 2rem;
  }
  nav a {
    font-size: 0.85rem;
    padding: 4px 12px;
  }
  .container {
    padding: 0 12px 30px;
  }
  .chapter {
    padding: 24px;
  }
}
/* æ•°å­¦å…¬å¼ï¼ˆç¬¬16-19ç« ï¼‰æ˜¾ç¤ºï¼šæ¢å¤ä¸ºâ€œæ­£å¸¸â€MathJaxï¼Œæ— èƒŒæ™¯/è¾¹æ¡†ï¼Œä»…ä¿ç•™é€‚åº¦é—´è· */
#chapter16 mjx-container[display="true"],
#chapter17 mjx-container[display="true"],
#chapter18 mjx-container[display="true"],
#chapter19 mjx-container[display="true"] {
  display: block;
  margin: 1.1em auto;       /* ä»…ç•™ç™½ï¼Œä¾¿äºé˜…è¯» */
  padding: 0;               /* æ— é¢å¤–å†…è¾¹è· */
  background: none !important;
  border: none !important;
  border-radius: 0;
  box-shadow: none;
  max-width: 100%;
}

/* è¡Œå†…å…¬å¼ï¼šä¿æŒé»˜è®¤ï¼Œä¸åšè‰²å—/è¾¹æ¡†ä¿®é¥° */
#chapter16 mjx-container:not([display="true"]),
#chapter17 mjx-container:not([display="true"]),
#chapter18 mjx-container:not([display="true"]),
#chapter19 mjx-container:not([display="true"]) {
  vertical-align: baseline;
  padding: 0;
  background: none;
}

/* equation è¾…åŠ©ç±»ï¼šä¸­æ€§æ ·å¼ï¼ˆæ— åº•è‰²/è¾¹æ¡†ï¼‰ */
.equation {
  margin: 1.1em auto;
  padding: 0;
  text-align: center;
  background: none;
  border: none;
  border-radius: 0;
  box-shadow: none;
  max-width: 100%;
}
.equation mjx-container { margin: 0 !important; }
</style>
</head>
<body>
<header>
  <h1>å¼ºåŒ–å­¦ä¹ ç¬”è®°ï¼ˆè¿›é˜¶ç¯‡ï¼‰</h1>
  <nav>
    <ul>
      <li><a href="index.html">é¦–é¡µ</a></li>
      <li><a href="home.html">åŸºç¡€ç†è®º</a></li>
      <li><a href="advanced.html" class="active">è¿›é˜¶ç®—æ³•</a></li>
      <li><a href="MARL.html">å¤šæ™ºèƒ½ä½“å­¦ä¹ </a></li>
      <li><a href="papers.html">è®ºæ–‡ç ”è¯»</a></li>
    </ul>
  </nav>
</header>

<div class="container">
  <aside>
    <h3>ğŸ”¥ è¿›é˜¶ç›®å½•</h3>
    <ul>
      <li><a href="#chapter1">ç¬¬ä¸€ç« ï¼šDueling DQN</a></li>
      <li><a href="#chapter2">ç¬¬äºŒç« ï¼šPER</a></li>
      <li><a href="#chapter3">ç¬¬ä¸‰ç« ï¼šNoisyNet DQN</a></li>
      <li><a href="#chapter4">ç¬¬å››ç« ï¼šRainbow DQN</a></li>
      <li><a href="#chapter5">ç¬¬äº”ç« ï¼šDPG & DDPG</a></li>
      <li><a href="#chapter6">ç¬¬å…­ç« ï¼šTD3</a></li>
      <li><a href="#chapter7">ç¬¬ä¸ƒç« ï¼šTRPO</a></li>
      <li><a href="#chapter8">ç¬¬å…«ç« ï¼šPPO</a></li>
      <li><a href="#chapter9">ç¬¬ä¹ç« ï¼šSAC</a></li>
      <li><a href="#chapter10">ç¬¬åç« ï¼šç­–ç•¥æ¢¯åº¦å¢å¼ºç»„åˆ</a></li>
      <li><a href="#chapter_summary">è¿›é˜¶æ€»ç»“</a></li>
    </ul>
  </aside>

  <main>
  <section id="chapter1" class="chapter">
    <h2>ç¬¬ä¸€ç« ï¼šDueling DQNï¼ˆåŒæµæ¶æ„ï¼‰</h2>
    <p>Dueling DQN å°† Q ç½‘ç»œæ‹†åˆ†ä¸º<strong>çŠ¶æ€ä»·å€¼åˆ†æ”¯ V(s)</strong>ä¸<strong>åŠ¨ä½œä¼˜åŠ¿åˆ†æ”¯ A(s,a)</strong>ï¼Œç”¨ä¸¤æ¡å¹¶è¡Œçš„å­ç½‘ç»œåˆ†åˆ«ä¼°è®¡çŠ¶æ€çš„æ•´ä½“ä»·å€¼ä¸åŠ¨ä½œçš„ç›¸å¯¹ä¼˜åŠ¿ï¼Œå†å°†äºŒè€…ç»„åˆå¾—åˆ° Q å€¼ã€‚è¿™ç§è®¾è®¡åœ¨ä¸€äº›çŠ¶æ€ä¸‹åŠ¨ä½œå·®å¼‚è¾ƒå°ï¼ˆæˆ–åŠ¨ä½œæ— å…³ï¼‰çš„åœºæ™¯ä¸‹èƒ½æ˜¾è‘—æå‡ä¼°è®¡ç¨³å®šæ€§ä¸æ”¶æ•›é€Ÿåº¦ã€‚</p>

    <h3>1. æ ¸å¿ƒç»“æ„ä¸æ•°å­¦å½¢å¼</h3>
    <p>ä¸»å¹²ç½‘ç»œï¼ˆå·ç§¯æˆ–å‰é¦ˆï¼‰æå–å…±äº«ç‰¹å¾ååˆ†ä¸ºä¸¤æ¡åˆ†æ”¯ï¼š</p>
    <ul>
      <li><strong>Value Streamï¼š</strong>è¾“å‡ºæ ‡é‡ $V(s)$ï¼Œè¡¨ç¤ºçŠ¶æ€æœ¬èº«çš„ä»·å€¼ï¼›</li>
      <li><strong>Advantage Streamï¼š</strong>è¾“å‡ºå‘é‡ $A(s,a)$ï¼Œè¡¨ç¤ºåœ¨çŠ¶æ€ $s$ ä¸‹å„åŠ¨ä½œçš„ç›¸å¯¹ä¼˜åŠ¿ã€‚</li>
    </ul>
    <p>ä¸ºä½¿åˆ†è§£å¯è¾¨è¯†ï¼ˆidentifiableï¼‰ï¼Œé€šå¸¸ç”¨ä¸‹å¼é‡ç»„ Q å€¼ï¼š</p>
    <p>$$Q(s, a) = V(s) + \Big(A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a')\Big)$$</p>
    <p>å‡å»ä¼˜åŠ¿å‡å€¼èƒ½ä¿è¯ $\sum_a A(s,a)=0$ï¼Œä»è€Œé¿å…ä»»æ„å¸¸æ•°åœ¨ V ä¸ A ä¹‹é—´è½¬ç§»å¯¼è‡´çš„ä¸å¯è¾¨è¯†é—®é¢˜ï¼ˆè§ä¸‹é¢çš„å”¯ä¸€åˆ†è§£è¯æ˜ï¼‰ã€‚</p>

    <h3>2. ç›´è§‰ä¸ä¼˜ç‚¹</h3>
    <ul>
      <li>å½“æŸäº›çŠ¶æ€ä¸‹ä¸åŒåŠ¨ä½œæ”¶ç›Šæ¥è¿‘æ—¶ï¼Œä¸“é—¨å­¦ä¹  $V(s)$ ä¼šæ›´ç¨³å®šï¼›</li>
      <li>$A(s,a)$ èšç„¦åŠ¨ä½œé—´å·®å¼‚ï¼Œèƒ½æ›´å¿«æ•æ‰å¾®å°ç­–ç•¥ä¼˜åŠ¿ï¼›</li>
      <li>å¯ä¸ Double DQNã€PERã€NoisyNet ç­‰æ”¹è¿›æ–¹æ³•ç›´æ¥ç»“åˆï¼Œå¸¸è§äºå¼ºåŒ–å­¦ä¹ ç«æŠ€å¹³å°ï¼ˆAtariã€ProcGen ç­‰ï¼‰ã€‚</li>
    </ul>

    <h3>3. å”¯ä¸€åˆ†è§£ï¼ˆIdentifiabilityï¼‰è¯æ˜</h3>
    <p>é—®é¢˜ï¼šç»™å®š Q(s,a)ï¼Œå°†å…¶å†™æˆ $Q(s,a)=V(s)+A(s,a)$ æ˜¯å¦å”¯ä¸€ï¼Ÿç­”æ¡ˆï¼šåœ¨æ²¡æœ‰çº¦æŸæ—¶ä¸æ˜¯å”¯ä¸€çš„ï¼›è‹¥å¯¹æ¯ä¸ª s æ–½åŠ çº¦æŸ $\sum_a A(s,a)=0$ï¼ˆæˆ–ç­‰ä»·çš„å¸¸æ•°çº¦æŸï¼‰ï¼Œåˆ™åˆ†è§£å”¯ä¸€ã€‚</p>
    <p><strong>è¯æ˜æ­¥éª¤ï¼š</strong></p>
    <p>å‡è®¾å­˜åœ¨ä¸¤ç»„åˆ†è§£ $(V, A)$ ä¸ $(V', A')$ æ»¡è¶³å¯¹æ‰€æœ‰ $(s,a)$ï¼š</p>
    <p>$$Q(s,a)=V(s)+A(s,a)=V'(s)+A'(s,a)$$</p>
    <p>ä»¤ $D(s)=V(s)-V'(s)$ï¼Œ$E(s,a)=A(s,a)-A'(s,a)$ï¼Œåˆ™å¯¹æ‰€æœ‰ $(s,a)$ æœ‰</p>
    <p>$$D(s)+E(s,a)=0 \quad \Rightarrow \quad E(s,a) = -D(s)$$</p>
    <p>å¯¹åŠ¨ä½œé›†åˆæ±‚å’Œå¹¶ä½¿ç”¨çº¦æŸ $\sum_a E(s,a)=0$ï¼š</p>
    <p>$$\sum_a E(s,a) = -\sum_a D(s) = -|\mathcal{A}|\, D(s) = 0 \Rightarrow D(s)=0$$</p>
    <p>äºæ˜¯ $D(s)=0$ï¼Œè¿›è€Œ $E(s,a)=0$ï¼Œå³ $(V,A)$ ä¸ $(V',A')$ åœ¨æ–½åŠ å‡å€¼çº¦æŸä¸‹ç›¸åŒï¼Œåˆ†è§£å”¯ä¸€ã€‚</p>

    <h3>4. ç½‘ç»œæ¶æ„è¯¦è§£</h3>
    <p>Dueling DQN çš„ç½‘ç»œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š</p>
    <ol>
      <li><strong>å…±äº«ç‰¹å¾æå–å±‚</strong>ï¼šå·ç§¯æˆ–å…¨è¿æ¥å±‚ç”¨äºæå–çŠ¶æ€ç‰¹å¾ï¼ˆä½œä¸ºåä¸¤ä¸ªåˆ†æ”¯çš„è¾“å…¥ï¼‰ï¼›</li>
      <li><strong>Value åˆ†æ”¯</strong>ï¼šç‹¬ç«‹çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œæœ€ç»ˆè¾“å‡ºå•ä¸ªæ ‡é‡ $V(s)$ï¼›</li>
      <li><strong>Advantage åˆ†æ”¯</strong>ï¼šç‹¬ç«‹çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œæœ€ç»ˆè¾“å‡º $|\mathcal{A}|$ ç»´å‘é‡ $A(s,a)$ã€‚</li>
    </ol>
    <p>å‰å‘ä¼ æ’­æ—¶ï¼Œå¯¹ Advantage å±‚è¾“å‡ºè¿›è¡Œ<strong>å»å‡å€¼å¤„ç†</strong>åä¸ Value å±‚ç»„åˆï¼š</p>
    <p>$$Q(s,a) = V(s) + \Big(A(s,a) - \frac{1}{|\mathcal{A}|}\sum_a A(s,a)\Big)$$</p>

    <h3>5. åˆå§‹åŒ–ä¸è®­ç»ƒç»†èŠ‚</h3>
    <p>ä¸ºç¡®ä¿ç½‘ç»œç¨³å®šæ€§å’Œå¯è¾¨è¯†æ€§ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„åˆå§‹åŒ–ï¼š</p>
    <ul>
      <li><strong>æƒé‡åˆå§‹åŒ–</strong>ï¼šä½¿ç”¨ Xavier uniform æˆ– Kaiming åˆå§‹åŒ–ï¼›å¯¹æ‰€æœ‰çº¿æ€§å±‚çš„åç½®åˆå§‹åŒ–ä¸º 0ï¼›</li>
      <li><strong>ä¸ºä»€ä¹ˆéœ€è¦ç‰¹æ®Šåˆå§‹åŒ–</strong>ï¼šAdvantage åˆ†æ”¯åˆæœŸåº”è¾“å‡ºæ¥è¿‘ 0 çš„å€¼ï¼Œè¿™æ · $A - \text{mean}(A) \approx 0$ï¼Œç¡®ä¿ Q å€¼åˆæœŸä¸»è¦ç”± $V(s)$ ä¸»å¯¼ï¼Œé¿å…ä¼˜åŠ¿åˆ†æ”¯è¿‡å¤§å¯¼è‡´çš„æ•°å€¼ä¸ç¨³å®šï¼›</li>
      <li><strong>ä¸»è¦ä¼˜åŠ¿</strong>ï¼šå°åˆå§‹ä¼˜åŠ¿ä¿è¯äº†åˆ†è§£çš„å¯è¾¨è¯†æ€§ï¼Œä½¿ V ä¸ A ä¸ä¼šåœ¨å‚æ•°ç©ºé—´ä¸­äº’ç›¸æŠµæ¶ˆã€‚</li>
    </ul>

    <h3>6. å®ç°æ­¥éª¤ï¼ˆé€æ­¥è¯´æ˜ï¼‰</h3>
    <p>ä¸‹é¢æŠŠ Dueling DQN çš„è®­ç»ƒè¿‡ç¨‹æ‹†æˆå¯ç›´æ¥è½åœ°çš„æ­¥éª¤ï¼Œä¾¿äºæŠŠä»£ç æ˜ å°„åˆ°å…·ä½“å®ç°ï¼š</p>
    <ol>
      <li><strong>å®šä¹‰ç½‘ç»œ</strong>ï¼šå…±äº«ä¸»å¹²æå–ç‰¹å¾ï¼›åˆ†å‡ºä¸¤ä¸ªå¤´â€”â€”Value è¾“å‡ºæ ‡é‡ V(s)ï¼ŒAdvantage è¾“å‡ºå‘é‡ A(s,a)ã€‚å‰å‘æ—¶å¯¹ A å»å‡å€¼å†ä¸ V é‡æ„ Qã€‚</li>
      <li><strong>ç»éªŒæ”¶é›†</strong>ï¼šä½¿ç”¨ Îµ-greedy æˆ–å¯å­¦ä¹ å™ªå£°ç­–ç•¥é‡‡æ · transition $(s,a,r,s',\text{done})$ï¼Œå¹¶å­˜å…¥ç»éªŒå›æ”¾æ±  ğ’Ÿã€‚</li>
      <li><strong>å°æ‰¹é‡é‡‡æ ·</strong>ï¼šä» ğ’Ÿ ä¸­éšæœºé‡‡æ · batchï¼›è‹¥ä½¿ç”¨ PER è¯·ç”¨é‡è¦æ€§æƒé‡ä¿®æ­£æŸå¤±ã€‚</li>
      <li><strong>ç›®æ ‡è®¡ç®—</strong>ï¼šç”¨ç›®æ ‡ç½‘ç»œ $\theta^-$ å‰å‘å¾—åˆ° $V'(s')$ ä¸ $A'(s',\cdot)$ï¼Œå¯¹ $A'$ å»å‡å€¼å¹¶é‡æ„ Q'ï¼›ç›®æ ‡ $y = r + \gamma \cdot \max_{a'} Q'(s',a')$ï¼ˆè‹¥ done åˆ™ $y = r$ï¼‰ã€‚</li>
      <li><strong>æŸå¤±ä¸æ›´æ–°</strong>ï¼šè®¡ç®— $L = \text{MSE}(y, Q(s,a;\theta))$ æˆ– Huber Lossï¼Œåå‘ä¼ æ’­å¹¶ç”¨ä¼˜åŒ–å™¨ step æ›´æ–°ä¸»ç½‘ç»œ $\theta$ã€‚</li>
      <li><strong>ç›®æ ‡ç½‘ç»œåŒæ­¥</strong>ï¼šé‡‡ç”¨ç¡¬æ›´æ–°ï¼ˆæ¯ C æ­¥å¤åˆ¶ï¼‰æˆ–è½¯æ›´æ–°ï¼ˆ$\theta^- \leftarrow \tau\theta + (1-\tau)\theta^-$ï¼‰ã€‚</li>
    </ol>

    <h3>7. PyTorch ä»£ç ç¤ºä¾‹</h3>
  <pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import deque

class DuelingNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DuelingNetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        
        # Value åˆ†æ”¯ï¼šè¾“å‡ºå•ä¸ªæ ‡é‡
        self.value_stream = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # Advantage åˆ†æ”¯ï¼šè¾“å‡º action_dim ç»´å‘é‡
        self.advantage_stream = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
        
        # æƒé‡åˆå§‹åŒ–
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        # æå–å…±äº«ç‰¹å¾
        features = self.feature(state)
        
        # Value ä¸ Advantage ç‹¬ç«‹å‰å‘
        v = self.value_stream(features)
        a = self.advantage_stream(features)
        
        # é‡ç»„ Q å€¼ï¼ˆå»å‡å€¼ä¿è¯å¯è¾¨è¯†ï¼‰
        a_mean = a.mean(dim=1, keepdim=True)
        q = v + (a - a_mean)
        
        return q

class DuelingDQN:
    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, epsilon=0.1):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        
        # ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_net = DuelingNetwork(state_dim, action_dim)
        self.target_q_net = DuelingNetwork(state_dim, action_dim)
        self.target_q_net.load_state_dict(self.q_net.state_dict())
        
        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        
        # ç»éªŒå›æ”¾æ± 
        self.memory = deque(maxlen=10000)
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state):
        """Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            with torch.no_grad():
                q_values = self.q_net(torch.FloatTensor(state).unsqueeze(0))
            return q_values.max(1)[1].item()
    
    def train_batch(self, batch_size):
        """è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡"""
        if len(self.memory) < batch_size:
            return
        
        # éšæœºé‡‡æ ·
        indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        
        states = torch.FloatTensor(np.array([x[0] for x in batch]))
        actions = torch.LongTensor([x[1] for x in batch])
        rewards = torch.FloatTensor([x[2] for x in batch])
        next_states = torch.FloatTensor(np.array([x[3] for x in batch]))
        dones = torch.FloatTensor([x[4] for x in batch])
        
        # å½“å‰ Q å€¼
        q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # ç›®æ ‡ Q å€¼
        with torch.no_grad():
            max_next_q = self.target_q_net(next_states).max(1)[0]
            target_q = rewards + self.gamma * max_next_q * (1 - dones)
        
        # æŸå¤±å’Œåå‘ä¼ æ’­
        loss = self.loss_fn(q_values, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)
        self.optimizer.step()
        
        return loss.item()
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_q_net.load_state_dict(self.q_net.state_dict())

# è®­ç»ƒç¤ºä¾‹
if __name__ == "__main__":
    state_dim = 4
    action_dim = 2
    agent = DuelingDQN(state_dim, action_dim)
    
    # æ¨¡æ‹Ÿç¯å¢ƒäº¤äº’
    for episode in range(100):
        state = np.random.randn(state_dim)
        episode_reward = 0
        
        for step in range(50):
            action = agent.act(state)
            next_state = np.random.randn(state_dim)
            reward = np.random.randn()
            done = step == 49
            
            agent.remember(state, action, reward, next_state, float(done))
            agent.train_batch(batch_size=32)
            
            episode_reward += reward
            state = next_state
        
        if (episode + 1) % 20 == 0:
            agent.update_target_network()
            print(f"Episode {episode+1}, Reward: {episode_reward:.2f}")
  </code></pre>
    <table>
      <tr>
        <th>é—®é¢˜</th>
        <th>è§£ç­”</th>
      </tr>
      <tr>
        <td>ä¸ºä»€ä¹ˆè¦å¯¹ Advantage å»å‡å€¼ï¼Ÿ</td>
        <td>å»å‡å€¼ä¿è¯äº†åˆ†è§£çš„å”¯ä¸€æ€§ã€‚è‹¥ä¸å»å‡å€¼ï¼Œå¯ä»¥åœ¨ V ä¸ A é—´ä»»æ„è½¬ç§»å¸¸æ•°å€¼è€Œäº§ç”Ÿç›¸åŒ Qï¼Œå¯¼è‡´å‚æ•°ä¸å¯è¾¨è¯†ã€‚</td>
      </tr>
      <tr>
        <td>V ä¸ A æ˜¯å¦éœ€è¦åŒæ—¶è®­ç»ƒï¼Ÿ</td>
        <td>æ˜¯çš„ã€‚ä¸¤ä¸ªåˆ†æ”¯å…±äº«ä¸»å¹²ï¼Œåå‘ä¼ æ’­ä¼šåŒæ—¶æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚V å­¦ä¹ çŠ¶æ€ä»·å€¼ï¼ŒA å­¦ä¹ åŠ¨ä½œç›¸å¯¹ä¼˜åŠ¿ï¼Œä¸¤è€…åˆ†å·¥æ˜ç¡®ã€‚</td>
      </tr>
      <tr>
        <td>åˆå§‹åŒ–ä¸ºä»€ä¹ˆå¾ˆé‡è¦ï¼Ÿ</td>
        <td>åˆæœŸ Advantage åº”æ¥è¿‘ 0ï¼Œä½¿ Q â‰ˆ Vï¼Œç¡®ä¿ç½‘ç»œç¨³å®šå¯åŠ¨ã€‚è¿‡å¤§çš„åˆå§‹ A ä¼šå¯¼è‡´æ•°å€¼éœ‡è¡ï¼Œå½±å“æ”¶æ•›ã€‚</td>
      </tr>
      <tr>
        <td>å¦‚ä½•ä¸ Double DQN ç»“åˆï¼Ÿ</td>
        <td>ç›®æ ‡è®¡ç®—æ—¶ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°ä»·å€¼ï¼Œä¸æ ‡å‡† Double DQN ç±»ä¼¼ï¼Œåªæ˜¯ç½‘ç»œç»“æ„ä»å•ä¸ª Q æ”¹ä¸º V+Aã€‚</td>
      </tr>
    </table>

    <h3>9. å®è·µè¦ç‚¹ä¸ç»„åˆç­–ç•¥</h3>
    <ul>
      <li><strong>ä¸ Double DQN ç»“åˆ</strong>ï¼šç›®æ ‡è®¡ç®—ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°ä»·å€¼ï¼Œæ¶ˆé™¤è¿‡ä¼°è®¡åå·®ï¼›</li>
      <li><strong>ä¸ PER ç»“åˆ</strong>ï¼šPrioritized Experience Replay èƒ½æ›´å¿«èšç„¦é«˜ TD è¯¯å·®æ ·æœ¬ï¼Œä¸ Dueling æ¶æ„ç›¸è¾…ç›¸æˆï¼›</li>
      <li><strong>ç¦»æ•£ vs è¿ç»­åŠ¨ä½œ</strong>ï¼šDueling DQN åœ¨ç¦»æ•£åŠ¨ä½œé—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ï¼›è¿ç»­æ§åˆ¶éœ€ä¸ç¡®å®šæ€§ç­–ç•¥ï¼ˆå¦‚ DDPGï¼‰æˆ–ç­–ç•¥åˆ†æ”¯æ”¹é€ ï¼›</li>
      <li><strong>è°ƒå‚å»ºè®®</strong>ï¼šé¿å…ä¼˜åŠ¿åˆ†æ”¯è¾“å‡ºè¿‡å¤§å¯¼è‡´æ•°å€¼ä¸ç¨³ï¼ˆå¯åŠ  L2 æ­£åˆ™æˆ–æ¢¯åº¦è£å‰ªï¼‰ï¼›ç›‘æ§ V ä¸ A çš„å¤§å°æ¯”ä¾‹ã€‚</li>
    </ul>
  </section>

    <section id="chapter2" class="chapter">
      <h2>ç¬¬äºŒç« ï¼šPER</h2>
      

      <h3>PERï¼šä¼˜å…ˆç»éªŒå›æ”¾</h3>
      
      <p><strong>é—®é¢˜ï¼š</strong>æ™®é€šç»éªŒå›æ”¾æ˜¯å‡åŒ€éšæœºé‡‡æ ·ï¼Œä½†æœ‰äº›æ ·æœ¬ä¿¡æ¯é‡å¤§ï¼ˆæ¯”å¦‚ TD è¯¯å·®å¾ˆå¤§ï¼‰ï¼Œæœ‰äº›æ²¡å•¥ç”¨ã€‚</p>
      
      <p><strong>è§£å†³ï¼š</strong>ç»™æ¯ä¸ªæ ·æœ¬æ‰“åˆ†ï¼ŒTD è¯¯å·®è¶Šå¤§ â†’ ä¼˜å…ˆçº§è¶Šé«˜ â†’ è¶Šå®¹æ˜“è¢«é‡‡æ ·ã€‚</p>

      <h4>é‡‡æ ·æ¦‚ç‡</h4>
      <p>$$P(i) = \frac{(|\delta_i| + \varepsilon)^\alpha}{\sum_k (|\delta_k| + \varepsilon)^\alpha}$$</p>
      
      <table>
        <tr><th>å‚æ•°</th><th>å«ä¹‰</th><th>å¸¸ç”¨å€¼</th></tr>
        <tr><td>Î±</td><td>æ§åˆ¶ä¼˜å…ˆçº§å½±å“åŠ›åº¦</td><td>0.6</td></tr>
        <tr><td>Îµ</td><td>é˜²æ­¢ä¼˜å…ˆçº§ä¸º 0</td><td>1e-6</td></tr>
      </table>

      <h4>é‡è¦æ€§æƒé‡</h4>
      <p>å› ä¸ºæ”¹å˜äº†é‡‡æ ·åˆ†å¸ƒï¼Œéœ€è¦ç”¨æƒé‡ä¿®æ­£ï¼Œé¿å…åå·®ï¼š</p>
      <p>$$w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta$$</p>
      
      <p>Î² ä» 0.4 é€æ¸å‡åˆ° 1.0ï¼Œè®­ç»ƒå‰æœŸå…è®¸æœ‰ç‚¹åå·®ï¼ŒåæœŸä¸¥æ ¼ä¿®æ­£ã€‚</p>

      <p><strong>æœ€ç»ˆæŸå¤±ï¼š</strong></p>
      <p>$$L = \text{mean}(w_i \cdot \delta_i^2)$$</p>

      <h3>ä¸‰ã€æ•´åˆæµç¨‹ï¼šDueling + PER + Double</h3>
      
      <ol>
        <li>åˆå§‹åŒ– online ç½‘ç»œï¼ˆDueling ç»“æ„ï¼‰ã€target ç½‘ç»œã€PER ç¼“å†²åŒº</li>
        <li>ä¸ç¯å¢ƒäº¤äº’ï¼Œå¾—åˆ° (s, a, r, s', done)</li>
        <li>è®¡ç®— TD è¯¯å·®ï¼š
          $$\delta = \big|r + \gamma \cdot Q_{\text{target}}\big(s', \arg\max_{a'} Q_{\text{online}}(s',a')\big) - Q_{\text{online}}(s,a)\big|$$
        </li>
        <li>å­˜å…¥ PERï¼Œä¼˜å…ˆçº§ = Î´ + Îµ</li>
        <li>æŒ‰ä¼˜å…ˆçº§é‡‡æ · batchï¼Œå¾—åˆ°æƒé‡ w</li>
        <li>è®¡ç®—åŠ æƒæŸå¤±ï¼Œåå‘ä¼ æ’­</li>
        <li>æ›´æ–° PER ä¸­å„æ ·æœ¬çš„ä¼˜å…ˆçº§</li>
        <li>å®šæœŸåŒæ­¥ target ç½‘ç»œ</li>
      </ol>

      <p><strong>æ³¨æ„ï¼š</strong>è¿™é‡Œç”¨çš„æ˜¯ Double DQN çš„ç›®æ ‡è®¡ç®—æ–¹å¼ï¼ˆonline é€‰åŠ¨ä½œï¼Œtarget è¯„ä¼°ï¼‰ï¼Œé˜²æ­¢ Q å€¼è¿‡ä¼°è®¡ã€‚</p>

      <h3>å››ã€å®Œæ•´ä»£ç å®ç°</h3>

      <h4>Dueling ç½‘ç»œ</h4>
<pre><code class="language-python">
import torch
import torch.nn as nn

class DuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # å…±äº«ç‰¹å¾å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        # åˆ†æ”¯ 1ï¼šçŠ¶æ€ä»·å€¼
        self.value = nn.Linear(128, 1)
        # åˆ†æ”¯ 2ï¼šåŠ¨ä½œä¼˜åŠ¿
        self.advantage = nn.Linear(128, action_dim)

    def forward(self, x):
        feat = self.feature(x)
        v = self.value(feat)
        a = self.advantage(feat)
        # å»å‡å€¼ç»„åˆ
        q = v + (a - a.mean(dim=1, keepdim=True))
        return q
</code></pre>

      <h4>PER ç¼“å†²åŒºï¼ˆSumTree å®ç°ï¼‰</h4>
<pre><code class="language-python">
import numpy as np
import random

class SumTree:
    """SumTree ç”¨äºå¿«é€Ÿé‡‡æ ·ï¼Œæ—¶é—´å¤æ‚åº¦ O(log N)"""
    def __init__(self, capacity):
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1)
        self.data = [None] * capacity
        self.write = 0
        self.n_entries = 0

    def _propagate(self, idx, change):
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)

    def update(self, idx, priority):
        change = priority - self.tree[idx]
        self.tree[idx] = priority
        self._propagate(idx, change)

    def add(self, priority, data):
        idx = self.write + self.capacity - 1
        self.data[self.write] = data
        self.update(idx, priority)
        self.write = (self.write + 1) % self.capacity
        self.n_entries = min(self.n_entries + 1, self.capacity)

    def get(self, s):
        idx = 0
        while True:
            left = 2 * idx + 1
            if left >= len(self.tree):
                break
            if s <= self.tree[left]:
                idx = left
            else:
                s -= self.tree[left]
                idx = left + 1
        data_idx = idx - self.capacity + 1
        return idx, self.tree[idx], self.data[data_idx]

    @property
    def total(self):
        return self.tree[0]

class PERBuffer:
    def __init__(self, capacity, alpha=0.6, eps=1e-6):
        self.tree = SumTree(capacity)
        self.alpha = alpha
        self.eps = eps

    def add(self, td_error, sample):
        priority = (abs(td_error) + self.eps) ** self.alpha
        self.tree.add(priority, sample)

    def sample(self, batch_size, beta=0.4):
        batch, idxs, priorities = [], [], []
        segment = self.tree.total / batch_size
        
        for i in range(batch_size):
            a, b = segment * i, segment * (i + 1)
            s = random.uniform(a, b)
            idx, p, data = self.tree.get(s)
            batch.append(data)
            idxs.append(idx)
            priorities.append(p)
        
        # è®¡ç®—é‡è¦æ€§æƒé‡
        probs = np.array(priorities) / self.tree.total
        weights = (self.tree.n_entries * probs) ** (-beta)
        weights /= weights.max()  # å½’ä¸€åŒ–
        
        return batch, idxs, torch.FloatTensor(weights)

    def update_priority(self, idx, td_error):
        priority = (abs(td_error) + self.eps) ** self.alpha
        self.tree.update(idx, priority)
</code></pre>

      <h4>è®­ç»ƒæµç¨‹</h4>
<pre><code class="language-python">
import torch.optim as optim
import torch.nn.functional as F

# åˆå§‹åŒ–
online_net = DuelingDQN(state_dim=4, action_dim=2)
target_net = DuelingDQN(state_dim=4, action_dim=2)
target_net.load_state_dict(online_net.state_dict())
optimizer = optim.Adam(online_net.parameters(), lr=1e-4)

buffer = PERBuffer(capacity=10000, alpha=0.6)
gamma = 0.99
beta = 0.4

for episode in range(1000):
    state = env.reset()
    done = False
    
    while not done:
        # Îµ-greedy é€‰åŠ¨ä½œ
        if random.random() < epsilon:
            action = random.randint(0, action_dim-1)
        else:
            with torch.no_grad():
                q = online_net(torch.FloatTensor(state).unsqueeze(0))
                action = q.argmax().item()
        
        next_state, reward, done, _ = env.step(action)
        
        # è®¡ç®— TD è¯¯å·®
        with torch.no_grad():
            q_next = target_net(torch.FloatTensor(next_state).unsqueeze(0))
            best_action = online_net(torch.FloatTensor(next_state).unsqueeze(0)).argmax()
            td_target = reward + (1 - done) * gamma * q_next[0, best_action]
            q_current = online_net(torch.FloatTensor(state).unsqueeze(0))[0, action]
            td_error = abs(td_target - q_current).item()
        
        # å­˜å…¥ PER
        buffer.add(td_error, (state, action, reward, next_state, done))
        state = next_state
        
        # è®­ç»ƒ
        if buffer.tree.n_entries > 64:
            batch, idxs, weights = buffer.sample(32, beta)
            
            states = torch.FloatTensor([x[0] for x in batch])
            actions = torch.LongTensor([x[1] for x in batch])
            rewards = torch.FloatTensor([x[2] for x in batch])
            next_states = torch.FloatTensor([x[3] for x in batch])
            dones = torch.FloatTensor([x[4] for x in batch])
            
            # Double DQN ç›®æ ‡
            q_eval = online_net(states).gather(1, actions.unsqueeze(1)).squeeze()
            with torch.no_grad():
                best_actions = online_net(next_states).argmax(1)
                q_next = target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze()
                q_target = rewards + (1 - dones) * gamma * q_next
            
            # åŠ æƒæŸå¤±
            td_errors = (q_target - q_eval).abs()
            loss = (weights * F.mse_loss(q_eval, q_target, reduction='none')).mean()
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # æ›´æ–°ä¼˜å…ˆçº§
            for idx, err in zip(idxs, td_errors.detach().numpy()):
                buffer.update_priority(idx, err)
    
    # å®šæœŸåŒæ­¥ target ç½‘ç»œ
    if episode % 10 == 0:
        target_net.load_state_dict(online_net.state_dict())
</code></pre>

      <h3>äº”ã€æ•ˆæœå¯¹æ¯”</h3>
      <ul>
        <li><strong>Dueling</strong>ï¼šç½‘ç»œæ›´é«˜æ•ˆï¼Œå°¤å…¶åœ¨åŠ¨ä½œå½±å“å°çš„çŠ¶æ€</li>
        <li><strong>PER</strong>ï¼šæ”¶æ•›æ›´å¿«ï¼Œæ ·æœ¬åˆ©ç”¨ç‡æ›´é«˜</li>
        <li><strong>Double</strong>ï¼šé˜²æ­¢ Q å€¼è¿‡ä¼°è®¡</li>
      </ul>

      <p>ä¸‰è€…ç»„åˆæ˜¯ Rainbow DQN çš„æ ¸å¿ƒï¼Œåœ¨ Atari æ¸¸æˆä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>

      <p><strong>æ”¹è¿›è·¯çº¿ï¼š</strong>DQN â†’ Double DQN â†’ Dueling DQN â†’ PER â†’ Rainbow DQN</p>
    </section>

    <section id="chapter3" class="chapter">
      <h2>ç¬¬ä¸‰ç« ï¼šNoisyNet DQNï¼ˆè‡ªé€‚åº”æ¢ç´¢ï¼‰</h2>

      <p>å‰é¢é€šè¿‡ Dueling å’Œ PER æ”¹è¿›äº†ç½‘ç»œç»“æ„å’Œé‡‡æ ·æ•ˆç‡ï¼Œä½†æ¢ç´¢é—®é¢˜è¿˜æ²¡è§£å†³å¥½ã€‚</p>

      <p>ä¼ ç»Ÿçš„ <strong>Îµ-greedy</strong> æ¢ç´¢é äººå·¥è®¾å®š Îµï¼Œå®ƒçš„éšæœºæ€§è·Ÿç½‘ç»œå­¦ä¹ æ²¡ä»€ä¹ˆå…³ç³»ï¼Œå¾ˆéš¾è‡ªé€‚åº”è°ƒæ•´æ¢ç´¢å¼ºåº¦ã€‚<br>
      <strong>NoisyNet DQN</strong>ï¼ˆFortunato et al., 2017ï¼‰æ¢äº†ä¸ªæ€è·¯ â€”â€” ç›´æ¥åœ¨ç½‘ç»œå‚æ•°é‡ŒåŠ å¯å­¦ä¹ çš„å™ªå£°ï¼Œè®©æ¢ç´¢å˜æˆç½‘ç»œå­¦ä¹ çš„ä¸€éƒ¨åˆ†ã€‚</p>

      <h3>ä¸€ã€æ ¸å¿ƒæƒ³æ³•</h3>
      <p>ç”¨å¸¦å™ªå£°çš„çº¿æ€§å±‚æ›¿ä»£æ™®é€šå…¨è¿æ¥å±‚ï¼š</p>

      <pre><code class="language-python">
y = (W + Ïƒ_W âŠ™ Îµ_W) Â· x + (b + Ïƒ_b âŠ™ Îµ_b)
      </code></pre>

      <ul>
        <li><code>W, b</code>ï¼šæ ‡å‡†æƒé‡å’Œåç½®</li>
        <li><code>Ïƒ_W, Ïƒ_b</code>ï¼šå¯å­¦ä¹ çš„å™ªå£°å¼ºåº¦ï¼ˆç½‘ç»œè®­ç»ƒæ—¶ä¼šè°ƒæ•´ï¼‰</li>
        <li><code>Îµ_W, Îµ_b</code>ï¼šæ¯æ¬¡å‰å‘ä¼ æ’­é‡æ–°é‡‡æ ·çš„é«˜æ–¯å™ªå£°</li>
      </ul>

      <p>è¿™æ ·ç½‘ç»œè¾“å‡ºè‡ªå¸¦éšæœºæ€§ï¼Œå™ªå£°å¤§å°éšè®­ç»ƒè‡ªåŠ¨è°ƒæ•´ã€‚ä¸ç”¨æ‰‹åŠ¨è°ƒ Îµ äº†ï¼Œæ¢ç´¢å¼ºåº¦ç½‘ç»œè‡ªå·±å­¦ã€‚</p>

      <h3>äºŒã€ä»£ç å®ç°</h3>
<pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class NoisyLinear(nn.Module):
    def __init__(self, in_features, out_features, std_init=0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        # å¯å­¦ä¹ å‚æ•°
        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.register_buffer("weight_eps", torch.empty(out_features, in_features))

        self.bias_mu = nn.Parameter(torch.empty(out_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))
        self.register_buffer("bias_eps", torch.empty(out_features))

        self.reset_parameters()
        self.sample_noise()

    def reset_parameters(self):
        mu_range = 1 / math.sqrt(self.in_features)
        self.weight_mu.data.uniform_(-mu_range, mu_range)
        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))
        self.bias_mu.data.uniform_(-mu_range, mu_range)
        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))

    def sample_noise(self):
        """é‡æ–°é‡‡æ ·å™ªå£°"""
        self.weight_eps.normal_()
        self.bias_eps.normal_()

    def forward(self, x):
        if self.training:
            # è®­ç»ƒæ—¶åŠ å™ªå£°
            w = self.weight_mu + self.weight_sigma * self.weight_eps
            b = self.bias_mu + self.bias_sigma * self.bias_eps
        else:
            # æµ‹è¯•æ—¶ä¸åŠ å™ªå£°
            w, b = self.weight_mu, self.bias_mu
        return F.linear(x, w, b)
</code></pre>

      <h3>ä¸‰ã€ä¸ Dueling DQN ç»“åˆ</h3>
<pre><code class="language-python">
class NoisyDuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # å…±äº«ç‰¹å¾å±‚ï¼ˆæ™®é€šå±‚ï¼‰
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        # ä»·å€¼æµå’Œä¼˜åŠ¿æµç”¨ NoisyLinear
        self.value = NoisyLinear(128, 1)
        self.advantage = NoisyLinear(128, action_dim)

    def forward(self, x):
        feat = self.feature(x)
        v = self.value(feat)
        a = self.advantage(feat)
        q = v + (a - a.mean(dim=1, keepdim=True))
        return q
    
    def sample_noise(self):
        """é‡é‡‡æ ·æ‰€æœ‰å™ªå£°å±‚"""
        self.value.sample_noise()
        self.advantage.sample_noise()
</code></pre>

      <h3>å››ã€è®­ç»ƒæµç¨‹</h3>
      <ol>
        <li>æŠŠ Dueling DQN çš„ <code>Linear</code> å±‚æ¢æˆ <code>NoisyLinear</code></li>
        <li><strong>å»æ‰ Îµ-greedy</strong>ï¼Œæ¢ç´¢å®Œå…¨é å™ªå£°é©±åŠ¨</li>
        <li>æ¯æ¬¡å‰å‘ä¼ æ’­å‰è°ƒç”¨ <code>sample_noise()</code> é‡æ–°é‡‡æ ·å™ªå£°</li>
        <li>æŸå¤±å‡½æ•°ã€TD ç›®æ ‡ã€target æ›´æ–°éƒ½è·Ÿ DQN ä¸€æ ·</li>
      </ol>

<pre><code class="language-python">
# è®­ç»ƒæ—¶æ— éœ€ Îµ-greedy
for episode in range(1000):
    state = env.reset()
    online_net.sample_noise()  # æ¯ä¸ª episode å¼€å§‹é‡é‡‡æ ·å™ªå£°
    
    while not done:
        # ç›´æ¥é€‰æœ€ä¼˜åŠ¨ä½œï¼Œå™ªå£°å·²ç»åœ¨ç½‘ç»œé‡Œäº†
        with torch.no_grad():
            q = online_net(torch.FloatTensor(state).unsqueeze(0))
            action = q.argmax().item()
        
        next_state, reward, done, _ = env.step(action)
        buffer.add((state, action, reward, next_state, done))
        state = next_state
        
        # è®­ç»ƒæ­¥éª¤
        if len(buffer) > batch_size:
            batch = buffer.sample(batch_size)
            online_net.sample_noise()  # è®­ç»ƒå‰é‡é‡‡æ ·
            target_net.sample_noise()
            
            # è®¡ç®—æŸå¤±å¹¶æ›´æ–°...
</code></pre>

      <h3>äº”ã€ä¼˜ç‚¹</h3>
      <ul>
        <li><strong>è‡ªé€‚åº”æ¢ç´¢</strong>ï¼šç½‘ç»œåœ¨ä¸ç¡®å®šçš„çŠ¶æ€è‡ªåŠ¨å¢å¤§å™ªå£°</li>
        <li><strong>å‡å°‘è¶…å‚</strong>ï¼šä¸ç”¨æ‰‹åŠ¨è°ƒ Îµ å’Œè¡°å‡ç­–ç•¥äº†</li>
        <li><strong>æ€§èƒ½æ›´å¥½</strong>ï¼šåœ¨ Atari å’Œ MuJoCo ä¸Šæ™®éä¼˜äº Îµ-greedy</li>
      </ul>

      <h3>å…­ã€å…¸å‹ç»„åˆ</h3>
      <p>NoisyNet ç»å¸¸å’Œè¿™äº›ä¸€èµ·ç”¨ï¼š</p>
      <ul>
        <li><strong>Dueling DQN</strong> â†’ æå‡ç‰¹å¾è¡¨è¾¾</li>
        <li><strong>Double DQN</strong> â†’ å‡å°‘è¿‡ä¼°è®¡</li>
        <li><strong>PER</strong> â†’ åŠ é€Ÿæ ·æœ¬åˆ©ç”¨</li>
      </ul>
      <p>ç»„åˆèµ·æ¥å°±æ˜¯ Rainbow DQN çš„ç¬¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚</p>

      <h3>ä¸ƒã€å°ç»“</h3>
      <p>NoisyNet æŠŠæ¢ç´¢æœºåˆ¶èå…¥ç½‘ç»œå‚æ•°ï¼Œè®©æ™ºèƒ½ä½“è‡ªå·±è°ƒèŠ‚æ¢ç´¢å¼ºåº¦ã€‚<br>
      ç›¸æ¯”ä¼ ç»Ÿ Îµ-greedyï¼Œè¿™æ‰æ˜¯çœŸæ­£çš„"å¯å­¦ä¹ æ¢ç´¢"ï¼Œåœ¨å¤æ‚ç¯å¢ƒé‡Œæ›´é«˜æ•ˆã€æ›´æ™ºèƒ½ã€‚</p>

      <p><strong>æ”¹è¿›è·¯çº¿ï¼š</strong>DQN â†’ Double DQN â†’ Dueling DQN â†’ PER â†’ <strong>NoisyNet DQN</strong> â†’ Rainbow DQN</p>
    </section>

    <section id="chapter4" class="chapter">
      <h2>ç¬¬å››ç« ï¼šRainbow DQNï¼ˆé›†å¤§æˆè€…ï¼‰</h2>

      <h3>é¢„å¤‡çŸ¥è¯†ï¼šN-Step Learningï¼ˆå¤šæ­¥æ—¶åºå·®åˆ†ï¼‰</h3>

      <p><strong>æ ¸å¿ƒåŠ¨æœºï¼š</strong> ç›¸æ¯” 1-step TD åªçœ‹ä¸‹ä¸€æ­¥å¥–åŠ±ï¼ŒN-step å°†æœªæ¥ N æ­¥çš„çœŸå®å¥–åŠ±ä¸€æ¬¡æ€§çº³å…¥ç›®æ ‡ï¼Œè®©å¥–åŠ±ä¿¡å·æ›´å¿«åœ°å›ä¼ åˆ°å½“å‰çŠ¶æ€ï¼Œä»è€ŒåŠ é€Ÿå­¦ä¹ ã€æé«˜æ ·æœ¬åˆ©ç”¨ç‡ã€‚</p>

      <h4>1. ç†è®ºèƒŒæ™¯ï¼šä»‹äº TD ä¸ Monte Carlo ä¹‹é—´çš„æŠ˜ä¸­</h4>
  <p>ä¼ ç»Ÿæ—¶åºå·®åˆ†ï¼ˆTDï¼‰æ¯æ­¥æ›´æ–°ä¸€æ¬¡ï¼Œåªåˆ©ç”¨ä¸‹ä¸€æ­¥å¥–åŠ±ï¼š</p>
  $$y_t^{(1)} = r_t + \gamma \max_{a} Q_{\text{target}}(s_{t+1}, a)$$
  <p>è€Œ Monte Carlo æ–¹æ³•ç­‰æ•´å±€ç»“æŸåç”¨å®Œæ•´å›æŠ¥æ›´æ–°ï¼š</p>
  $$y_t^{(MC)} = r_t + \gamma r_{t+1} + \gamma^{2} r_{t+2} + \dots + \gamma^{T-t} r_T$$

      <p>äºŒè€…ç‰¹ç‚¹å¦‚ä¸‹ï¼š</p>
      <ul>
        <li><strong>TDï¼š</strong> æ›´æ–°å¿«ã€ä½æ–¹å·®ï¼Œä½†ä»…çœ‹ä¸€æ­¥ï¼Œä¼ æ’­æ…¢ã€‚</li>
        <li><strong>Monte Carloï¼š</strong> åˆ©ç”¨å®Œæ•´ä¿¡æ¯ã€æ— åï¼Œä½†éœ€ç­‰æ•´å±€ç»“æŸã€æ–¹å·®å¤§ã€‚</li>
      </ul>

  <p><strong>N-step Learning</strong> ä»‹äºä¸¤è€…ä¹‹é—´ï¼Œé€šè¿‡å›ºå®šé•¿åº¦ N çš„æ»šåŠ¨çª—å£ï¼Œå°†æœªæ¥ N æ­¥çš„çœŸå®å¥–åŠ±çº³å…¥æ›´æ–°ç›®æ ‡ï¼š</p>
  $$y_t^{(N)} = \sum_{i=0}^{N-1} \gamma^{i} r_{t+i} + \gamma^{N} \max_{a} Q_{\text{target}}(s_{t+N}, a)$$

      <p>è¿™æ ·å¯ä»¥åŒæ—¶ä¿ç•™ TD çš„åœ¨çº¿æ›´æ–°èƒ½åŠ›ï¼Œåˆå¼•å…¥ Monte Carlo çš„é•¿è¿œè§†è§’ï¼Œå…¼é¡¾ç¨³å®šæ€§ä¸é«˜æ•ˆæ€§ã€‚</p>

      <h4>2. å®ç°é€»è¾‘ï¼ˆæ»‘åŠ¨çª—å£æœºåˆ¶ï¼‰</h4>
      <ol>
        <li>ç»´æŠ¤é•¿åº¦ä¸º N çš„é˜Ÿåˆ— <code>n_step_buffer</code>ï¼Œæ¯æ¬¡ç¯å¢ƒäº¤äº’åå…¥é˜Ÿæ ·æœ¬ <code>(s, a, r, s_next, done)</code>ã€‚</li>
        <li>å½“é˜Ÿåˆ—æ»¡ N æ—¶ï¼Œè®¡ç®—ç´¯è®¡æŠ˜æ‰£å¥–åŠ±ï¼š
          $$R = \sum_{i=0}^{N-1} \gamma^{i} r_{t+i}$$</li>
        <li>å–é˜Ÿé¦–çŠ¶æ€ <code>(s_t, a_t)</code> å’Œé˜Ÿå°¾ä¸‹ä¸€ä¸ªçŠ¶æ€ <code>s_{t+N}</code> æ„é€  N-step æ ·æœ¬ï¼š
          $$(s_t, a_t, R, s_{t+N}, done)$$</li>
        <li>å­˜å…¥ç»éªŒå›æ”¾æ± ï¼Œè®­ç»ƒæ—¶ä»¥ç›®æ ‡ï¼š
          $$y_t^{(N)} = R + \gamma^{N}\max_{a} Q_{\text{target}}(s_{t+N}, a)$$</li>
      </ol>

      <h4>3. å‚æ•°ä¸æ³¨æ„äº‹é¡¹</h4>
      <ul>
        <li>N é€šå¸¸å– <strong>3~5</strong>ï¼ˆRainbow é»˜è®¤ N=3ï¼‰ã€‚</li>
        <li>è‹¥å›åˆæå‰ç»“æŸï¼ˆä¸è¶³ N æ­¥ï¼‰ï¼Œç›´æ¥ç”¨å·²è§‚æµ‹åˆ°çš„å¥–åŠ±ç´¯åŠ ã€‚</li>
        <li>ä¸ PERã€NoisyNetã€Dueling å¯æ— ç¼ç»“åˆï¼Œå¢å¼ºè®­ç»ƒæ•ˆç‡ä¸ç¨³å®šæ€§ã€‚</li>
      </ul>

      <hr>

      <h3>é¢„å¤‡çŸ¥è¯†ï¼šC51ï¼ˆCategorical DQNï¼Œåˆ†å¸ƒå¼ä»·å€¼å­¦ä¹ ï¼‰</h3>

      <p><strong>æ ¸å¿ƒåŠ¨æœºï¼š</strong> DQN åªå­¦ä¹ å›æŠ¥çš„æœŸæœ› Q(s,a)ï¼Œè€Œ C51 ç›´æ¥å­¦ä¹ å›æŠ¥åˆ†å¸ƒ Z(s,a)ï¼Œæ•æ‰é£é™©ä¸ä¸ç¡®å®šæ€§ï¼Œä½¿æ™ºèƒ½ä½“çš„ç­–ç•¥æ›´ç¨³å¥ã€æ›´å…·è¡¨è¾¾åŠ›ã€‚</p>

      <h4>1. å›ºå®šæ”¯æŒåŒºé—´ä¸ 51 ä¸ªç¦»æ•£åŸå­ï¼ˆatomsï¼‰</h4>
      <ul>
        <li>C51 å°†æœªæ¥å›æŠ¥çš„èŒƒå›´å›ºå®šåœ¨ <code>[V_min, V_max]</code> ä¸Šï¼Œå¹¶å‡åŒ€åˆ’åˆ†ä¸º 51 ä¸ªå–å€¼ç‚¹ï¼š</li>
  $$z_i = V_{\min} + i\,\frac{V_{\max}-V_{\min}}{50},\quad i=0,1,\dots,50$$
        <li>æ¯ä¸ªåŠ¨ä½œ a è¾“å‡º 51 ä¸ªæ¦‚ç‡ <code>p_i(s,a)</code>ï¼Œè¡¨ç¤ºå›æŠ¥è½åœ¨ z_i å¤„çš„æ¦‚ç‡ã€‚</li>
      </ul>

      <h4>2. V<sub>min</sub> / V<sub>max</sub> çš„é€‰å–ä¾æ®</h4>
      <ul>
        <li>ç†è®ºä¸Šï¼šè‹¥æ¯æ­¥å¥–åŠ±èŒƒå›´ä¸º <code>[r_min, r_max]</code>ï¼ŒæŠ˜æ‰£å› å­ä¸º Î³ï¼Œåˆ™æœªæ¥ç´¯è®¡å›æŠ¥çš„ä¸Šä¸‹é™ä¸ºï¼š
  $$V_{\min} = \frac{r_{\min}}{1-\gamma},\quad V_{\max} = \frac{r_{\max}}{1-\gamma}$$</li>
        <li>å®è·µä¸­å¯æ ¹æ®ä»»åŠ¡ç»éªŒé€‰å–æ›´ç´§åŒºé—´ã€‚ä¾‹å¦‚ï¼š
          <ul>
            <li>Atari ç¯å¢ƒï¼šé€šå¸¸è®¾ä¸º [-10, 10] æˆ– [-100, 100]</li>
            <li>è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼ˆå¦‚ MuJoCoï¼‰ï¼šæ ¹æ®å¥–åŠ±å°ºåº¦é€‚å½“ç¼©æ”¾</li>
          </ul>
        </li>
        <li>å›ºå®š <code>[V_min, V_max]</code> ä½¿ç½‘ç»œè¾“å‡ºç»´åº¦ç¨³å®šï¼Œä¾¿äºåˆ†å¸ƒæŠ•å½±ã€‚</li>
      </ul>

      <h4>3. ç½‘ç»œè¾“å‡ºä¸åŠ¨ä½œé€‰æ‹©</h4>
      <ul>
        <li>è¾“å‡ºï¼šæ¯ä¸ªåŠ¨ä½œå¯¹åº” 51 ç»´æ¦‚ç‡åˆ†å¸ƒï¼Œç» softmax å½’ä¸€åŒ–ã€‚</li>
        <li>è®­ç»ƒï¼šä¼˜åŒ–æ•´æ¡åˆ†å¸ƒï¼›æ‰§è¡ŒåŠ¨ä½œæ—¶å–æœŸæœ›ï¼š
  $$Q(s,a) = \sum_i p_i(s,a) z_i$$</li>
      </ul>

      <h4>4. åˆ†å¸ƒå¼ Bellman æ›´æ–°ä¸æŠ•å½±</h4>
      <ol>
        <li>ç”¨ Double DQN æ€æƒ³é€‰æ‹©ä¸‹ä¸€åŠ¨ä½œï¼š
  $$a^* = \arg\max_{a} \sum_i p_i(s',a) z_i$$</li>
        <li>æ„é€ ç›®æ ‡åˆ†å¸ƒï¼š
  $$T Z = r + \gamma \; Z_{target}(s', a^*)$$</li>
        <li>å°†å…¶çº¿æ€§æŠ•å½±å›å›ºå®šæ”¯æŒåŒºé—´ï¼Œå¾—åˆ°ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒ m_iã€‚</li>
      </ol>

      <h4>5. æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µï¼ˆCross-Entropyï¼‰</h4>
      <p>è®­ç»ƒç›®æ ‡ï¼šè®©é¢„æµ‹åˆ†å¸ƒ <code>p</code> é€¼è¿‘ç›®æ ‡åˆ†å¸ƒ <code>m</code>ã€‚</p>
  $$L = - \sum_i m_i \log p_i$$

      <h5>äº¤å‰ç†µæœ€å°å€¼è¯æ˜ï¼ˆåŸºäº Jensen ä¸ç­‰å¼ï¼‰</h5>
  <p>å®šä¹‰äº¤å‰ç†µï¼š$$H(p,q) = -\sum_i p_i \ln q_i$$ ï¼Œè¯æ˜å½“ $p=q$ æ—¶ $H(p,q)$ æœ€å°ã€‚</p>
  $$H(p,q)-H(p,p)=\sum_i p_i \ln \frac{q_i}{p_i}$$
  <p>ç”±äº $\ln(x)$ ä¸ºå‡¹å‡½æ•°ï¼ŒJensen ä¸ç­‰å¼ï¼š</p>
  $$\sum_i p_i \ln x_i \le \ln\Big(\sum_i p_i x_i\Big),\; \sum_i p_i = 1$$
  <p>å– $x_i = q_i/p_i$ å¾—ï¼š</p>
  $$\sum_i p_i \ln \frac{q_i}{p_i} \le \ln\Big(\sum_i q_i\Big)=\ln 1=0$$
  $$\Rightarrow\; H(p,p) - H(p,q) \le 0 \;\Rightarrow\; H(p,p) \le H(p,q)$$
  <p>å½“ $p=q$ æ—¶ç­‰å·æˆç«‹ã€‚</p>
      <p>å› æ­¤ï¼Œé¢„æµ‹åˆ†å¸ƒç­‰äºç›®æ ‡åˆ†å¸ƒæ™‚ï¼Œäº¤å‰ç†µå–å¾—æœ€å°å€¼ï¼Œè¿™å°±æ˜¯åˆ†å¸ƒå­¦ä¹ ç¨³å®šçš„ç†è®ºåŸºç¡€ã€‚</p>

      <h4>6. å°ç»“</h4>
      <ul>
        <li><strong>N-Stepï¼š</strong> æ˜¯ TD ä¸ Monte Carlo çš„ä¸­é—´å½¢æ€ï¼Œç»“åˆçŸ­æœŸç¨³å®šä¸é•¿æœŸè§†é‡ï¼ŒåŠ å¿«å¥–åŠ±ä¼ æ’­ã€‚</li>
        <li><strong>C51ï¼š</strong> ç›´æ¥å­¦ä¹ å›æŠ¥åˆ†å¸ƒï¼ˆ51 ä¸ªæ¦‚ç‡ï¼‰ï¼Œé€šè¿‡äº¤å‰ç†µä¼˜åŒ–é¢„æµ‹åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚</li>
        <li><strong>V<sub>min</sub>/V<sub>max</sub>ï¼š</strong> è¡¨ç¤ºç¯å¢ƒå¯èƒ½å›æŠ¥çš„å…¨å±€ä¸Šä¸‹ç•Œï¼Œé€šå¸¸è®¾ä¸º <code>[r_min/(1-Î³), r_max/(1-Î³)]</code> æˆ–ç»éªŒèŒƒå›´ã€‚</li>
        <li>ä¸¤è€…å‡ä¸º Rainbow DQN çš„å…³é”®ç»„æˆæ¨¡å—ï¼Œä¸ Doubleã€Duelingã€PERã€NoisyNet å…±åŒç»„æˆå®Œæ•´å¼ºåŒ–å­¦ä¹ å½©è™¹ä½“ç³»ã€‚</li>
      </ul>

      <hr>

      <h3>å¼ºåŒ–å­¦ä¹ è¿›é˜¶ï¼šRainbow DQN ğŸŒˆï¼ˆç»ˆæ DQN æ”¹è¿›ç‰ˆï¼‰</h3>

      <p><strong>è®ºæ–‡ï¼š</strong> Hessel et al., "<em>Rainbow: Combining Improvements in Deep Reinforcement Learning</em>", DeepMind, 2018</p>

      <p><strong>æ ¸å¿ƒæ€æƒ³ï¼š</strong> Rainbow å°†å…­å¤§å¼ºåŒ–å­¦ä¹ æ”¹è¿›æ¨¡å—æ•´åˆå…¥ DQN æ¡†æ¶ï¼Œä½¿æ™ºèƒ½ä½“åœ¨ç¨³å®šæ€§ã€æ ·æœ¬æ•ˆç‡ã€æ¢ç´¢èƒ½åŠ›ä¸è¡¨è¾¾èƒ½åŠ›ä¸Šå…¨é¢æå‡ã€‚</p>

      <h4>1. Rainbow çš„å…­å¤§ç»„æˆæ¨¡å—</h4>

      <table>
        <tr><th>æ¨¡å—</th><th>æ”¹è¿›æ–¹å‘</th><th>ä¸»è¦ä½œç”¨</th></tr>
        <tr><td><strong>Double DQN</strong></td><td>ç›®æ ‡è®¡ç®—</td><td>å‡å°‘ Q å€¼è¿‡ä¼°è®¡åå·®</td></tr>
        <tr><td><strong>Dueling DQN</strong></td><td>ç½‘ç»œç»“æ„</td><td>åˆ†ç¦»çŠ¶æ€ä»·å€¼ä¸åŠ¨ä½œä¼˜åŠ¿</td></tr>
        <tr><td><strong>PERï¼ˆPrioritized Experience Replayï¼‰</strong></td><td>æ•°æ®é‡‡æ ·</td><td>æé«˜å…³é”®æ ·æœ¬é‡‡æ ·é¢‘ç‡</td></tr>
        <tr><td><strong>NoisyNet</strong></td><td>æ¢ç´¢ç­–ç•¥</td><td>åœ¨å‚æ•°ä¸­æ³¨å…¥å¯å­¦ä¹ å™ªå£°ï¼Œæ›¿ä»£ Îµ-greedy</td></tr>
        <tr><td><strong>N-Step Learning</strong></td><td>å­¦ä¹ ä¿¡å·</td><td>åŠ é€Ÿå¥–åŠ±ä¼ æ’­ï¼Œå…¼é¡¾ TD ä¸ Monte Carlo</td></tr>
        <tr><td><strong>C51</strong></td><td>ä»·å€¼è¡¨ç¤º</td><td>é¢„æµ‹å›æŠ¥åˆ†å¸ƒè€Œéå•ä¸€æœŸæœ›å€¼</td></tr>
      </table>

      <p>è¿™å…­ä¸ªæ¨¡å—æ„æˆäº† DQN çš„"å½©è™¹"å¢å¼ºä½“ç³»ã€‚</p>

      <hr>

      <h4>2. ç½‘ç»œç»“æ„ï¼ˆDueling + NoisyNet + C51ï¼‰</h4>

      <ul>
        <li><strong>Dueling ç»“æ„ï¼š</strong> å…±äº«ç‰¹å¾æå–å±‚ååˆ†ä¸ºä¸¤æ”¯ï¼š
        <pre><code>Q(s,a) = V(s) + [A(s,a) - mean(A(s,Â·))]</code></pre>
        ä½¿ç½‘ç»œèƒ½åŒºåˆ†"çŠ¶æ€æœ¬èº«ä»·å€¼"ä¸"åŠ¨ä½œå¸¦æ¥çš„å¢ç›Š"ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚</li>

        <li><strong>NoisyNetï¼š</strong> å°†å…¨è¿æ¥å±‚æ›¿æ¢ä¸ºå¸¦å™ªå£°çº¿æ€§å±‚ï¼š
        <pre><code>W' = W + Ïƒ_W âŠ™ Îµ_W,   b' = b + Ïƒ_b âŠ™ Îµ_b</code></pre>
        æ¯æ¬¡å‰å‘ä¼ æ’­å‰é‡‡æ ·éšæœºå™ªå£° Îµï¼ŒÏƒ ä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œå½¢æˆå¯å­¦ä¹ æ¢ç´¢æœºåˆ¶ã€‚</li>

        <li><strong>C51 è¾“å‡ºå±‚ï¼š</strong> æ¯ä¸ªåŠ¨ä½œè¾“å‡º 51 ä¸ª logitsï¼Œç» softmax å¾—åˆ°åˆ†å¸ƒæ¦‚ç‡ï¼š
        <pre><code>p_i(s,a) = softmax(z_i)</code></pre>
  æœ€ç»ˆ Q å€¼ä¸ºæœŸæœ›ï¼š
  <div class="equation">$$Q(s,a) = \sum_i p_i(s,a)\, z_i$$</div></li>
      </ul>

      <hr>

      <h4>3. æ•°æ®ä¸ç›®æ ‡æ„å»ºï¼ˆN-Step + Double + C51 Projectionï¼‰</h4>

      <h5>3.1 N-Step æ»‘åŠ¨çª—å£æœºåˆ¶</h5>
      <p>ç»´æŠ¤é•¿åº¦ä¸º N çš„é˜Ÿåˆ— <code>n_step_buffer</code>ï¼Œæ¯æ¬¡äº¤äº’åå­˜å…¥æ ·æœ¬ï¼š</p>
      <pre><code>(s_t, a_t, r_t, s_{t+1}, done_t)</code></pre>
      <p>å½“é˜Ÿåˆ—æ»¡ N æ­¥æ—¶ï¼Œè®¡ç®—ç´¯è®¡æŠ˜æ‰£å¥–åŠ±ï¼š</p>
  <div class="equation">$$R = \sum_{i=0}^{N-1} \gamma^{i} r_{t+i}$$</div>
      <p>å¹¶ç”Ÿæˆ N-Step æ ·æœ¬ <code>(s_t, a_t, R, s_{t+N}, done_{t+N})</code> å­˜å…¥å›æ”¾æ± ã€‚</p>

      <h5>3.2 Double DQN ç›®æ ‡åŠ¨ä½œé€‰æ‹©</h5>
      <ul>
        <li>åœ¨çº¿ç½‘ç»œï¼ˆQ<sub>online</sub>ï¼‰é€‰åŠ¨ä½œï¼š  
  $$a^* = \arg\max_{a} \sum_i p_i(s',a) z_i$$</li>
        <li>ç›®æ ‡ç½‘ç»œï¼ˆQ<sub>target</sub>ï¼‰ç”¨æ¥è¯„ä¼°åˆ†å¸ƒï¼š  
        <pre><code>Z_target = Z_target(s', a*)</code></pre></li>
      </ul>

      <h5>3.3 C51 åˆ†å¸ƒå¼ Bellman æŠ•å½±</h5>
      <p>å¯¹ç›®æ ‡åˆ†å¸ƒçš„æ¯ä¸ªåŸå­ z<sub>j</sub> è®¡ç®—ä»¿å°„å¹³ç§»ï¼š</p>
  $$t_z = \operatorname{clip}(r_{acc} + \gamma^{N} z_j, V_{\min}, V_{\max})$$
      <p>å°†å…¶æŠ•å½±å›å›ºå®šæ”¯æŒç‚¹é›†åˆ <code>{z_i}</code> ä¸Šï¼ŒæŒ‰çº¿æ€§æƒé‡åˆ†æ‘Šæ¦‚ç‡è´¨é‡ï¼Œå¾—åˆ°ç›®æ ‡åˆ†å¸ƒ mã€‚</p>

      <hr>

      <h4>4. æŸå¤±å‡½æ•°ä¸ PER ä¼˜å…ˆé‡‡æ ·</h4>

      <h5>4.1 äº¤å‰ç†µä¸ KL æ•£åº¦çš„å…³ç³»</h5>

      <p>ç»™å®šç›®æ ‡åˆ†å¸ƒ <code>p</code>ï¼ˆçœŸå®ï¼‰ä¸é¢„æµ‹åˆ†å¸ƒ <code>q</code>ï¼ˆæ¨¡å‹è¾“å‡ºï¼‰ï¼Œæœ‰ï¼š</p>
  $$D_{KL}(p\Vert q) = \sum_x p(x) \log \frac{p(x)}{q(x)},\quad H(p,q) = -\sum_x p(x) \log q(x)$$
      <p>å±•å¼€åå¯å¾—ï¼š</p>
  $$H(p,q)=H(p)+D_{KL}(p\Vert q)$$
      <p>ç”±äº H(p) æ˜¯å¸¸æ•°ï¼Œæœ€å°åŒ–äº¤å‰ç†µ <code>H(p,q)</code> ç­‰ä»·äºæœ€å°åŒ– KL æ•£åº¦ <code>D_KL(p||q)</code>ã€‚</p>
      <p>å› æ­¤ï¼ŒC51 ä½¿ç”¨çš„äº¤å‰ç†µæŸå¤±ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯æœ€å°åŒ–ç›®æ ‡åˆ†å¸ƒä¸é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„ KL å·®å¼‚ã€‚</p>

      <h5>4.2 ä¸‰ç§å¸¸è§æŸå¤±å‡½æ•°è®¾è®¡æ–¹å¼</h5>

      <table>
        <tr><th>ç±»å‹</th><th>å®šä¹‰</th><th>è¯´æ˜</th></tr>
        <tr>
          <td><strong>(1) åˆ†å¸ƒäº¤å‰ç†µæŸå¤±</strong></td>
          <td>$$L = -\sum_i m_i \log p_i$$</td>
          <td>æœ€å¸¸ç”¨ï¼Œä¸ C51 åŸè®ºæ–‡ä¸€è‡´ï¼›ç›´æ¥åº¦é‡åˆ†å¸ƒå·®å¼‚ï¼Œç­‰ä»·äºæœ€å°åŒ– KL æ•£åº¦ã€‚</td>
        </tr>
        <tr>
          <td><strong>(2) æœŸæœ› TD è¯¯å·®æŸå¤±</strong></td>
          <td><code>L = (R + Î³^NÂ·Q_target - Q_online)^2</code></td>
          <td>å°†åˆ†å¸ƒå–æœŸæœ›åå›é€€åˆ°ä¼ ç»Ÿ TD å½¢å¼ï¼Œè®¡ç®—æ›´ç›´è§‚ï¼Œä½†ä¸¢å¤±åˆ†å¸ƒä¿¡æ¯ã€‚</td>
        </tr>
        <tr>
          <td><strong>(3) æ··åˆæŸå¤±</strong></td>
          <td><code>L = Î»Â·L_CE + (1âˆ’Î»)Â·L_TD</code></td>
          <td>èåˆäº¤å‰ç†µä¸ TD è¯¯å·®ä¿¡å·ï¼Œå…¼é¡¾åˆ†å¸ƒæ‹Ÿåˆä¸ç¨³å®šæ€§ï¼ˆÎ»â‰ˆ0.5 å¸¸ç”¨ï¼‰ã€‚</td>
        </tr>
      </table>

      <p>ä¸€èˆ¬æ¨èä½¿ç”¨åˆ†å¸ƒäº¤å‰ç†µæŸå¤±ï¼ˆæ–¹æ¡ˆ 1ï¼‰ï¼Œä¸ C51 ç†è®ºå®Œå…¨ä¸€è‡´ï¼›åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œä¸ºå¢å¼ºç¨³å®šæ€§å¯ä½¿ç”¨æ··åˆæŸå¤±ã€‚</p>

      <h5>4.3 PER ä¼˜å…ˆçº§è®¡ç®—ï¼ˆåˆ†å¸ƒå¼ç‰ˆæœ¬ï¼‰</h5>
      <ul>
  <li><strong>æ–¹æ³• 1ï¼š</strong> ä½¿ç”¨æ ·æœ¬äº¤å‰ç†µ $$CE_i = -\sum_i m_i \log p_i$$ ä½œä¸ºä¼˜å…ˆçº§ã€‚</li>
        <li><strong>æ–¹æ³• 2ï¼š</strong> å–åˆ†å¸ƒæœŸæœ›è®¡ç®— TD è¯¯å·® <code>|Î´| = |y - Q|</code>ã€‚</li>
        <li><strong>æ–¹æ³• 3ï¼š</strong> æ··åˆä¼˜å…ˆçº§ <code>prio_i = Î»Â·CE_i + (1âˆ’Î»)Â·|Î´|</code>ã€‚</li>
      </ul>
      <p>é‡‡æ ·æ¦‚ç‡ä¸é‡è¦æ€§æƒé‡å¦‚ä¸‹ï¼š</p>
      <pre><code>P(i) = prio_i^Î± / Î£_j prio_j^Î±
w_i = (NÂ·P(i))^-Î²
</code></pre>
      <p>å…¶ä¸­ Î±â‰ˆ0.6 æ§åˆ¶é‡‡æ ·åå¥½å¼ºåº¦ï¼ŒÎ² ä» 0.4 çº¿æ€§ä¸Šå‡è‡³ 1.0 ç”¨äºæ ¡æ­£é‡‡æ ·åå·®ã€‚</p>

      <hr>

      <h4>5. ç›®æ ‡ç½‘ç»œæ›´æ–°ä¸å™ªå£°åˆ·æ–°</h4>
      <ul>
        <li>Target ç½‘ç»œå‚æ•°æ¯ K æ­¥"ç¡¬æ›´æ–°"ï¼š<code>Î¸^- â† Î¸</code>ï¼ˆä¾‹å¦‚æ¯ 2000 æ­¥ï¼‰ã€‚</li>
        <li>æˆ–é‡‡ç”¨è½¯æ›´æ–°ï¼š<code>Î¸^- â† Ï„Î¸ + (1âˆ’Ï„)Î¸^âˆ’</code>ï¼ŒÏ„â‰ˆ1eâˆ’3ã€‚</li>
        <li>æ¯æ¬¡å‰å‘ä¼ æ’­å‰ <code>sample_noise()</code>ï¼Œä»¥åˆ·æ–°æ¢ç´¢å™ªå£°ã€‚</li>
      </ul>

      <hr>

      <h4>6. è®­ç»ƒä¸»æµç¨‹ï¼ˆä¼ªä»£ç ï¼‰</h4>

<pre><code class="language-python"># Rainbow DQN ä¸»å¾ªç¯

Initialize Q_online, Q_target â† Q_online
Initialize prioritized replay buffer

for each episode:
    s â† env.reset()
    n_step_buffer â† deque(maxlen=N)

    while not done:
        # 1. ä½¿ç”¨ NoisyNet é€‰æ‹©åŠ¨ä½œ
        a â† argmax_a E[Z(s,a;Î¸)]
        s_next, r, done â† env.step(a)
        n_step_buffer.append((s, a, r, s_next, done))

        # 2. è‹¥ n_step_buffer æ»¡ N æ­¥ï¼Œç”Ÿæˆ N-Step æ ·æœ¬
        if len(n_step_buffer) == N:
            R â† Î£ Î³^iÂ·r_{t+i}
            store_transition(s, a, R, s_{t+N}, done)

        s â† s_next

        # 3. æ¯æ­¥è®­ç»ƒ
        batch â† replay_buffer.sample(B)
        compute target distribution m
  compute loss  \(L = -\sum_i m_i \log p_i\)
        update Î¸ with weighted loss
        update priorities with per-sample loss
        periodically update Q_target
</code></pre>

      <hr>

      <h4>7. è¶…å‚æ•°å‚è€ƒ</h4>

      <table>
        <tr><th>å‚æ•°</th><th>å…¸å‹å€¼</th><th>è¯´æ˜</th></tr>
        <tr><td>æŠ˜æ‰£ Î³</td><td>0.99</td><td>é•¿æœŸå¥–åŠ±æŠ˜æ‰£</td></tr>
        <tr><td>N-Step</td><td>3</td><td>å¥–åŠ±ä¼ æ’­æ­¥æ•°</td></tr>
        <tr><td>Î±</td><td>0.6</td><td>PER é‡‡æ ·åå¥½å¼ºåº¦</td></tr>
        <tr><td>Î²</td><td>0.4 â†’ 1.0</td><td>é‡è¦æ€§æƒé‡ä¿®æ­£</td></tr>
        <tr><td>V_min, V_max</td><td>[-10, 10] æˆ– [-100, 100]</td><td>C51 æ”¯æŒåŒºé—´</td></tr>
        <tr><td>å­¦ä¹ ç‡</td><td>1eâˆ’4</td><td>Adam ä¼˜åŒ–å™¨</td></tr>
        <tr><td>æ‰¹é‡å¤§å°</td><td>32 / 64</td><td>è®­ç»ƒæ‰¹æ¬¡å¤§å°</td></tr>
      </table>

      <hr>

      <h4>8. æ€»ç»“</h4>

      <ul>
        <li><strong>Rainbow</strong> æ˜¯ DQN çš„å…¨é¢æ•´åˆç‰ˆæœ¬ï¼Œèåˆå…­å¤§æ¨¡å—ï¼š</li>
        <pre><code>Rainbow = Dueling + NoisyNet + PER + N-Step + Double + C51</code></pre>
        <li>åœ¨çº¿ç½‘ç»œé€‰åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°åˆ†å¸ƒï¼›N-Step å¥–åŠ±ä¼ æ’­æ›´å¿«ï¼›äº¤å‰ç†µæŸå¤±æœ€å°åŒ–é¢„æµ‹ä¸ç›®æ ‡åˆ†å¸ƒçš„ KL æ•£åº¦å·®è·ã€‚</li>
        <li>ç»“åˆæ‰€æœ‰æ”¹è¿›åï¼ŒRainbow åœ¨ Atari ç³»åˆ—ç¯å¢ƒä¸­å¤§å¹…è¶…è¶ŠåŸå§‹ DQN ä¸å„ç‹¬ç«‹å˜ä½“ï¼Œæˆä¸ºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é‡è¦åŸºçº¿ç®—æ³•ã€‚</li>
      </ul>
    </section>

    <section id="chapter5" class="chapter">
      <h2>ç¬¬äº”ç« ï¼šDPG & DDPGï¼ˆä»ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦åˆ°æ·±åº¦å®ç°ï¼‰</h2>

      <p><strong> æ·±å…¥ç†è§£ DPGï¼ˆDeterministic Policy Gradientï¼‰ï¼šä»éšæœºåˆ°ç¡®å®šæ€§ç­–ç•¥çš„æ¡¥æ¢</strong></p>

      <p>å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆPolicy Gradientï¼‰å®¶æ—ä¸­ï¼Œä»æœ€æ—©çš„ REINFORCE åˆ° Actor-Criticï¼ˆACï¼‰ã€A2C/A3C å†åˆ° DPG/DDPG/TD3/SACï¼Œå…¶æ ¸å¿ƒæ€æƒ³ä¸€ç›´å›´ç»•ä¸€ä¸ªé—®é¢˜ï¼š</p>
      <blockquote>

      <p><strong>å¦‚ä½•ç›´æ¥ä¼˜åŒ–ä¸€ä¸ªå‚æ•°åŒ–çš„ç­–ç•¥ï¼Œä½¿å¾—é•¿æœŸå›æŠ¥æœ€å¤§ï¼Ÿ</strong></p>
      </blockquote>

      <p>DPGï¼ˆDeterministic Policy Gradientï¼‰æ˜¯å…¶ä¸­çš„é‡è¦åˆ†æ”¯ï¼Œå®ƒé€šè¿‡å°†éšæœºç­–ç•¥ç®€åŒ–ä¸ºç¡®å®šæ€§å‡½æ•°ï¼Œå¤§å¹…é™ä½æ–¹å·®ã€æé«˜å­¦ä¹ æ•ˆç‡ï¼Œæˆä¸ºè¿ç»­åŠ¨ä½œæ§åˆ¶ä»»åŠ¡ï¼ˆå¦‚æœºæ¢°è‡‚ã€æ— äººé©¾é©¶ã€ä»¿çœŸæ§åˆ¶ç­‰ï¼‰çš„å…³é”®ç®—æ³•ã€‚</p>

      <hr>

      <h3>ä¸€ã€ä»éšæœºåˆ°ç¡®å®šæ€§ï¼šDPG çš„è¯ç”ŸåŠ¨æœº</h3>

      <p>åœ¨ç»å…¸çš„ Actor-Critic (AC) ç®—æ³•ä¸­ï¼Œç­–ç•¥æ˜¯<strong>éšæœºçš„</strong>ï¼š</p>
      <pre><code>Ï€_Î¸(a|s)</code></pre>
      <p>ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šçŠ¶æ€ $s$ï¼ŒActor è¾“å‡ºä¸€ä¸ªåŠ¨ä½œåˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯é«˜æ–¯åˆ†å¸ƒï¼‰ï¼Œå†ä»ä¸­é‡‡æ ·åŠ¨ä½œ $a$ã€‚</p>

      <p>è¿™æ ·çš„å¥½å¤„æ˜¯â€”â€”å¯ä»¥è‡ªç„¶åœ°å®ç°æ¢ç´¢ã€‚ä½†é—®é¢˜ä¹Ÿå¾ˆæ˜æ˜¾ï¼š</p>
      <ul>
        <li><strong>æ–¹å·®é«˜</strong>ï¼šæ¢¯åº¦ä¼°è®¡ä¾èµ–é‡‡æ ·ï¼Œæ›´æ–°æ–¹å‘å™ªå£°å¤§ï¼›</li>
        <li><strong>æ•ˆç‡ä½</strong>ï¼šè¿ç»­åŠ¨ä½œç©ºé—´ä¸‹çš„ç§¯åˆ†æéš¾ä¼°è®¡ï¼›</li>
        <li><strong>ä¸ç¨³å®š</strong>ï¼šåŠ¨ä½œé‡‡æ ·å¼•å…¥çš„éšæœºæ€§å¢åŠ äº†ç­–ç•¥å­¦ä¹ çš„ä¸ç¡®å®šæ€§ã€‚</li>
      </ul>

      <p>äºæ˜¯ï¼ŒSilver ç­‰äººåœ¨ 2014 å¹´æå‡ºäº† <strong>DPG</strong>ï¼ˆè®ºæ–‡ï¼š<em>Deterministic Policy Gradient Algorithms</em>, ICML 2014ï¼‰ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š</p>
      <blockquote>
        <p>åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è®©ç­–ç•¥ç›´æ¥è¾“å‡ºä¸€ä¸ª<strong>ç¡®å®šæ€§çš„åŠ¨ä½œ</strong> $a = \mu_\theta(s)$ï¼Œè€Œä¸æ˜¯åŠ¨ä½œåˆ†å¸ƒã€‚</p>
      </blockquote>

      <hr>

      <h3>äºŒã€DPG å®šç†ï¼ˆDeterministic Policy Gradient Theoremï¼‰</h3>

      <p>DPG ç†è®ºçš„æ ¹åŸºå°±æ˜¯è¿™ä¸ªå®šç†ï¼š</p>

      <div style="background: rgba(255, 140, 66, 0.1); padding: 15px; border-left: 4px solid #ff8c42; margin: 20px 0;">
        <p><strong>Theorem (Deterministic Policy Gradient Theorem)</strong></p>
        <p>è®¾ç­–ç•¥ä¸ºç¡®å®šæ€§å‡½æ•° $\mu_\theta(s)$ï¼Œå…¶æ€§èƒ½ç›®æ ‡ä¸ºï¼š</p>
        <p>$$
        J(\mu_\theta) = \mathbb{E}_{s \sim \rho^\mu}[Q^\mu(s, \mu_\theta(s))]
        $$</p>
        <p>å…¶ä¸­ $\rho^\mu` æ˜¯ç­–ç•¥ $\mu$ ä¸‹çš„æŠ˜æ‰£çŠ¶æ€åˆ†å¸ƒã€‚</p>
        <p>åˆ™åœ¨æ»¡è¶³å¯å¯¼æ¡ä»¶ä¸‹ï¼Œæœ‰ï¼š</p>
        <p>$$
        \nabla_\theta J(\mu_\theta) = \mathbb{E}_{s \sim \rho^\beta}\left[\nabla_\theta \mu_\theta(s) \, \nabla_a Q^\mu(s,a)\Big|_{a=\mu_\theta(s)}\right]
        $$</p>
        <p>å…¶ä¸­ $\rho^\beta$ ä¸ºä»»æ„è¡Œä¸ºç­–ç•¥çš„çŠ¶æ€åˆ†å¸ƒï¼Œå› æ­¤è¯¥æ¢¯åº¦å¯<strong>ç¦»ç­–ç•¥ï¼ˆoff-policyï¼‰</strong>ä¼°è®¡ã€‚</p>
      </div>

      <p><em>ï¼ˆè¯¦ç»†è¯æ˜è§è®ºæ–‡ï¼šSilver et al., 2014, ICMLï¼‰</em></p>

      <h4>ğŸ“˜ å®šç†è¦ç‚¹è§£è¯»</h4>
      <ul>
        <li>ä¸å†éœ€è¦ $\log \pi_\theta(a|s)$ï¼›</li>
        <li>æ¢¯åº¦é€šè¿‡é“¾å¼æ³•åˆ™ç›´æ¥è®¡ç®—ï¼š$\nabla_\theta \mu_\theta(s) \times \nabla_a Q(s,a)$ï¼›</li>
        <li>ä¸éœ€è¦å¯¹åŠ¨ä½œç©ºé—´ç§¯åˆ†ï¼›</li>
        <li>è®¡ç®—æ–¹å·®å¤§å¤§é™ä½ï¼›</li>
        <li>å¯ä»¥ç¦»ç­–ç•¥è®­ç»ƒï¼ˆä½¿ç”¨ç»éªŒå›æ”¾ï¼‰ã€‚</li>
      </ul>

      <hr>

      <h3>ä¸‰ã€ä¸ºä»€ä¹ˆå¯ä»¥ç¡®å®šæ€§è¾“å‡º Î¼(s)ï¼Ÿ</h3>

  <p>åœ¨éšæœºç­–ç•¥ä¸‹ï¼š</p>
  $$a \sim \pi_\theta(a\mid s)$$

      <p>åœ¨è¿ç»­åŠ¨ä½œä»»åŠ¡ä¸­ï¼Œé€šå¸¸å‡è®¾ $\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma^2 I)$ã€‚</p>
  <p>å¦‚æœæˆ‘ä»¬è®© $\sigma \rightarrow 0$ï¼Œåˆ™åˆ†å¸ƒæ”¶æ•›ä¸º $\delta$ åˆ†å¸ƒï¼š</p>
  $$\pi_\theta(a\mid s) \;\Rightarrow\; \delta\big(a - \mu_\theta(s)\big)$$

  <p>æ­¤æ—¶æ¢¯åº¦ä»</p>
  $$\nabla_\theta J = \mathbb{E}\big[\nabla_\theta \log \pi_\theta(a\mid s)\; Q(s,a)\big]$$
  <p>è‡ªç„¶è¿‡æ¸¡åˆ°</p>
  $$\nabla_\theta J = \mathbb{E}\big[\nabla_\theta \mu_\theta(s)\; \nabla_a Q(s,a)\big]$$

      <p>è¿™æ­£æ˜¯ DPG çš„æ¢¯åº¦å½¢å¼ã€‚</p>

      <p><strong>å› æ­¤ï¼šDPG æ˜¯éšæœºç­–ç•¥æ¢¯åº¦çš„é›¶æ–¹å·®æé™å½¢å¼ã€‚</strong></p>

      <hr>

      <h3>å››ã€ä¸ºä»€ä¹ˆ DPG å¯ä»¥ off-policyï¼Ÿ</h3>

      <p>DPG ä¹‹æ‰€ä»¥å¯ä»¥ off-policyï¼Œæ˜¯å› ä¸ºå®ƒçš„æ¢¯åº¦å½¢å¼<strong>ä¸ä¾èµ–åŠ¨ä½œåˆ†å¸ƒ</strong>ï¼Œåªä¾èµ–çŠ¶æ€åˆ†å¸ƒï¼›è€ŒçŠ¶æ€åˆ†å¸ƒçš„åå·®å¯ä»¥ç”¨ç»éªŒå›æ”¾è¿‘ä¼¼è¡¥é½ã€‚</p>

      <p>å…·ä½“æ¥è¯´ï¼š</p>
      <ul>
        <li>éšæœºç­–ç•¥æ¢¯åº¦ä¾èµ– $\pi(a|s)$ï¼Œå¿…é¡»æ˜¯å½“å‰ç­–ç•¥é‡‡æ ·çš„æ•°æ®ï¼›</li>
        <li>ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦åªä¾èµ– $Q(s, \mu(s))$ çš„æ¢¯åº¦æ–¹å‘ï¼Œä¸é‡‡æ ·ç­–ç•¥æ— å…³ï¼›</li>
        <li>åªè¦çŠ¶æ€åˆ†å¸ƒ $\rho^\beta$ è¦†ç›–äº† $\rho^\mu$ï¼Œæ¢¯åº¦ä¼°è®¡å°±æ˜¯æ— åçš„ï¼›</li>
        <li>ç»éªŒå›æ”¾æ± ä¸­çš„å†å²æ•°æ®å¯ä»¥è¿‘ä¼¼æä¾›è¿™ç§è¦†ç›–ã€‚</li>
      </ul>

      <hr>

      <h3>äº”ã€ä»ç†è®ºä¸Šå¦‚ä½•éªŒè¯"ç¡®å®šæ€§ç­–ç•¥å‡è®¾"æ˜¯åˆç†çš„ï¼Ÿ</h3>

      <p>ä½ å¯èƒ½ä¼šé—®ï¼š<strong>ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯ä»¥æŠŠç­–ç•¥"é™å®š"ä¸ºç¡®å®šæ€§å‡½æ•° $\mu_\theta(s)$ï¼Ÿè¿™ä¸ªå‡è®¾æ˜¯å¦ä¸¢å¤±äº†æœ€ä¼˜è§£ï¼Ÿ</strong></p>

      <p>éªŒè¯å®ƒ"æœ‰ç”¨"ä¸”"æ­£ç¡®"ï¼Œå…¶å®è¦çœ‹ä¸‰å±‚é€»è¾‘ï¼š</p>

      <h4>å±‚æ¬¡ â‘  å­˜åœ¨æ€§å±‚é¢ï¼šæœ€ä¼˜ç­–ç•¥æ˜¯å¦å¯ä»¥æ˜¯ç¡®å®šæ€§çš„ï¼Ÿ</h4>

      <div style="background: rgba(76, 175, 80, 0.1); padding: 15px; border-left: 4px solid #4CAF50; margin: 20px 0;">
        <p><strong>ç»“è®ºï¼šâœ… æ˜¯çš„ï¼ˆç”± Bellman æœ€ä¼˜æ€§åŸç†ä¿è¯ï¼‰</strong></p>
        <p><strong>ç†è®ºä¾æ®ï¼š</strong></p>
        <p>åœ¨ MDP æ¡†æ¶ä¸‹ï¼Œå¯¹äºä»»ä½•éšæœºç­–ç•¥ $\pi(a|s)$ï¼Œæ€»å­˜åœ¨ä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥ $\mu(s)$ ä½¿å¾—ï¼š</p>
        <p>$$
        V^{\mu}(s) \geq V^{\pi}(s), \quad \forall s
        $$</p>
        <p>è¿™æ˜¯å› ä¸ºæœ€ä¼˜ç­–ç•¥å¯ä»¥é€šè¿‡ Bellman æœ€ä¼˜æ–¹ç¨‹ç›´æ¥æ„é€ ï¼š</p>
        <p>$$
        \mu^*(s) = \arg\max_a Q^*(s, a)
        $$</p>
        <p>ä¹Ÿå°±æ˜¯è¯´ï¼Œ<strong>è‡³å°‘å­˜åœ¨ä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥æ˜¯æœ€ä¼˜çš„</strong>ã€‚</p>
      </div>

      <p><strong>ç›´è§‰è§£é‡Šï¼š</strong></p>
      <ul>
        <li>å¦‚æœæŸä¸ªçŠ¶æ€ä¸‹ï¼ŒåŠ¨ä½œ $a_1$ çš„ Q å€¼æœ€é«˜ï¼Œé‚£ä¹ˆæ€»æ˜¯é€‰ $a_1$ å°±æ˜¯æœ€ä¼˜çš„ï¼›</li>
        <li>æ²¡æœ‰å¿…è¦æŒ‰æ¦‚ç‡"æ·éª°å­"åœ¨ $a_1$ å’Œæ¬¡ä¼˜åŠ¨ä½œä¹‹é—´éšæœºé€‰æ‹©ï¼›</li>
        <li>éšæœºæ€§åªä¼šé™ä½æœŸæœ›å›æŠ¥ï¼ˆé™¤éæ˜¯ä¸ºäº†æ¢ç´¢ï¼‰ã€‚</li>
      </ul>

      <h4>å±‚æ¬¡ â‘¡ å¯ä¼˜åŒ–æ€§å±‚é¢ï¼šåœ¨ç¡®å®šæ€§ç­–ç•¥ç±»ä¸­ï¼Œæ€§èƒ½ç›®æ ‡ $J(\mu_\theta)$ æ˜¯å¦å¯å¾®ã€å¯ä¼˜åŒ–ï¼Ÿ</h4>

      <div style="background: rgba(76, 175, 80, 0.1); padding: 15px; border-left: 4px solid #4CAF50; margin: 20px 0;">
        <p><strong>ç»“è®ºï¼šâœ… æ˜¯çš„ï¼ˆDPG å®šç†ç»™å‡ºæ¢¯åº¦è¡¨è¾¾å¼ï¼‰</strong></p>
        <p><strong>ç†è®ºä¾æ®ï¼š</strong></p>
        <p>Silver et al. (2014) è¯æ˜äº†ï¼Œåœ¨ç¡®å®šæ€§ç­–ç•¥ä¸‹ï¼Œæ€§èƒ½ç›®æ ‡å…³äºå‚æ•° $\theta$ çš„æ¢¯åº¦ä¸ºï¼š</p>
        <p>$$
        \nabla_\theta J(\mu_\theta) = \mathbb{E}_{s \sim \rho}\left[\nabla_\theta \mu_\theta(s) \, \nabla_a Q(s,a)\Big|_{a=\mu_\theta(s)}\right]
        $$</p>
        <p>è¿™ä¸ªæ¢¯åº¦ï¼š</p>
        <ul>
          <li>âœ… <strong>å¯è®¡ç®—</strong>ï¼šé€šè¿‡é“¾å¼æ³•åˆ™ä»ç¥ç»ç½‘ç»œåå‘ä¼ æ’­ï¼›</li>
          <li>âœ… <strong>æ— å</strong>ï¼šæœŸæœ›ä¸çœŸå®æ¢¯åº¦æ–¹å‘ä¸€è‡´ï¼›</li>
          <li>âœ… <strong>ä½æ–¹å·®</strong>ï¼šä¸éœ€è¦é‡‡æ ·åŠ¨ä½œï¼Œæ–¹å·®è¿œä½äºéšæœºç­–ç•¥æ¢¯åº¦ã€‚</li>
        </ul>
      </div>

      <p><strong>ç›´è§‰è§£é‡Šï¼š</strong></p>
      <ul>
        <li>æˆ‘ä»¬å¯ä»¥æŠŠ $J(\mu_\theta)$ çœ‹ä½œæ˜¯ä¸€ä¸ªå…³äº $\theta$ çš„å‡½æ•°ï¼›</li>
        <li>åªè¦ $\mu_\theta(s)$ å’Œ $Q(s,a)$ å¯å¾®ï¼Œ$J$ å°±å¯å¾®ï¼›</li>
        <li>å› æ­¤å¯ä»¥ç”¨æ¢¯åº¦ä¸Šå‡æ³•ä¼˜åŒ–å®ƒã€‚</li>
      </ul>

      <h4>å±‚æ¬¡ â‘¢ é€¼è¿‘æ€§å±‚é¢ï¼šå‚æ•°åŒ–å‡½æ•° $\mu_\theta$ æ˜¯å¦è¶³å¤Ÿè¡¨è¾¾ä¸°å¯Œç­–ç•¥ï¼Ÿ</h4>

      <div style="background: rgba(76, 175, 80, 0.1); padding: 15px; border-left: 4px solid #4CAF50; margin: 20px 0;">
        <p><strong>ç»“è®ºï¼šâœ… è‹¥ç¥ç»ç½‘ç»œè¶³å¤Ÿå¤§/éçº¿æ€§è¶³å¤Ÿå¼ºï¼Œåˆ™å¯é€¼è¿‘ä»»æ„ç¡®å®šæ€§ç­–ç•¥</strong></p>
        <p><strong>ç†è®ºä¾æ®ï¼š</strong></p>
        <p>æ ¹æ®<strong>ä¸‡èƒ½é€¼è¿‘å®šç†ï¼ˆUniversal Approximation Theoremï¼‰</strong>ï¼š</p>
        <p>ä¸€ä¸ªè¶³å¤Ÿå®½çš„å•å±‚ç¥ç»ç½‘ç»œï¼ˆæˆ–è¶³å¤Ÿæ·±çš„å¤šå±‚ç½‘ç»œï¼‰å¯ä»¥é€¼è¿‘ä»»æ„è¿ç»­å‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}^m$ã€‚</p>
        <p>å› æ­¤ï¼Œå¯¹äºä»»æ„ç¡®å®šæ€§ç­–ç•¥ $\mu^*(s)$ï¼Œåªè¦å®ƒæ˜¯è¿ç»­çš„ï¼ˆæˆ–åˆ†æ®µè¿ç»­ï¼‰ï¼Œå°±å­˜åœ¨å‚æ•° $\theta$ ä½¿å¾—ï¼š</p>
        <p>$$
        \mu_\theta(s) \approx \mu^*(s)
        $$</p>
      </div>

      <p><strong>ç›´è§‰è§£é‡Šï¼š</strong></p>
      <ul>
        <li>ç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ª"å‡½æ•°é€¼è¿‘å™¨"ï¼›</li>
        <li>åªè¦ç½‘ç»œå¤Ÿå¤§ã€è®­ç»ƒè¶³å¤Ÿï¼Œå®ƒå¯ä»¥å­¦åˆ°å¤æ‚çš„ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼›</li>
        <li>å®è·µä¸­ï¼Œè¿™ä¸ªå‡è®¾åœ¨é«˜ç»´è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸Šè¡¨ç°å¾ˆå¥½ï¼ˆå¦‚ MuJoCoã€æœºå™¨äººæ§åˆ¶ï¼‰ã€‚</li>
      </ul>

      <hr>

      <h4>ä¸‰å±‚éªŒè¯æ€»ç»“è¡¨</h4>

      <table>
        <tr><th>å±‚æ¬¡</th><th>æ£€éªŒå†…å®¹</th><th>ç»“è®º</th></tr>
        <tr>
          <td><strong>â‘  å­˜åœ¨æ€§å±‚é¢</strong></td>
          <td>æœ€ä¼˜ç­–ç•¥æ˜¯å¦å¯ä»¥æ˜¯ç¡®å®šæ€§çš„ï¼Ÿ</td>
          <td>âœ… æ˜¯ï¼ˆç”± Bellman åŸç†ä¿è¯ï¼‰</td>
        </tr>
        <tr>
          <td><strong>â‘¡ å¯ä¼˜åŒ–æ€§å±‚é¢</strong></td>
          <td>åœ¨ç¡®å®šæ€§ç­–ç•¥ç±»ä¸­ï¼Œæ€§èƒ½ç›®æ ‡ $J(\mu_\theta)$ æ˜¯å¦å¯å¾®ã€å¯ä¼˜åŒ–ï¼Ÿ</td>
          <td>âœ… æ˜¯ï¼ˆDPG å®šç†ç»™å‡ºæ¢¯åº¦è¡¨è¾¾å¼ï¼‰</td>
        </tr>
        <tr>
          <td><strong>â‘¢ é€¼è¿‘æ€§å±‚é¢</strong></td>
          <td>å‚æ•°åŒ–å‡½æ•° $\mu_\theta$ æ˜¯å¦è¶³å¤Ÿè¡¨è¾¾ä¸°å¯Œç­–ç•¥ï¼Ÿ</td>
          <td>âœ… è‹¥ç¥ç»ç½‘ç»œè¶³å¤Ÿå¤§/éçº¿æ€§è¶³å¤Ÿå¼ºï¼Œåˆ™å¯é€¼è¿‘ä»»æ„ç¡®å®šæ€§ç­–ç•¥</td>
        </tr>
      </table>

      <p><strong>ç»“è®ºï¼šç¡®å®šæ€§ç­–ç•¥å‡è®¾æ˜¯ç†è®ºä¸Šåˆç†ã€å®è·µä¸Šå¯è¡Œçš„ã€‚</strong></p>

      <p>è¿™ä¸‰å±‚éªŒè¯å‘Šè¯‰æˆ‘ä»¬ï¼š</p>
      <ul>
        <li>æˆ‘ä»¬<strong>æ²¡æœ‰ä¸¢å¤±æœ€ä¼˜è§£</strong>ï¼ˆè‡³å°‘å­˜åœ¨ä¸€ä¸ªç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥ï¼‰ï¼›</li>
        <li>æˆ‘ä»¬<strong>å¯ä»¥æ‰¾åˆ°å®ƒ</strong>ï¼ˆæ¢¯åº¦å¯è®¡ç®—ã€æ–¹å‘æ­£ç¡®ï¼‰ï¼›</li>
        <li>æˆ‘ä»¬<strong>å¯ä»¥ç”¨ç¥ç»ç½‘ç»œè¡¨ç¤ºå®ƒ</strong>ï¼ˆä¸‡èƒ½é€¼è¿‘å®šç†ä¿è¯ï¼‰ã€‚</li>
      </ul>

      <hr>

      <h3>å…­ã€DPG ä¸ AC çš„å¯¹æ¯”</h3>

      <table>
        <tr><th>ç‰¹å¾</th><th>AC / A2C / A3C</th><th>DPG / DDPG</th></tr>
        <tr><td>ç­–ç•¥ç±»å‹</td><td>éšæœºç­–ç•¥ $\pi(a|s)$</td><td>ç¡®å®šæ€§ç­–ç•¥ $\mu(s)$</td></tr>
        <tr><td>åŠ¨ä½œé€‰æ‹©</td><td>é‡‡æ · $a \sim \pi(a|s)$</td><td>ç›´æ¥è¾“å‡º $a = \mu(s)$</td></tr>
        <tr><td>æ›´æ–°å½¢å¼</td><td>$\nabla_\theta \log \pi(a|s) A(s,a)$</td><td>$\nabla_\theta \mu(s) \nabla_a Q(s,a)$</td></tr>
        <tr><td>æ–¹å·®</td><td>é«˜</td><td>ä½</td></tr>
        <tr><td>æ•°æ®ç±»å‹</td><td>on-policy</td><td>off-policy</td></tr>
        <tr><td>æ–¹å·®æŠ‘åˆ¶</td><td>å€¼å‡½æ•°åŸºçº¿</td><td>ç›®æ ‡ç½‘ç»œ + å›æ”¾æ± </td></tr>
        <tr><td>æ¢ç´¢æ¥æº</td><td>ç­–ç•¥é‡‡æ ·</td><td>å¤–éƒ¨å™ªå£°</td></tr>
        <tr><td>ç¨³å®šæœºåˆ¶</td><td>Advantage + å¤šçº¿ç¨‹</td><td>Target Net + Replay Buffer</td></tr>
        <tr><td>å…¸å‹ä»»åŠ¡</td><td>ç¦»æ•£/ä½ç»´è¿ç»­</td><td>é«˜ç»´è¿ç»­æ§åˆ¶</td></tr>
        <tr><td>æ ¸å¿ƒç†è®º</td><td>éšæœºç­–ç•¥æ¢¯åº¦å®šç†</td><td>ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦å®šç†</td></tr>
      </table>

      <hr>

      <h3>ä¸ƒã€DDPG ç®—æ³•æµç¨‹ï¼ˆå·¥ç¨‹ç‰ˆï¼‰</h3>

      <p>DPG ç†è®ºå¾ˆä¼˜é›…ï¼Œä½†åœ¨å®è·µä¸­é€šå¸¸ä¸ DQN çš„ç¨³å®šåŒ–æŠ€å·§ç»“åˆï¼Œå½¢æˆ <strong>DDPGï¼ˆDeep DPGï¼‰</strong>ã€‚ä»¥ä¸‹æµç¨‹æ˜¯å¸¦ç›®æ ‡ç½‘ç»œå’Œå›æ”¾æ± çš„å¯ç”¨ç‰ˆã€‚</p>

      <h4>åˆå§‹åŒ–</h4>
      <ul>
        <li>éšæœºåˆå§‹åŒ– Actor $\mu_\theta$ã€Critic $Q_\phi$</li>
        <li>å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œ $\mu_{\theta'}, Q_{\phi'}$</li>
        <li>å»ºç«‹ç»éªŒå›æ”¾æ±  $\mathcal{D}$</li>
        <li>è®¾å®šè¶…å‚æ•°ï¼šå­¦ä¹ ç‡ã€$\gamma$ã€$\tau$ã€æ‰¹å¤§å°ã€å™ªå£°æ ‡å‡†å·®ç­‰</li>
      </ul>

      <h4>äº¤äº’é‡‡æ ·ï¼ˆå¸¦æ¢ç´¢å™ªå£°ï¼‰</h4>
      <p>åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼š</p>
  $$a_t = \operatorname{clip}\big(\mu_\theta(s_t) + \epsilon_t,\; \text{bounds}\big),\quad \epsilon_t \sim \mathcal{N}(0,\, \sigma_{\text{explore}}^{2} I)$$
      <p>æ‰§è¡Œ $a_t$ï¼Œæ”¶é›† $(s_t, a_t, r_t, s_{t+1}, d_t)$ï¼ŒåŠ å…¥å›æ”¾æ± ã€‚</p>

      <h4>é‡‡æ ·æ‰¹æ¬¡</h4>
      <p>ä» $\mathcal{D}$ ä¸­éšæœºé‡‡æ · $N$ æ¡ç»éªŒï¼š</p>
  $$(s_i, a_i, r_i, s'_i, d_i)$$

      <h4>Critic æ›´æ–°</h4>
  <p>ç›®æ ‡å€¼ï¼ˆTD ç›®æ ‡ï¼‰ï¼š</p>
  $$y_i = r_i + \gamma\,(1 - d_i) \, Q_{\phi'}\!\big(s'_i,\, \mu_{\theta'}(s'_i)\big)$$
  <p>Critic æŸå¤±ï¼š</p>
  $$L_Q = \frac{1}{N} \sum_i \big(Q_{\phi}(s_i,a_i) - y_i\big)^2$$
  <p>ä¼˜åŒ–ï¼š</p>
  <div class="equation">$$\phi \leftarrow \phi - \eta_Q \, \nabla_{\phi} L_Q$$</div>

      <h4>Actor æ›´æ–°ï¼ˆç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼‰</h4>
  $$\nabla_{\theta} J \;\approx\; \frac{1}{N} \sum_i \nabla_{\theta} \mu_{\theta}(s_i)\; \nabla_{a} Q_{\phi}(s_i, a)\big|_{a=\mu_{\theta}(s_i)}$$
  <p>é€šå¸¸é€šè¿‡æœ€å°åŒ–ï¼š</p>
  $$L_\pi = -\frac{1}{N} \sum_i Q_{\phi}\big(s_i, \mu_{\theta}(s_i)\big)$$
  <p>ä¼˜åŒ–ï¼š</p>
  $$\theta \leftarrow \theta - \eta_\pi \, \nabla_{\theta} L_\pi$$

      <h4>ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°</h4>
  $$\theta' \leftarrow \tau\,\theta + (1-\tau)\,\theta',\quad \phi' \leftarrow \tau\,\phi + (1-\tau)\,\phi'$$

      <h4>é‡å¤ç›´åˆ°æ”¶æ•›</h4>
      <p>å®šæœŸåœ¨æ— å™ªå£°æ¡ä»¶ä¸‹è¯„ä¼°ç­–ç•¥æ€§èƒ½ã€‚</p>

      <hr>

      <h3>å…«ã€DDPG ä¼ªä»£ç </h3>

<pre><code class="language-python"># DDPG ç®—æ³•ä¼ªä»£ç 

Initialize Actor Î¼_Î¸, Critic Q_Ï†
Initialize target networks Î¼_Î¸' â† Î¼_Î¸, Q_Ï†' â† Q_Ï†
Initialize replay buffer D
Set hyperparameters: Î³, Ï„, Ïƒ, batch_size, max_episodes

for episode = 1 to max_episodes:
    s â† env.reset()
    done â† False
    
    while not done:
        # 1. é€‰æ‹©åŠ¨ä½œï¼ˆå¸¦æ¢ç´¢å™ªå£°ï¼‰
        a â† clip(Î¼_Î¸(s) + Îµ),  Îµ âˆ¼ N(0, ÏƒÂ²)
        
        # 2. æ‰§è¡ŒåŠ¨ä½œ
        s', r, done â† env.step(a)
        
        # 3. å­˜å‚¨ç»éªŒ
        D.store((s, a, r, s', done))
        
        # 4. é‡‡æ ·æ‰¹æ¬¡è®­ç»ƒ
        if |D| â‰¥ batch_size:
            batch â† D.sample(batch_size)
            
            # 5. è®¡ç®— Critic ç›®æ ‡
            for (s_i, a_i, r_i, s'_i, d_i) in batch:
                y_i â† r_i + Î³(1 - d_i) * Q_Ï†'ï¼ˆs'_i, Î¼_Î¸'ï¼ˆs'_iï¼‰)
            
            # 6. æ›´æ–° Critic
            L_Q â† mean((Q_Ï†(s_i, a_i) - y_i)Â²)
            Ï† â† Ï† - Î·_Q * âˆ‡_Ï† L_Q
            
            # 7. æ›´æ–° Actor
            L_Ï€ â† -mean(Q_Ï†(s_i, Î¼_Î¸(s_i)))
            Î¸ â† Î¸ - Î·_Ï€ * âˆ‡_Î¸ L_Ï€
            
            # 8. è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            Î¸' â† Ï„Î¸ + (1-Ï„)Î¸'
            Ï†' â† Ï„Ï† + (1-Ï„)Ï†'
        
        s â† s'
</code></pre>

      <hr>

      <h3>ä¹ã€ç›´è§‰æ€»ç»“</h3>

      <p><strong>åœ¨ AC ä¸­ï¼š</strong></p>
      <ul>
        <li>Actor å­¦ä¹ ä¸€ä¸ªåˆ†å¸ƒï¼ŒCritic è¯„ä¼°åŠ¨ä½œçš„å¥½åã€‚</li>
        <li>Actor æ›´æ–°ä¾èµ– $\log \pi$ï¼Œå­˜åœ¨é‡‡æ ·æ–¹å·®ã€‚</li>
      </ul>

      <p><strong>åœ¨ DPG ä¸­ï¼š</strong></p>
      <ul>
        <li>Actor ç›´æ¥ç»™å‡ºåŠ¨ä½œï¼ŒCritic é€šè¿‡ $Q(s,a)$ æ›²é¢å‘Šè¯‰ Actor å“ªä¸ªæ–¹å‘æ›´å¥½ã€‚</li>
        <li>Actor é¡ºç€ $\nabla_a Q$ çš„ä¸Šå‡æ–¹å‘æ›´æ–°ï¼Œåƒæ˜¯åœ¨çˆ¬ä»·å€¼å‡½æ•°çš„"å±±ä¸˜"ã€‚</li>
      </ul>

      <blockquote>
        <p><strong>ä¸€å¥è¯è®°ä½ï¼š</strong></p>
        <p>AC åœ¨æ·éª°å­å­¦ç­–ç•¥ï¼›<br>
        DPG ç›´æ¥çˆ¬å±±æ‰¾æœ€ä¼˜ï¼›<br>
        ä¸¤è€…çš„åŒºåˆ«ï¼Œå°±æ˜¯ä»"æœŸæœ›æ„ä¹‰çš„éšæœºä¸Šå‡"å˜ä¸º"ç¡®å®šæ€§æ–¹å‘çš„ç²¾ç¡®ä¸Šå‡"ã€‚</p>
      </blockquote>

      <hr>

      <h3>åã€å¸¸è§é—®é¢˜ä¸æ‰©å±•</h3>

      <table>
        <tr><th>é—®é¢˜</th><th>è§£å†³æ–¹å¼</th></tr>
        <tr><td>è¿‡ä¼°è®¡ Q</td><td>ç”¨ TD3ï¼šåŒ Q ç½‘ç»œå–æœ€å°å€¼</td></tr>
        <tr><td>æ¢ç´¢ä¸è¶³</td><td>è°ƒæ•´å™ªå£°å¼ºåº¦æˆ–ä½¿ç”¨å‚æ•°å™ªå£°</td></tr>
        <tr><td>å‘æ•£</td><td>é™ä½å­¦ä¹ ç‡ã€ä½¿ç”¨ç›®æ ‡å¹³æ»‘</td></tr>
        <tr><td>è®­ç»ƒæ…¢</td><td>å¢å¤§æ‰¹å¤§å°ã€å½’ä¸€åŒ–çŠ¶æ€/å¥–åŠ±</td></tr>
      </table>

      <hr>

      <h3>åä¸€ã€æ€»ç»“</h3>

      <table>
        <tr><th>æ¨¡å—</th><th>å˜åŒ–è¦ç‚¹</th></tr>
        <tr><td>ç­–ç•¥å½¢å¼</td><td>ä»éšæœºåˆ†å¸ƒ â†’ ç¡®å®šæ€§å‡½æ•°</td></tr>
        <tr><td>å­¦ä¹ ä¿¡å·</td><td>ä» log Ï€ â†’ é“¾å¼æ³•åˆ™ (Î¼, Q)</td></tr>
        <tr><td>ç¨³å®šæ€§æ¥æº</td><td>ä» Advantage â†’ ç›®æ ‡ç½‘ç»œ/å›æ”¾æ± </td></tr>
        <tr><td>æ•°æ®ç±»å‹</td><td>ä» on-policy â†’ off-policy</td></tr>
        <tr><td>æ ¸å¿ƒä¼˜ç‚¹</td><td>ä½æ–¹å·®ã€é«˜æ•°æ®æ•ˆç‡ã€é€‚åˆè¿ç»­æ§åˆ¶</td></tr>
        <tr><td>ç†è®ºæ”¯æ’‘</td><td>Deterministic Policy Gradient Theorem (Silver et al., 2014)</td></tr>
      </table>

      <p><strong> ç»“è¯­</strong></p>
      <p>DPG æ˜¯"ä»éšæœºåˆ°ç¡®å®š"çš„é‡è¦æ¡¥æ¢ã€‚å®ƒè®©å¼ºåŒ–å­¦ä¹ åœ¨è¿ç»­æ§åˆ¶é¢†åŸŸæ‹¥æœ‰äº†å®ç”¨å¯è¡Œçš„è·¯å¾„ï¼Œå¹¶æˆä¸ºåç»­ DDPGã€TD3ã€SAC ç­‰ç®—æ³•çš„ç†è®ºåŸºç¡€ã€‚</p>
    </section>

    <section id="chapter6" class="chapter">
      <h2>ç¬¬å…­ç« ï¼šTD3ï¼ˆTwin Delayed DDPGï¼‰</h2>
      <h3>0. ä¸€å¥è¯ä¸å®šä½</h3>
      <p><strong>TD3 = DDPG çš„ä¸‰ä»¶å¥—ç¨³æ€å‡çº§ï¼š</strong></p>


      <ul>
        <li><strong>Clipped Double Qï¼ˆåŒ Q å–æœ€å°ï¼‰</strong>ï¼šæŠ‘åˆ¶ Q çš„ç³»ç»Ÿæ€§è¿‡ä¼°è®¡</li>
        <li><strong>Delayed Policy Updateï¼ˆå»¶è¿Ÿç­–ç•¥æ›´æ–°ï¼‰</strong>ï¼šå…ˆç¨³ Critic å†åŠ¨ Actor</li>
        <li><strong>Target Policy Smoothingï¼ˆç›®æ ‡ç­–ç•¥å¹³æ»‘ï¼‰</strong>ï¼šè®© TD ç›®æ ‡å¯¹å°–é” Q å³°ä¸æ•æ„Ÿ</li>
      </ul>
      <p>åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼ˆMuJoCoã€æœºæ¢°è‡‚ç­‰ï¼‰é‡Œï¼ŒTD3 é€šå¸¸æ¯” DDPG æ˜æ˜¾æ›´ç¨³ã€æ›´é«˜æ•ˆã€‚</p>

      <h3>1. èƒŒæ™¯ï¼šä¸ºä»€ä¹ˆéœ€è¦ TD3ï¼Ÿ</h3>
      <h4>1.1 DDPG çš„ä¸‰å¤§ç—›ç‚¹</h4>
      <ul>
        <li><strong>è¿‡ä¼°è®¡åå·®</strong>ï¼šå• Critic + â€œæœ€å¤§åŒ– Qâ€ çš„ç»“æ„ï¼Œä¼šæŠŠå™ªå£°å½“ä¼˜åŠ¿æ”¾å¤§ï¼Œé•¿æœŸåä¹è§‚ã€‚</li>
        <li><strong>ç­–ç•¥â€”å€¼å‡½æ•°æ­¥è°ƒå¤±è¡¡</strong>ï¼šCritic è¿˜æ²¡å­¦ç¨³ï¼ŒActor å°±è·Ÿç€ä¸ç¨³å®šçš„ä¿¡å·ç§»åŠ¨ï¼Œæ˜“å‘æ•£ã€‚</li>
        <li><strong>ç›®æ ‡å€¼å¯¹åŠ¨ä½œå¾®æ‰°ææ•æ„Ÿ</strong>ï¼šç›®æ ‡ç”¨ Q(s', Î¼'(s'))ï¼Œåœ¨å°–å³°å¤„å¯¹å¾®å°åŠ¨ä½œå˜åŒ–å‰§çƒˆï¼Œè¿‡æ‹Ÿåˆâ€œè„†å¼±å³°â€ã€‚</li>
      </ul>

      <h4>1.2 TD3 çš„ä¸‰å‰‚è¯</h4>
      <ul>
        <li><strong>åŒ Q å–æœ€å°</strong>ï¼šç”¨ä¸¤å¥—ç‹¬ç«‹çš„ç›®æ ‡ Qï¼ŒTD ç›®æ ‡é‡Œå– min(Q1', Q2')ï¼Œæ•°å€¼ä¸Šâ€œå‘ä¸‹è£å‰ªâ€æ‰é«˜ä¼°ã€‚</li>
        <li><strong>å»¶è¿Ÿç­–ç•¥æ›´æ–°</strong>ï¼šCritic æ¯æ­¥éƒ½æ›´ï¼›Actor/ç›®æ ‡ç½‘æ¯ d æ­¥ï¼ˆå¸¸ä¸º 2ï¼‰æ‰æ›´ï¼Œé¿å…ç­–ç•¥è¿½é€æœªæ”¶æ•›çš„ Qã€‚</li>
        <li><strong>ç›®æ ‡å¹³æ»‘</strong>ï¼šåœ¨ç›®æ ‡ç«¯ç»™ Î¼'(s') åŠ å°é«˜æ–¯å™ªå£°å¹¶è£å‰ªï¼Œå†é€å…¥ Q'ï¼Œé™ä½å¯¹å°–é”å³°å€¼çš„ä¾èµ–ã€‚</li>
      </ul>

      <h3>2. ç®—æ³•ç»†èŠ‚ä¸å…¬å¼</h3>
      <h4>2.1 ç»„ä»¶</h4>
      <ul>
        <li>Actorï¼ˆç¡®å®šæ€§ç­–ç•¥ï¼‰ï¼ša = Î¼Î¸(s)ï¼Œè¾“å‡ºç¼©æ”¾åˆ°åŠ¨ä½œè¾¹ç•Œã€‚</li>
        <li>ä¸¤ä¸ª Criticï¼šQÏ•1(s,a), QÏ•2(s,a)ï¼›ä»¥åŠå„è‡ªçš„ç›®æ ‡ç½‘ç»œ QÏ•1', QÏ•2'ã€‚</li>
        <li>ç›®æ ‡ Actorï¼šÎ¼Î¸'ã€‚</li>
        <li>å›æ”¾æ±  D å­˜ (s,a,r,s',d)ã€‚</li>
      </ul>

      <h4>2.2 è¡Œä¸ºç­–ç•¥ï¼ˆæ¢ç´¢ï¼‰</h4>
  <p>è®­ç»ƒäº¤äº’æ—¶æ‰§è¡Œï¼š</p>
  $$a_t = \operatorname{clip}\big(\mu_{\theta}(s_t) + \epsilon_t,\; \text{bounds}\big),\quad \epsilon_t \sim \mathcal{N}(0,\sigma_{\text{explore}}^2 I)$$
      <p>è¯„ä¼°/æµ‹è¯•æ—¶å»æ‰å™ªå£°ï¼Œåªç”¨ Î¼Î¸(s)ã€‚</p>

      <h4>2.3 TD ç›®æ ‡ï¼ˆæ ¸å¿ƒä¸‰ä»¶å¥—ï¼‰</h4>
      <p>ç›®æ ‡åŠ¨ä½œå¹³æ»‘ï¼š</p>
  $$\tilde a' = \operatorname{clip}\big(\mu_{\theta'}(s') + \epsilon,\; \text{bounds}\big),\quad \epsilon \sim \operatorname{clip}\big(\mathcal{N}(0,\sigma_{\text{targ}}^2),\,-c,\,c\big)$$
      <p>Clipped Double Qï¼š</p>
  $$y = r + \gamma(1-d)\, \min\!\big(Q_{\phi_1'}(s', \tilde a'),\; Q_{\phi_2'}(s', \tilde a')\big)$$
  <p>Critic æŸå¤±ï¼š</p>
  $$L_{Q_j} = \frac{1}{N} \sum_i \Big(Q_{\phi_j}(s_i, a_i) - y_i\Big)^2,\quad j\in\{1,2\}$$
  <p>Actor æŸå¤±ï¼ˆå»¶è¿Ÿæ›´æ–°ï¼‰ï¼š</p>
  $$L_\pi = -\frac{1}{N} \sum_i Q_{\phi_1}(s_i, \mu_{\theta}(s_i))$$
  <p>è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆä¸ Actor åŒæ­¥å»¶è¿Ÿï¼‰ï¼š</p>
  $$\theta' \leftarrow \tau\,\theta + (1-\tau)\,\theta',\quad \phi_j' \leftarrow \tau\,\phi_j + (1-\tau)\,\phi_j'$$

      <h3>3. å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆå·¥ç¨‹åŒ–ç¬”è®°ï¼‰</h3>
      <ol>
        <li>åˆå§‹åŒ–ï¼šéšæœºåˆå§‹åŒ– Î¼Î¸, QÏ•1, QÏ•2ï¼›å¤åˆ¶åˆ°ç›®æ ‡ç½‘ Î¸', Ï•j'ï¼›å»ºå›æ”¾æ±  Dï¼›è®¾è¶…å‚ Î³, Ï„, Ïƒ_explore, Ïƒ_targ, c, dã€‚</li>
        <li>äº¤äº’ & å­˜å‚¨ï¼šç”¨ a_t = clip(Î¼Î¸(s_t) + Ïµ_t) ä¸ç¯å¢ƒäº¤äº’ï¼Œå­˜ (s_t, a_t, r_t, s_{t+1}, d_t) åˆ° Dã€‚</li>
        <li>é‡‡æ ·æ‰¹æ¬¡ï¼šä» D å‡åŒ€é‡‡æ · N æ¡æ ·æœ¬ã€‚</li>
        <li>ç›®æ ‡åŠ¨ä½œå¹³æ»‘ & æ„é€  TD ç›®æ ‡ï¼šæŒ‰ 2.3ã€‚</li>
        <li>æ›´æ–°ä¸¤ä¸ª Criticï¼šæœ€å°åŒ– L_Q1, L_Q2ã€‚</li>
        <li>æ¯éš” d æ­¥ï¼šæ›´æ–° Actorï¼ˆæœ€å¤§åŒ– Q1ï¼‰ï¼›è½¯æ›´æ–° Î¸', Ï•1', Ï•2'ã€‚</li>
        <li>è¯„ä¼°ï¼šå®šæœŸåœ¨æ— å™ªå£°æ¡ä»¶ä¸‹è¿è¡Œè‹¥å¹²å›åˆæ±‚å¹³å‡å›æŠ¥ã€ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚</li>
        <li>å¾ªç¯è‡³æ”¶æ•›ã€‚</li>
      </ol>

      <h3>4. TD3 ä¼ªä»£ç ï¼ˆé«˜å¯†åº¦å·¥ç¨‹ç‰ˆï¼‰</h3>
<pre><code class="language-python"># TD3 ç®—æ³•ä¼ªä»£ç 
init Î¼Î¸, QÏ†1, QÏ†2; targets Î¼Î¸', QÏ†1', QÏ†2'; replay D
for t in 1..T:
  a = clip( Î¼Î¸(s) + N(0, Ïƒ_explore^2), bounds )
  s', r, done = env.step(a)
  replay.add(s,a,r,s',d)
  s â† (s' if not d else reset)

  if timestep > warmup:
      batch = replay.sample(batch_size)

      # --- Critic update ---
      with torch.no_grad():
          a2, logp2 = policy.sample_with_logp(batch.s2)
          target_q = r + gamma * (1 - done) * (
              torch.min(q1_target(batch.s2,a2), q2_target(batch.s2,a2)) - alpha * logp2
          )
      q1_loss = mse(q1(batch.s,a), target_q)
      q2_loss = mse(q2(batch.s,a), target_q)
      opt_q.zero_grad(); (q1_loss+q2_loss).backward(); opt_q.step()

      # --- Actor update ---
      a_pi, logp = policy.sample_with_logp(batch.s)
      q_pi = torch.min(q1(batch.s,a_pi), q2(batch.s,a_pi))
      pi_loss = (alpha * logp - q_pi).mean()
      opt_pi.zero_grad(); pi_loss.backward(); opt_pi.step()

      # --- Alpha update ---
      alpha_loss = -(alpha_log * (logp + target_entropy).detach()).mean()
      opt_alpha.zero_grad(); alpha_loss.backward(); opt_alpha.step()
      alpha = alpha_log.exp()

      # --- Soft update ---
      for p, tp in zip(q1.parameters(), q1_target.parameters()):
          tp.data.copy_(tau * p.data + (1 - tau) * tp.data)</code></pre>

      <h3>5. æ€»ç»“ä¸å·¥ç¨‹å»ºè®®</h3>
      <ul>
        <li>TD3 çš„ä¸‰å¤§æŠ€å·§æœ¬è´¨éƒ½æ˜¯â€œç¨³ä½Criticï¼Œæ…¢æ…¢åŠ¨Actorâ€ï¼Œè®©ç­–ç•¥æ›´æ–°æ›´å¯é ã€‚</li>
        <li>å®é™…ç”¨ TD3ï¼Œå»ºè®®å…ˆç”¨é»˜è®¤å‚æ•°ï¼Œä¼˜å…ˆå…³æ³¨ Q ä¼°è®¡å’Œç­–ç•¥æ”¶æ•›æ›²çº¿ã€‚</li>
        <li>å¦‚é‡è®­ç»ƒä¸ç¨³å®šï¼Œä¼˜å…ˆæ£€æŸ¥å™ªå£°ã€ç›®æ ‡å¹³æ»‘ã€å»¶è¿Ÿæ­¥æ•°ç­‰è¶…å‚ã€‚</li>
        <li>TD3 é€‚åˆå¤§å¤šæ•°è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼Œæ˜¯ DDPG çš„å·¥ç¨‹å‡çº§ç‰ˆã€‚</li>
      </ul>
    </section>

    <section id="chapter7" class="chapter">
      <h2>ç¬¬ä¸ƒç« ï¼šTRPOï¼ˆTrust Region Policy Optimizationï¼‰</h2>

      <h3>å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰ç®€ä»‹</h3>
      <p>åœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­ï¼Œå‡†ç¡®ä¸”ç¨³å®šåœ°ä¼°è®¡ä¼˜åŠ¿å‡½æ•° $A_t$ æ˜¯å…³é”®ã€‚<strong>å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGeneralized Advantage Estimation, GAEï¼‰</strong>
      æä¾›äº†ä¸€ç§åœ¨åå·®ä¸æ–¹å·®ä¹‹é—´è¿›è¡Œå¹³è¡¡çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå¹¿æ³›ç”¨äº TRPOã€PPOã€A2C/A3C åŠå…¶å®ƒ actor-critic å˜ä½“ã€‚</p>

      <p>GAE æœ¬è´¨ä¸Šå°±æ˜¯ï¼š
      â€œæŠŠå¤šæ­¥ TD è¯¯å·®æŒ‰ç…§è¡°å‡æƒé‡æ··åˆåœ¨ä¸€èµ·â€
      â†’ å¾—åˆ°ä¸€ä¸ªæ—¢ä¸å¤ªåï¼Œä¹Ÿä¸å¤ªåµçš„ Advantageã€‚</p>

      <p>ä¸‹é¢æ˜¯å¯¹ GAE çš„è¯¦ç»†è¯´æ˜ï¼š</p>

      <h4> 1. TD(0)ï¼šä¸€æ­¥ TD</h4>
      <p>æœ€ç»å…¸çš„ TD(0)ï¼š</p>
      <p class="equation">$$A_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$</p>
      <p>ä¼˜ç‚¹ï¼šè®¡ç®—å¿«ï¼Œæ–¹å·®ä½ã€‚</p>
      <p>ç¼ºç‚¹ï¼šåå·®éå¸¸å¤§ï¼ˆå¤ªçŸ­è§†ï¼‰ï¼ŒåŸºæœ¬ç”¨ä¸äº†ï¼Œé«˜éš¾åº¦ä»»åŠ¡ä¼šå´©ã€‚</p>

      <h4> 2. MCï¼ˆè’™ç‰¹å¡æ´›ï¼‰ï¼š</h4>
      <p>Monte Carlo Advantageï¼š</p>
      <p class="equation">$$A_t = \sum_{l=0}^{\infty} \gamma^l r_{t+l} - V(s_t)$$</p>
      <p>ä¼˜ç‚¹ï¼šæ— åï¼ˆæœ€å‡†ç¡®ï¼‰ã€‚</p>
      <p>ç¼ºç‚¹ï¼šæ–¹å·®å¤§åˆ°ç¦»è°± â†’ å¤§å¤šæ•° RL ä¼šå‘æ•£ã€‚</p>

      <h4> 3. GAEï¼šä»‹äº TD å’Œ MC ä¹‹é—´çš„â€œæŠ˜ä¸­ç‰ˆæœ¬â€</h4>
      <p>GAE å®šä¹‰ï¼š</p>
      <p class="equation">$$A_t = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$$</p>
      <p>ä¹Ÿå°±æ˜¯ï¼šç”¨å¤šæ­¥ TDï¼Œä½†ä¸æ˜¯å¹³å‡ï¼Œè€Œæ˜¯ç”¨ $\gamma\lambda$ é€æ­¥ä¸‹é™çš„æƒé‡æ··åˆæ‰€æœ‰æœªæ¥ TDã€‚</p>
      <p>æœ€ç»ˆæ•ˆæœï¼šåƒä¸€ä¸ªâ€œåå·®-æ–¹å·®å¹³è¡¡å™¨â€ã€‚</p>

      <h4> 4. ç»“è®ºï¼šâ€œä¼˜åŠ¿å‡½æ•°æ¯”è¾ƒé€‚ä¸­ä¸€ç‚¹â€</h4>
      <p>è¿™æ˜¯å¯¹çš„ï¼Œè€Œä¸”éå¸¸å‡†ç¡®ã€‚</p>
      <p>GAE çš„ Advantageï¼š
      ä¸åƒ TD é‚£æ ·å¤ªçŸ­è§†ï¼ˆåå·®å¤§ï¼‰ï¼Œä¸åƒ MC é‚£æ ·æ–¹å·®å·¨é«˜ï¼ˆå‘æ•£ï¼‰ï¼Œä¸åƒ n-step é‚£æ ·å¼ºä¾èµ–å›ºå®šæ­¥é•¿ï¼Œä¸ä¼šçªç„¶éœ‡è¡ã€‚</p>
      <p>æ‰€ä»¥ PPOã€MAPPO è®­ç»ƒæ‰ä¼šç¨³å®šã€‚</p>

      <h4> 5. ç”¨ä¸€å¥â€œç¨‹åºå‘˜èƒ½ç«‹åˆ»æ‡‚â€çš„è¯è¯´æ˜ï¼š</h4>
      <p>GAE = æŠŠ $\delta$ï¼ˆTD-errorï¼‰é€’æ¨åŠ æƒå¹³æ»‘ä¸€ä¸‹ï¼Œå¾—åˆ°æ—¢ä¸åµåˆä¸åçš„ Advantageã€‚</p>
      <pre><code>gae = 0
for t in reversed(range(T)):
    delta = r[t] + Î³ * V[t+1] - V[t]
    gae = delta + Î³ * Î» * gae
    A[t] = gae
</code></pre>

      <p>è¿™å°±æ˜¯ï¼šä¸€æ­¥ TDï¼š$\delta_t$ï¼ŒäºŒæ­¥ TDï¼š$\gamma\lambda \delta_{t+1}$ï¼Œä¸‰æ­¥ TDï¼š$\gamma^2\lambda^2 \delta_{t+2}$ï¼Œå…¨éƒ¨æ··åˆ â†’ å¾—åˆ°ä¸€ä¸ªâ€œéå¸¸é€‚ä¸­â€çš„ Advantageã€‚</p>

      <h4> 6. ä¸ºä»€ä¹ˆ PPO / MAPPO å¿…é¡»ç”¨ GAEï¼Ÿ</h4>
      <p>å› ä¸ºï¼šPPO éå¸¸ä¾èµ– Advantage çš„ç¨³å®šæ€§ã€‚</p>
      <p>Advantage å¦‚æœæ³¢åŠ¨å¤ªå¤§ â†’ KL ä¼šçˆ†è£‚ã€‚</p>
      <p>Advantage å¦‚æœåå·®å¤ªå¤š â†’ ç­–ç•¥ä¼šè¶Šå­¦è¶Šå·®ã€‚</p>
      <p>Multi-Agent æ›´éœ€è¦ç¨³å®šï¼ˆä½ çš„ä»»åŠ¡ 3v1 å°¤å…¶å¦‚æ­¤ï¼‰ã€‚</p>
      <p>GAE æ˜¯å”¯ä¸€åŒæ—¶æ»¡è¶³ï¼šä½æ–¹å·®ã€å¯æ§åå·®ã€å¯é€’æ¨è®¡ç®—ã€ç¨³å®šæ€§æå¼ºçš„ Advantage æ–¹æ³•ã€‚</p>

      <h4> 7. æœ€ç»ˆæ€»ç»“ï¼š</h4>
      <p>GAE å°±æ˜¯â€œå¤šæ­¥ TD-error çš„æŒ‡æ•°åŠ æƒæ··åˆâ€ã€‚
      å®ƒæœ¬è´¨æ˜¯ä¸€æ­¥æ­¥ TD çš„æ‰©å±•ç‰ˆã€‚
      å¾—åˆ°çš„ Advantage æ¯” TD æ›´å‡†ï¼Œæ¯” MC æ›´ç¨³ï¼Œæ‰€ä»¥æ˜¯ PPO/MAPPO çš„æ ‡å‡†åšæ³•ã€‚</p>

      <p>ä½ å·²ç»æŠ“ä½è¦ç‚¹ï¼šGAE = é€‚ä¸­çš„ Advantageã€‚</p>

      <p>åœ¨å®è·µä¸­ï¼ŒGAE é€šå¸¸å¯¹ä¼˜åŠ¿åšå½’ä¸€åŒ–åç”¨äºç­–ç•¥æ›´æ–°ã€‚</p>
    </section>

    <section id="chapter8" class="chapter">
      <h2>ç¬¬å…«ç« ï¼šPPO å…¨è§£æâ€”â€”è®©ç­–ç•¥ä¼˜åŒ–åˆç¨³åˆç®€å•</h2>

      <p>â€œTRPO ç»™äº†æˆ‘ä»¬ç†è®ºä¸Šçš„å®‰å…¨æ„Ÿï¼ŒPPO æŠŠå®ƒå˜æˆäº†èƒ½è·‘åœ¨æ˜¾å¡ä¸Šçš„ç°å®ã€‚â€</p>


      <h3>æ ¸å¿ƒæ´å¯Ÿï¼šPPO æ˜¯"å°èŒƒå›´ Off-Policy"ç®—æ³•</h3>
      <p>è™½ç„¶ PPO é€šå¸¸è¢«å½’ç±»ä¸º <strong>on-policy</strong> ç®—æ³•ï¼Œä½†ä¸¥æ ¼æ¥è¯´ï¼Œå®ƒæ˜¯ä¸€ç§ <strong>"å±€éƒ¨ off-policyï¼ˆsmall off-policyï¼‰"</strong> ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ã€‚</p>
      
      <p><strong>ä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´ï¼Ÿ</strong></p>
      <p>åœ¨ä¸€æ¬¡è®­ç»ƒå¾ªç¯ä¸­ï¼š</p>
      <ol>
        <li>æˆ‘ä»¬ç”¨å½“å‰ç­–ç•¥ $\pi_{\text{old}}$ ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ä¸€æ‰¹è½¨è¿¹ï¼›</li>
        <li>ç„¶å<strong>å›ºå®šè¿™äº›æ•°æ®</strong>ï¼Œåœ¨æ­¤åŸºç¡€ä¸Š<strong>å¤šè½®æ›´æ–°</strong>æ–°ç­–ç•¥ $\pi_{\theta}$ã€‚</li>
      </ol>
      <p>è¿™æ„å‘³ç€åœ¨ä¼˜åŒ–æ—¶ï¼š$\pi_{\theta} \neq \pi_{\text{old}}$ï¼Œå› æ­¤å½“å‰çš„ä¼˜åŒ–æ­¥éª¤ä¸¥æ ¼æ¥è¯´å·²ç»æ˜¯ <strong>off-policy æ›´æ–°</strong>ã€‚</p>
      
      <p>ä½† PPO é€šè¿‡<strong>é‡è¦æ€§æ¯”ç‡</strong> $r_{\theta}=\frac{\pi_{\theta}(a|s)}{\pi_{\text{old}}(a|s)}$ ä»¥åŠ<strong>å‰ªåˆ‡çº¦æŸ</strong> $\text{clip}(r_{\theta},1-\epsilon,1+\epsilon)$ï¼Œ
      å¼ºè¡Œé™åˆ¶æ–°æ—§ç­–ç•¥å·®å¼‚åœ¨ä¸€ä¸ª<strong>å±€éƒ¨ä¿¡èµ–åŸŸï¼ˆtrust regionï¼‰</strong>å†…ã€‚</p>

      <p><strong>äºæ˜¯ï¼š</strong></p>
      <ul>
        <li>PPO çš„æ›´æ–°å¯ä»¥è¢«ç†è§£ä¸ºä¸€æ¬¡ <strong>"å°èŒƒå›´ off-policyã€å…¨å±€è¿‘ä¼¼ on-policy"</strong> çš„ä¼˜åŒ–è¿‡ç¨‹ï¼›</li>
        <li>å®ƒæ—¢å…è®¸ä¸€å®šç¨‹åº¦çš„<strong>æ•°æ®å¤ç”¨</strong>ï¼Œåˆé¿å…äº† off-policy æ–¹æ³•å¸¸è§çš„<strong>åˆ†å¸ƒåç§»é—®é¢˜</strong>ï¼›</li>
        <li>è¿™ä¹Ÿæ˜¯ PPO ä¹‹æ‰€ä»¥èƒ½åœ¨<strong>"ç¨³å®šæ€§ä¸æ•ˆç‡ä¹‹é—´"</strong>å–å¾—æä½³å¹³è¡¡çš„åŸå› ã€‚</li>
      </ul>

      <h3>ä¸€ã€ä»ç­–ç•¥æ¢¯åº¦è¯´èµ·</h3>
      <p>ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ç›®æ ‡æ˜¯ç›´æ¥ä¼˜åŒ–å‚æ•°åŒ–ç­–ç•¥ $\pi_{\theta}(a|s)$ï¼Œæœ€å¤§åŒ–æœŸæœ›å›æŠ¥ï¼š</p>
      <p class="equation">$$J(\theta)=\mathbb{E}_{\pi_{\theta}}\Big[\sum_{t} \gamma^{t} r_t\Big].$$</p>
      <p>åŸºæœ¬çš„æ¢¯åº¦ä¼°è®¡ä¸ºï¼š</p>
      <p class="equation">$$\nabla_{\theta} J(\theta)=\mathbb{E}\big[\nabla_{\theta} \log \pi_{\theta}(a|s)\; A^{\pi}(s,a)\big],$$</p>
      <p>å…¶ä¸­ $A^{\pi}(s,a)$ æ˜¯ä¼˜åŠ¿å‡½æ•°ï¼Œè¡¨ç¤ºæŸä¸ªåŠ¨ä½œç›¸æ¯”å¹³å‡å†³ç­–çš„å¢ç›Šã€‚</p>

      <h3>äºŒã€ä» TRPO åˆ° PPO çš„åŠ¨æœº</h3>
      <p>TRPO é€šè¿‡å¯¹å¹³å‡ KL æ•£åº¦æ–½åŠ çº¦æŸï¼Œä¿è¯ç­–ç•¥ä¸â€œè·³å¾—å¤ªè¿œâ€ã€‚ç„¶è€Œ TRPO çš„å®ç°ä¾èµ–äºŒé˜¶ä¿¡æ¯ï¼ˆFisher çŸ©é˜µï¼‰ã€å…±è½­æ¢¯åº¦ä¸çº¿æœç´¢ï¼Œå·¥ç¨‹ä¸Šå¤æ‚ä¸”è®¡ç®—å¼€é”€å¤§ã€‚</p>
      <p>PPO çš„è®¾è®¡åŸåˆ™æ˜¯ä¿ç•™ TRPO çš„â€œé™åˆ¶æ›´æ–°å¹…åº¦â€æ€æƒ³ï¼Œä½†ç”¨ç®€å•å¯é çš„ä¸€é˜¶æ–¹æ³•æ›¿ä»£å¤æ‚çš„äºŒé˜¶æ­¥éª¤ï¼Œä»è€Œæ˜“å®ç°ä¸”é«˜æ•ˆã€‚</p>

      <p><strong>æ ¸å¿ƒç›®æ ‡å‡½æ•°ï¼š</strong></p>
      <p class="equation">$$L_{\text{CLIP}}(\theta)=\mathbb{E}_t\Big[\min\big(r_{\theta}(t)\hat{A}_t,\; \text{clip}(r_{\theta}(t),1-\epsilon,1+\epsilon)\hat{A}_t\big)\Big],$$</p>
      <p>å…¶ä¸­ $r_{\theta}(t)=\dfrac{\pi_{\theta}(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}$ ç”¨äºä¿®æ­£é‡‡æ ·åˆ†å¸ƒçš„å·®å¼‚ï¼Œ$\hat{A}_t$ ä¸ºä¼˜åŠ¿å‡½æ•°ï¼ˆé€šå¸¸ä½¿ç”¨ GAE è®¡ç®—ï¼‰ã€‚</p>

      <p><strong>ç›´è§‰è§£é‡Šï¼š</strong></p>
      <ul>
        <li>å¦‚æœ $A>0$ï¼šè¯´æ˜è¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡å¥½ï¼Œå¸Œæœ› $r_{\theta}>1$ï¼ˆæ”¾å¤§æ¦‚ç‡ï¼‰ï¼›ä½†è‹¥ $r_{\theta}$ å¤ªå¤§ï¼Œå°±è¢« clip é™åˆ¶åœ¨ $1+\epsilon$ã€‚</li>
        <li>å¦‚æœ $A<0$ï¼šè¯´æ˜è¿™ä¸ªåŠ¨ä½œä¸å¥½ï¼Œç­–ç•¥åº”å‡å°‘æ¦‚ç‡ï¼›è‹¥ $r_{\theta}$ å¤ªå°ï¼ŒåŒæ ·è¢« clip é™åˆ¶ã€‚</li>
      </ul>
      <p>è¿™æ ·ä¸€æ¥ï¼šPPO åœ¨"å¸Œæœ›æ›´æ–°çš„æ–¹å‘"ä¸Šå‰è¿›ï¼Œä½†ä¸ä¼šèµ°å¤ªè¿œã€‚<strong>clip å°±åƒç»™ç­–ç•¥åŠ äº†ä¸€ä¸ªå®‰å…¨å¸¦</strong>ã€‚</p>

      <h3>å››ã€å®Œæ•´è®­ç»ƒç›®æ ‡ï¼ˆç­–ç•¥ + å€¼å‡½æ•° + ç†µæ­£åˆ™ï¼‰</h3>
      <p>åœ¨å®é™…è®­ç»ƒæ—¶ï¼ŒPPO çš„æœ€ç»ˆä¼˜åŒ–ç›®æ ‡åŒ…å«ä¸‰éƒ¨åˆ†ï¼š</p>
      <p class="equation">$$L(\theta,\psi)=\underbrace{L_{\text{CLIP}}(\theta)}_{\text{ç­–ç•¥ç›®æ ‡}}-c_v\underbrace{\mathbb{E}\big[(V_{\psi}(s)-\hat{R})^2\big]}_{\text{å€¼å‡½æ•°å›å½’}}+c_e\underbrace{\mathbb{E}\big[\mathcal{H}(\pi_{\theta}(\cdot|s))\big]}_{\text{ç†µå¥–åŠ±}}.$$</p>

      <p><strong>å„éƒ¨åˆ†ä½œç”¨ï¼š</strong></p>
      <table>
        <tr><th>éƒ¨åˆ†</th><th>ä½œç”¨</th></tr>
        <tr><td>ç­–ç•¥é¡¹ $L_{\text{CLIP}}$</td><td>ä¸»ç›®æ ‡ï¼šç¨³å®šåœ°æé«˜æœŸæœ›å›æŠ¥</td></tr>
        <tr><td>å€¼å‡½æ•°é¡¹ $L_V$</td><td>æä¾›æ›´ç¨³å®šçš„ Advantage ä¼°è®¡ï¼Œå‡å°‘æ–¹å·®</td></tr>
        <tr><td>ç†µæ­£åˆ™é¡¹ $\mathcal{H}$</td><td>ä¿æŒæ¢ç´¢æ€§ï¼Œé˜²æ­¢ç­–ç•¥å¡Œç¼©</td></tr>
      </table>
      <p>è¿™æ ·å¯ä»¥<strong>å…±äº«åŒä¸€ç½‘ç»œéª¨å¹²</strong>ï¼ˆç‰¹å¾å±‚ï¼‰ï¼Œä¸€æ¬¡åå‘ä¼ æ’­åŒæ—¶æ›´æ–°ç­–ç•¥ä¸ä»·å€¼ç½‘ç»œã€‚</p>

      <h3>äº”ã€å®Œæ•´ç®—æ³•æµç¨‹è¯¦è§£</h3>
      
      <p><strong>Step 1 â€” é‡‡æ ·é˜¶æ®µ</strong></p>
      <p>ç”¨å½“å‰ç­–ç•¥ $\pi_{\text{old}}$ ä¸ç¯å¢ƒäº¤äº’ï¼Œé‡‡é›† T æ¡è½¨è¿¹æ•°æ®ï¼š</p>
      <p class="equation">$$\mathcal{D}=\{(s_t,a_t,r_t,s_{t+1},d_t,\log\pi_{\text{old}}(a_t|s_t))\}_{t=1}^T$$</p>
      <p>è¡Œä¸ºç­–ç•¥é€šå¸¸ä¸ºï¼š$a_t\sim\pi_{\text{old}}(a_t|s_t)$ã€‚è‹¥ä¸ºè¿ç»­åŠ¨ä½œï¼Œåˆ™åŠ¨ä½œåˆ†å¸ƒä¸ºé«˜æ–¯åˆ†å¸ƒ $\mathcal{N}(\mu_{\theta}(s),\sigma_{\theta}(s))$ã€‚</p>

      <p><strong>Step 2 â€” ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰</strong></p>
      <p>é‡‡ç”¨ <strong>GAEï¼ˆGeneralized Advantage Estimationï¼‰</strong> è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼š</p>
      <ol>
        <li><strong>è®¡ç®— TD æ®‹å·®ï¼š</strong></li>
      </ol>
      <p class="equation">$$\delta_t=r_t+\gamma(1-d_t)V_{\psi}(s_{t+1})-V_{\psi}(s_t)$$</p>
      <ol start="2">
        <li><strong>å‘åé€’æ¨å¾—åˆ°ä¼˜åŠ¿ï¼š</strong></li>
      </ol>
      <p class="equation">$$\hat{A}_t=\delta_t+\gamma\lambda(1-d_t)\hat{A}_{t+1}$$</p>
      <ol start="3">
        <li><strong>è®¡ç®—ç›®æ ‡å›æŠ¥ï¼š</strong></li>
      </ol>
      <p class="equation">$$\hat{R}_t=\hat{A}_t+V_{\psi}(s_t)$$</p>
      <ol start="4">
        <li><strong>å¯¹ä¼˜åŠ¿åšå½’ä¸€åŒ–å¤„ç†ï¼š</strong></li>
      </ol>
      <p class="equation">$$\hat{A}_t \leftarrow \frac{\hat{A}_t-\text{mean}(\hat{A})}{\text{std}(\hat{A})+10^{-8}}$$</p>

      <p><strong>Step 3 â€” ç­–ç•¥ä¸ä»·å€¼å‡½æ•°æ›´æ–°</strong></p>
      <p>å¯¹æ¯ä¸ªå°æ‰¹é‡æ•°æ®ï¼ˆminibatchï¼‰è®¡ç®—ä»¥ä¸‹æŸå¤±å‡½æ•°ï¼š</p>
      
      <p><strong>â‘  ç­–ç•¥æŸå¤±ï¼ˆå‰ªåˆ‡å½¢å¼ï¼‰</strong></p>
      <p class="equation">$$L_{\text{CLIP}}(\theta)=\mathbb{E}\Big[\min\big(r_{\theta}\hat{A},\;\text{clip}(r_{\theta},1-\epsilon,1+\epsilon)\hat{A}\big)\Big]$$</p>

      <p><strong>â‘¡ å€¼å‡½æ•°æŸå¤±</strong></p>
      <p class="equation">$$L_V(\psi)=\frac{1}{2}\mathbb{E}\big[(V_{\psi}(s_t)-\hat{R}_t)^2\big]$$</p>

      <p><strong>â‘¢ ç†µæ­£åˆ™</strong></p>
      <p class="equation">$$L_H(\theta)=\mathbb{E}\big[\mathcal{H}(\pi_{\theta}(\cdot|s_t))\big]$$</p>

      <p><strong>â‘£ æ€»æŸå¤±</strong></p>
      <p class="equation">$$L_{\text{total}}=-L_{\text{CLIP}}+c_v L_V-c_e L_H$$</p>
      <p>å…¶ä¸­ $c_v,c_e$ åˆ†åˆ«ä¸ºå€¼å‡½æ•°å’Œç†µæ­£åˆ™çš„æƒé‡ç³»æ•°ã€‚</p>

      <p><strong>Step 4 â€” å°æ‰¹é‡å¤šè½®ä¼˜åŒ–</strong></p>
      <ol>
        <li>å°† T æ¡é‡‡æ ·æ•°æ®æ‰“ä¹±æˆ minibatchï¼ˆå¤§å°é€šå¸¸ä¸º 64â€“256ï¼‰ï¼›</li>
        <li>ç”¨ Adam ä¼˜åŒ–å™¨åœ¨è¿™æ‰¹æ•°æ®ä¸Šè®­ç»ƒ K è½®ï¼ˆ3â€“10 epochsï¼‰ï¼›</li>
        <li>è‹¥å®é™… KL åå·®è¶…è¿‡é˜ˆå€¼ï¼ˆå¦‚ 0.02ï¼‰ï¼Œæå‰åœæ­¢å½“å‰ epochï¼›</li>
        <li>å®Œæˆä¼˜åŒ–åï¼Œæ›´æ–°æ—§ç­–ç•¥ï¼š$\pi_{\text{old}}\leftarrow\pi_{\theta}$ï¼›</li>
        <li>ç„¶åé‡æ–°é‡‡æ ·æ–°ä¸€è½®æ•°æ®ã€‚</li>
      </ol>

      <h3>å…­ã€ä¼ªä»£ç </h3>
      <pre class="line-numbers"><code class="language-python">Initialize Î¸, Ïˆ
for iteration = 1, 2, ... do
    # Step 1: é‡‡æ ·
    Collect {s_t, a_t, r_t, s_{t+1}, done_t, logp_old_t} using Ï€_old
    
    # Step 2: è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
    Compute advantages Ã‚_t and targets Å”_t via GAE
    Normalize Ã‚_t: Ã‚ â† (Ã‚ - mean(Ã‚)) / (std(Ã‚) + 1e-8)
    
    # Step 3-4: å¤šè½®å°æ‰¹é‡æ›´æ–°
    for epoch in 1..K do
        for each minibatch in D do
            # è®¡ç®—æ¯”ç‡
            Compute ratio r = exp(logÏ€_Î¸ - logp_old)
            
            # å‰ªåˆ‡ç›®æ ‡
            L_clip = mean(min(rÂ·Ã‚, clip(r,1-Îµ,1+Îµ)Â·Ã‚))
            
            # å€¼å‡½æ•°æŸå¤±
            L_value = Â½(V_Ïˆ(s)-Å”)Â²
            
            # ç†µæ­£åˆ™
            entropy = H(Ï€_Î¸(Â·|s))
            
            # æ€»æŸå¤±
            loss = -L_clip + c_v*L_value - c_e*entropy
            
            # ä¼˜åŒ–
            Update Î¸, Ïˆ using Adam
        
        # KL æ—©åœ
        if mean_KL > threshold:
            break
    
    # æ›´æ–°æ—§ç­–ç•¥
    Ï€_old â† Ï€_Î¸
end for</code></pre>

      <h3>ä¸ƒã€å…³é”®è¶…å‚æ•°å»ºè®®</h3>
      <table>
        <tr><th>å‚æ•°</th><th>å«ä¹‰</th><th>æ¨èå€¼</th></tr>
        <tr><td>Î³</td><td>æŠ˜æ‰£ç³»æ•°</td><td>0.99</td></tr>
        <tr><td>Î»</td><td>GAE å‚æ•°</td><td>0.95</td></tr>
        <tr><td>Îµ</td><td>Clip èŒƒå›´</td><td>0.1â€“0.2</td></tr>
        <tr><td>c_v</td><td>å€¼å‡½æ•°ç³»æ•°</td><td>0.5</td></tr>
        <tr><td>c_e</td><td>ç†µæ­£åˆ™ç³»æ•°</td><td>0.01 (è¿ç»­åŠ¨ä½œæ›´å¤§)</td></tr>
        <tr><td>å­¦ä¹ ç‡</td><td>Adam LR</td><td>3e-4ï¼ˆä»»åŠ¡ä¾èµ–ï¼‰</td></tr>
        <tr><td>æ‰¹é‡å¤§å°</td><td>æ¯æ¬¡æ›´æ–°æ ·æœ¬æ•°</td><td>64â€“256</td></tr>
        <tr><td>T</td><td>æ¯æ¬¡é‡‡æ ·æ­¥æ•°</td><td>2048 æˆ–æ›´å¤š</td></tr>
        <tr><td>K (Epochs)</td><td>æ¯æ‰¹æ•°æ®é‡å¤æ›´æ–°æ¬¡æ•°</td><td>3â€“10</td></tr>
      </table>

      <h3>å…«ã€ç†µæ­£åˆ™é¡¹è¦ä¸è¦ä¿ç•™ï¼Ÿ</h3>
      <p>è¿™æ˜¯ä¸€é“ç»å…¸é—®é¢˜ã€‚</p>
      <ul>
        <li>åœ¨<strong>ç®€å•ç¦»æ•£ä»»åŠ¡</strong>ï¼ˆå¦‚ CartPoleï¼‰ä¸­ï¼Œå¯ä»¥è®¾ä¸º 0ï¼›</li>
        <li>åœ¨<strong>è¿ç»­æ§åˆ¶ä»»åŠ¡</strong>ï¼ˆMuJoCoã€PyBulletï¼‰ä¸­ï¼Œå¼ºçƒˆå»ºè®®ä¿ç•™ï¼›</li>
        <li><strong>ç¨€ç–å¥–åŠ±ä»»åŠ¡</strong>ä¸­ï¼ˆå¦‚æ¢ç´¢ç±»ï¼‰ï¼Œç”šè‡³éœ€è¦è¾ƒå¤§ç†µç³»æ•°ï¼›</li>
        <li>å¸¸è§åšæ³•æ˜¯<strong>å‰æœŸè¾ƒå¤§ï¼ŒåæœŸé€æ­¥è¡°å‡</strong>ï¼ˆä¾‹å¦‚ä» 0.02 â†’ 0ï¼‰ã€‚</li>
      </ul>
      <p>ç†µé¡¹çš„æœ¬è´¨æ˜¯é˜²æ­¢ç­–ç•¥æ–¹å·®è¿‡æ—©æ”¶ç¼©ï¼Œå¯ä»¥ç†è§£ä¸º<strong>"è®©æ™ºèƒ½ä½“ä¿ç•™ä¸€ç‚¹ç‚¹çŠ¹è±«"</strong>ã€‚</p>

      <h3>ä¹ã€å¸¸è§é—®é¢˜ä¸å·¥ç¨‹å»ºè®®</h3>
      <table>
        <tr><th>é—®é¢˜</th><th>åŸå› </th><th>è§£å†³æ€è·¯</th></tr>
        <tr><td>è®­ç»ƒå‘æ•£</td><td>å­¦ä¹ ç‡æˆ– $\epsilon$ å¤ªå¤§</td><td>å‡å°å­¦ä¹ ç‡æˆ– clip è¾¹ç•Œ</td></tr>
        <tr><td>æå‡ååˆé€€æ­¥</td><td>epoch å¤ªå¤šã€è¿‡æ‹Ÿåˆå½“å‰é‡‡æ ·</td><td>å‡å°‘ epoch æˆ–ç›‘æ§ KL</td></tr>
        <tr><td>ç­–ç•¥å¡Œç¼©</td><td>ç†µå¤ªä½</td><td>æé«˜ entropy_coef</td></tr>
        <tr><td>æ”¶æ•›å¤ªæ…¢</td><td>ä¼˜åŠ¿ä¼°è®¡åå·®å¤§</td><td>æ£€æŸ¥ GAEï¼›å¢åŠ  rollout æ­¥æ•°</td></tr>
        <tr><td>å€¼å‡½æ•°ä¸å‡†</td><td>å­¦ä¹ ç‡è¿‡å¤§æˆ–æœª clip</td><td>å¼€å¯ value clipping</td></tr>
      </table>

      <h3>åã€PPO ä¸ TRPO çš„å…³ç³»ä¸åŒºåˆ«</h3>
      <table>
        <tr><th>æ–¹é¢</th><th>TRPO</th><th>PPO</th></tr>
        <tr><td>ç­–ç•¥çº¦æŸ</td><td>ç¡¬ KL çº¦æŸ</td><td>æ¯”ç‡å‰ªåˆ‡ / KL æƒ©ç½š</td></tr>
        <tr><td>ä¼˜åŒ–æ–¹æ³•</td><td>äºŒé˜¶ï¼ˆFisher + å…±è½­æ¢¯åº¦ + çº¿æœç´¢ï¼‰</td><td>ä¸€é˜¶ï¼ˆSGD / Adamï¼‰</td></tr>
        <tr><td>ç†è®ºæ€§è´¨</td><td>å•è°ƒæ”¹è¿›ä¿è¯</td><td>æ— ä¿è¯ï¼Œä½†ç»éªŒç¨³</td></tr>
        <tr><td>å®ç°å¤æ‚åº¦</td><td>é«˜</td><td>ä½</td></tr>
        <tr><td>æ ·æœ¬åˆ©ç”¨ç‡</td><td>ä½ï¼ˆå•æ¬¡ï¼‰</td><td>é«˜ï¼ˆå¤šè½®å¤ç”¨ï¼‰</td></tr>
        <tr><td>é€šå¸¸è¡¨ç°</td><td>éå¸¸ç¨³å®šï¼Œæ”¶æ•›æ…¢</td><td>ç¨³å®š + é«˜æ•ˆï¼Œæ˜¯ä¸»æµåŸºçº¿</td></tr>
      </table>
      <p>PPO å°±æ˜¯ TRPO çš„ä¸€é˜¶åŒ–è¿‘ä¼¼ï¼šTRPO æ§åˆ¶ KLï¼›PPO ç›´æ¥æ§åˆ¶æ¯”ç‡ã€‚ä¸¤è€…ç›®æ ‡å‡ ä¹ç­‰ä»·ï¼Œåªæ˜¯ PPO æ›´ä¾¿äºæ¢¯åº¦ä¸‹é™å®ç°ã€‚</p>

      <h3>åä¸€ã€æ€»ç»“</h3>
      <p><strong>PPO æ˜¯ä¸€ç§ "å±€éƒ¨ off-policyã€æ•´ä½“è¿‘ä¼¼ on-policy" çš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•ã€‚</strong></p>
      <p>å®ƒåœ¨æ—§ç­–ç•¥æ•°æ®ä¸Šåšå¤šè½®å‰ªåˆ‡æ›´æ–°ï¼Œç”¨æ¯”ç‡ä¿®æ­£è½»å¾®åˆ†å¸ƒå·®å¼‚ï¼Œç”¨ Clip æˆ– KL é™åˆ¶ç­–ç•¥åç§»ï¼Œ
      å…¼é¡¾äº†<strong>æ ·æœ¬åˆ©ç”¨ç‡ã€è®­ç»ƒç¨³å®šæ€§ä¸å®ç°ç®€æ´æ€§</strong>ï¼Œ
      æˆä¸ºç›®å‰å¼ºåŒ–å­¦ä¹ ä¸­æœ€é€šç”¨ã€æœ€ç¨³å®šçš„ baseline ä¹‹ä¸€ã€‚</p>

      <h3>åäºŒã€å·¥ç¨‹å®è·µå»ºè®®</h3>
      <p>PPO ä¹‹æ‰€ä»¥è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä¸ä»…å› ä¸ºå®ƒåœ¨å¤šæ•°åŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œæ›´é‡è¦çš„æ˜¯å®ƒæä¾›äº†ä¸€æ¡<strong>å·¥ç¨‹å¯è¡Œçš„è·¯å¾„</strong>ï¼š
      ç”¨ç®€å•çš„å‰ªåˆ‡ä¸å¤šè½®å°æ‰¹è®­ç»ƒæ›¿ä»£å¤æ‚çš„äºŒé˜¶æ±‚è§£ï¼Œä½¿å¾—ç­–ç•¥ä¼˜åŒ–å¯ä»¥ç›´æ¥å—ç›Šäºæ·±åº¦å­¦ä¹ ç°æœ‰çš„ä¸€é˜¶ä¼˜åŒ–å™¨ä¸å¹¶è¡ŒåŒ–å®ç°ã€‚</p>
      
      <p><strong>æˆ‘çš„ç»éªŒæ˜¯ï¼š</strong></p>
      <ul>
        <li>å…ˆç”¨æ¨èçš„è¶…å‚æ•°è·‘é€šï¼ˆä¾‹å¦‚ Adam lr=3e-4ï¼Œ$\epsilon=0.1$ï¼ŒT=2048ï¼ŒEpochs=4ï¼‰ï¼Œè§‚å¯Ÿå­¦ä¹ æ›²çº¿ï¼›</li>
        <li>è‹¥å‘æ•£ï¼Œå…ˆè°ƒå° lrï¼Œå…¶æ¬¡å‡å° $\epsilon$ï¼›</li>
        <li>åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ä¿ç•™ç†µæ­£åˆ™å¹¶é€æ­¥è¡°å‡ï¼›</li>
        <li>å€ŸåŠ©ç°æˆå®ç°ï¼ˆ<strong>OpenAI Spinning Up</strong>ã€<strong>Stable-Baselines3</strong>ï¼‰ä½œä¸ºåŸºçº¿å†åšæ”¹è¿›ï¼›</li>
        <li>ç›‘æ§å…³é”®æŒ‡æ ‡ï¼špolicy lossã€value lossã€entropyã€approx_klã€clip_fractionã€‚</li>
      </ul>

      <p><strong>å‚è€ƒå®ç°ï¼š</strong>OpenAI Spinning Upã€OpenAI åŸå§‹å®ç°ä¸ Stable-Baselines3 çš„ PPO æ¨¡å—å‡æ˜¯å¾ˆå¥½çš„å·¥ç¨‹æ¨¡æ¿ã€‚</p>

    </section>

    <section id="chapter9" class="chapter">
      <h2>ç¬¬ä¹ç« ï¼šSAC å…¨è§£æâ€”â€”æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ çš„é»„é‡‘æ ‡å‡†</h2>

      <p><strong>Soft Actor-Critic (SAC)ï¼šæœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ çš„é»„é‡‘æ ‡å‡†</strong></p>

      <p>åœ¨å¼ºåŒ–å­¦ä¹ çš„ä¸–ç•Œé‡Œï¼Œ<strong>Soft Actor-Criticï¼ˆSACï¼‰</strong> æ˜¯ç›®å‰è¿ç»­æ§åˆ¶é¢†åŸŸä¸­æœ€å—æ¬¢è¿ã€æœ€ç¨³å®šã€ä¹Ÿæ˜¯æœ€é€šç”¨çš„ç®—æ³•ä¹‹ä¸€ã€‚
      å®ƒå…¼å…· <strong>é«˜æ ·æœ¬æ•ˆç‡ï¼ˆoff-policyï¼‰ã€ç¨³å®šæ€§å¼ºï¼ˆåŒ Q ç½‘ç»œ + ç›®æ ‡ç½‘ç»œï¼‰</strong>å’Œ <strong>å¼ºæ¢ç´¢èƒ½åŠ›ï¼ˆæœ€å¤§ç†µæ¡†æ¶ï¼‰</strong>ã€‚</p>

      <p>SAC å¯ä»¥è¢«çœ‹ä½œæ˜¯ï¼š</p>
      <p class="equation"><strong>"DDPG + ç†µæ­£åˆ™åŒ– + åŒ Q + è‡ªé€‚åº”æ¢ç´¢"</strong>çš„å‡çº§ç‰ˆã€‚</p>

      <h3>ä¸€ã€ä»æœ€å¤§æœŸæœ›åˆ°æœ€å¤§ç†µ</h3>

      <p>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç›®æ ‡æ˜¯æœ€å¤§åŒ–å›æŠ¥ï¼š</p>
      <p class="equation">$$J(\pi)=\mathbb{E}_{\pi}\Big[\sum_{t} \gamma^{t} r(s_t,a_t)\Big]$$</p>

      <p>è€Œ <strong>SAC</strong> åœ¨æ­¤åŸºç¡€ä¸Šå¼•å…¥äº†<strong>ç­–ç•¥ç†µï¼ˆentropyï¼‰</strong>ï¼Œç›®æ ‡å˜ä¸ºï¼š</p>
      <p class="equation">$$J(\pi)=\mathbb{E}_{\pi}\Big[\sum_{t} \gamma^{t} \big(r(s_t,a_t)+\alpha \mathcal{H}(\pi(\cdot|s_t))\big)\Big]$$</p>

      <p>å…¶ä¸­ï¼š</p>
      <p class="equation">$$\mathcal{H}(\pi(\cdot|s))=-\mathbb{E}_{a\sim\pi}\big[\log \pi(a|s)\big]$$</p>

      <ul>
        <li><strong>Î±</strong>ï¼šæ¸©åº¦ç³»æ•°ï¼Œç”¨äºå¹³è¡¡"å›æŠ¥"å’Œ"ç†µ"ï¼›</li>
        <li><strong>ç†µé¡¹</strong>ï¼šé¼“åŠ±ç­–ç•¥æ›´éšæœºï¼Œä»è€Œæ›´å…·æ¢ç´¢æ€§ã€‚</li>
      </ul>

      <p>ç›´è§‚åœ°è¯´ï¼ŒSAC è®©æ™ºèƒ½ä½“æ—¢<strong>è¿½æ±‚é«˜åˆ†</strong>ï¼Œä¹Ÿä¿æŒ<strong>"å¤šæ ·åŒ–é€‰æ‹©"</strong>ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚</p>

      <h3>äºŒã€SAC ä¸ä¼ ç»Ÿ Actor-Critic çš„æ ¹æœ¬åŒºåˆ«</h3>

      <p>å¾ˆå¤šäººç¬¬ä¸€æ¬¡çœ‹åˆ° SAC åå­—æ—¶ä¼šä»¥ä¸ºå®ƒå°±æ˜¯ AC çš„ä¸€ä¸ªå°å˜ä½“ï¼Œä½†å…¶å®å®ƒæ˜¯ <strong>off-policy + value-based</strong> çš„ ACã€‚</p>

      <table>
        <tr><th>å¯¹æ¯”é¡¹</th><th>ä¼ ç»Ÿ ACï¼ˆå¦‚ A2C / PPOï¼‰</th><th>SAC</th></tr>
        <tr><td>é‡‡æ ·æ–¹å¼</td><td>On-policyï¼ˆæ¯æ¬¡æ–°æ•°æ®ï¼‰</td><td>Off-policyï¼ˆå¯é‡ç”¨æ—§æ•°æ®ï¼‰</td></tr>
        <tr><td>å€¼å‡½æ•°</td><td>V(s) æˆ– Q(s,a)</td><td>åŒ Q ç½‘ç»œ</td></tr>
        <tr><td>ç­–ç•¥ç±»å‹</td><td>éšæœºç­–ç•¥ï¼ˆç¨³å®šä½†æ¢ç´¢å¼±ï¼‰</td><td>æœ€å¤§ç†µéšæœºç­–ç•¥ï¼ˆæ¢ç´¢å¼ºï¼‰</td></tr>
        <tr><td>ç›®æ ‡ç½‘ç»œ</td><td>âŒ æ— éœ€</td><td>âœ… å¿…éœ€ï¼Œç”¨äºç¨³å®š TD ç›®æ ‡</td></tr>
        <tr><td>æ˜¯å¦è‡ªé€‚åº”æ¢ç´¢</td><td>âŒ å›ºå®š</td><td>âœ… Î± è‡ªåŠ¨è°ƒèŠ‚æ¢ç´¢å¼ºåº¦</td></tr>
      </table>

      <p>SAC ç»§æ‰¿äº† <strong>DDPG çš„ Off-policy æ¶æ„</strong>ï¼ˆå«ç›®æ ‡ç½‘ç»œï¼‰ï¼Œåˆèåˆäº†<strong>éšæœºç­–ç•¥å’Œç†µæœ€å¤§åŒ–</strong>ï¼Œå½¢æˆä¸€ä¸ªæ›´é²æ£’çš„æ¡†æ¶ã€‚</p>

      <h3>ä¸‰ã€ç®—æ³•ç»“æ„ä¸æ ¸å¿ƒç»„ä»¶</h3>

      <table>
        <tr><th>æ¨¡å—</th><th>ä½œç”¨</th></tr>
        <tr><td>Actorï¼ˆç­–ç•¥ç½‘ç»œï¼‰</td><td>è¾“å‡ºé«˜æ–¯åˆ†å¸ƒå‚æ•° Î¼ã€Ïƒï¼Œç» Tanh å‹ç¼©é‡‡æ ·åŠ¨ä½œ</td></tr>
        <tr><td>Criticï¼ˆQ ç½‘ç»œ Ã—2ï¼‰</td><td>ä¼°è®¡åŠ¨ä½œä»·å€¼ï¼Œå–æœ€å°å€¼å‡å°‘è¿‡ä¼°è®¡</td></tr>
        <tr><td>ç›®æ ‡ Critic ç½‘ç»œï¼ˆTarget Qï¼‰</td><td>æä¾›ç¨³å®šç›®æ ‡å€¼ï¼Œç¼“æ…¢æ›´æ–°</td></tr>
        <tr><td>æ¸©åº¦å‚æ•° Î±</td><td>è‡ªé€‚åº”è°ƒèŠ‚æ¢ç´¢å¼ºåº¦</td></tr>
      </table>

      <p>SAC ä¸­å­˜åœ¨ <strong>ä¸¤å¥— Critic ç½‘ç»œ</strong>ï¼š</p>

      <ul>
        <li><strong>åœ¨çº¿ Q ç½‘ç»œ</strong>ï¼š$Q_{\theta_1}, Q_{\theta_2}$</li>
        <li><strong>ç›®æ ‡ Q ç½‘ç»œ</strong>ï¼š$Q_{\bar{\theta}_1}, Q_{\bar{\theta}_2}$</li>
      </ul>

      <p>ç›®æ ‡ç½‘ç»œå‚æ•°æ¥æºäºåœ¨çº¿ç½‘ç»œçš„æ»‘åŠ¨å¹³å‡ï¼š</p>
      <p class="equation">$$\bar{\theta}_i \leftarrow \tau \theta_i + (1-\tau) \bar{\theta}_i$$</p>
      <p>ï¼ˆä¸€èˆ¬ Ï„=0.005ï¼‰</p>

      <h3>å››ã€ç›®æ ‡ç½‘ç»œçš„ç”±æ¥ä¸å¿…è¦æ€§</h3>

      <p>ä¼ ç»Ÿ ACï¼ˆå¦‚ PPOã€A2Cï¼‰æ˜¯ <strong>on-policy</strong> ç®—æ³•ï¼Œç”¨çš„æ•°æ®éƒ½æ˜¯å½“å‰ç­–ç•¥åˆšé‡‡çš„ï¼Œä¸å¤ç”¨æ—§æ ·æœ¬ï¼Œå› æ­¤ç›®æ ‡è®¡ç®—ç¨³å®šï¼Œä¸éœ€è¦ç›®æ ‡ç½‘ç»œã€‚</p>

      <p>ä½† <strong>SAC æ˜¯ off-policy</strong>ï¼š</p>

      <ul>
        <li>ä½¿ç”¨<strong>ç»éªŒå›æ”¾æ± </strong>å¤ç”¨æ—§æ•°æ®ï¼›</li>
        <li>å½“å‰ Q ç½‘ç»œä¸æ–­æ›´æ–°ï¼›</li>
        <li>è‹¥ç›´æ¥ç”¨æœ€æ–° Q ç½‘ç»œè®¡ç®— TD ç›®æ ‡ï¼Œä¼šå‡ºç°<strong>"è‡ªæˆ‘åé¦ˆ"</strong>ï¼Œé€ æˆç›®æ ‡æ¼‚ç§»ã€‚</li>
      </ul>

      <p>ä¸ºé¿å…è¿™ç§ä¸ç¨³å®šï¼ŒSAC å€Ÿé‰´äº† <strong>DQN</strong> çš„åšæ³•ï¼š</p>
      <p>ç”¨ä¸€ä¸ª<strong>ç¼“æ…¢æ›´æ–°çš„ç›®æ ‡ Q ç½‘ç»œ</strong>æ¥ç”Ÿæˆç¨³å®šçš„ TD ç›®æ ‡ã€‚</p>

      <p>è¿™æ ·ä¸€æ¥ï¼ŒCritic çš„æ›´æ–°ç›®æ ‡æ›´å¹³æ»‘ï¼Œè®­ç»ƒè¿‡ç¨‹æ›´ç¨³å®šã€‚</p>

      <h3>äº”ã€SAC çš„æ ¸å¿ƒä¼˜åŒ–ç›®æ ‡</h3>

      <h4>Critic æ›´æ–°</h4>

      <p><strong>ç›®æ ‡å€¼ï¼š</strong></p>
      <p class="equation">$$y = r + \gamma \mathbb{E}_{a' \sim \pi_{\phi}}\Big[\min_i Q_{\bar{\theta}_i}(s',a') - \alpha \log \pi_{\phi}(a'|s')\Big]$$</p>

      <p><strong>æŸå¤±å‡½æ•°ï¼š</strong></p>
      <p class="equation">$$L_Q(\theta_i) = \mathbb{E}\Big[\big(Q_{\theta_i}(s,a) - y\big)^2\Big]$$</p>

      <h4>Actor æ›´æ–°</h4>
      <p class="equation">$$L_{\pi}(\phi) = \mathbb{E}_{s \sim D, a \sim \pi_{\phi}}\Big[\alpha \log \pi_{\phi}(a|s) - \min_i Q_{\theta_i}(s,a)\Big]$$</p>
      
      <p><strong>ç›®æ ‡ï¼š</strong>æé«˜é«˜ Q åŠ¨ä½œçš„æ¦‚ç‡ï¼ŒåŒæ—¶ä¿æŒç†µã€‚</p>

      <h4>æ¸©åº¦ Î± æ›´æ–°</h4>
      <p class="equation">$$L_{\alpha} = \mathbb{E}_{a \sim \pi_{\phi}}\Big[-\alpha \big(\log \pi_{\phi}(a|s) + H_{\text{target}}\big)\Big]$$</p>
      
      <p>è®©å¹³å‡ç†µæ¥è¿‘ç›®æ ‡ç†µï¼Œè‡ªåŠ¨è°ƒèŠ‚æ¢ç´¢ã€‚</p>

      <h3>å…­ã€SAC å®Œæ•´è®­ç»ƒæµç¨‹</h3>

      <h4>åˆå§‹åŒ–</h4>
      <ul>
        <li>åˆå§‹åŒ– Actorã€Criticã€ç›®æ ‡ Critic ç½‘ç»œï¼›</li>
        <li>åˆå§‹åŒ–ç»éªŒå›æ”¾æ±  Dï¼›</li>
        <li>è®¾ç½®ç›®æ ‡ç†µ $H_{\text{target}}$ã€‚</li>
      </ul>

      <h4>ä¸»å¾ªç¯</h4>
      <ol>
        <li>ä»å½“å‰ç­–ç•¥é‡‡æ ·åŠ¨ä½œ $a_t \sim \pi_{\phi}(a|s_t)$ï¼Œä¸ç¯å¢ƒäº¤äº’ï¼›</li>
        <li>å­˜å‚¨ $(s_t, a_t, r_t, s_{t+1}, \text{done})$ï¼›</li>
        <li>ä» D ä¸­é‡‡æ ·æ‰¹é‡æ•°æ®ï¼›</li>
        <li><strong>æ›´æ–° Critic</strong>ï¼šç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—ç›®æ ‡ $y$ï¼Œæœ€å°åŒ– Q æŸå¤±ï¼›</li>
        <li><strong>æ›´æ–° Actor</strong>ï¼šé€šè¿‡é‡å‚æ•°åŒ–æŠ€å·§é‡‡æ ·åŠ¨ä½œï¼Œæœ€å°åŒ–ç­–ç•¥æŸå¤±ï¼›</li>
        <li><strong>æ›´æ–°æ¸©åº¦ Î±</strong>ï¼›</li>
        <li><strong>è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ</strong>ï¼š$\bar{\theta}_i \leftarrow \tau \theta_i + (1-\tau) \bar{\theta}_i$ã€‚</li>
      </ol>

      <h3>ä¸ƒã€æ ¸å¿ƒå®ç°è¦ç‚¹</h3>

      <h4>é‡å‚æ•°åŒ–æŠ€å·§</h4>
      <p>é€šè¿‡</p>
      <p class="equation">$$a = \tanh\big(\mu_{\phi}(s) + \sigma_{\phi}(s) \odot \epsilon\big), \quad \epsilon \sim \mathcal{N}(0,I)$$</p>
      <p>ä½¿é‡‡æ ·è¿‡ç¨‹å¯å¾®ï¼Œä»è€Œç›´æ¥å¯¹ Actor åä¼ æ¢¯åº¦ã€‚</p>

      <h4>Tanh ä¿®æ­£ log-prob</h4>
      <p>å› åŠ¨ä½œç»è¿‡ Tanh å‹ç¼©ï¼Œéœ€å¯¹ $\log \pi(a|s)$ åŠ é›…å¯æ¯”ä¿®æ­£é¡¹ã€‚</p>

      <h4>åŒ Q ç½‘ç»œé˜²è¿‡ä¼°è®¡</h4>
      <p>ä½¿ç”¨ $\min(Q_1, Q_2)$ è€Œéå¹³å‡ã€‚</p>

      <h4>ç›®æ ‡ç½‘ç»œç¨³å®šæ›´æ–°</h4>
      <p>ä½¿ç”¨ Polyak å¹³æ»‘ï¼Œé¿å…ç›®æ ‡æŠ–åŠ¨ã€‚</p>

      <h3>å…«ã€ä¼ªä»£ç ï¼ˆPyTorch é£æ ¼ï¼‰</h3>

      <pre class="line-numbers"><code class="language-python">for each timestep:
    a_t = policy.sample_action(s_t)
    s2, r, done = env.step(a_t)
    replay.add(s_t, a_t, r, s2, done)
    s_t = s2

    if timestep > warmup:
        batch = replay.sample(batch_size)

        # --- Critic update ---
        with torch.no_grad():
            a2, logp2 = policy.sample_with_logp(batch.s2)
            target_q = r + gamma * (1 - done) * (
                torch.min(q1_target(batch.s2,a2), q2_target(batch.s2,a2)) - alpha * logp2
            )
        q1_loss = mse(q1(batch.s,a), target_q)
        q2_loss = mse(q2(batch.s,a), target_q)
        opt_q.zero_grad(); (q1_loss+q2_loss).backward(); opt_q.step()

        # --- Actor update ---
        a_pi, logp = policy.sample_with_logp(batch.s)
        q_pi = torch.min(q1(batch.s,a_pi), q2(batch.s,a_pi))
        pi_loss = (alpha * logp - q_pi).mean()
        opt_pi.zero_grad(); pi_loss.backward(); opt_pi.step()

        # --- Alpha update ---
        alpha_loss = -(alpha_log * (logp + target_entropy).detach()).mean()
        opt_alpha.zero_grad(); alpha_loss.backward(); opt_alpha.step()
        alpha = alpha_log.exp()

        # --- Soft update ---
        for p, tp in zip(q1.parameters(), q1_target.parameters()):
            tp.data.copy_(tau * p.data + (1 - tau) * tp.data)</code></pre>

      <h3>ä¹ã€è¶…å‚æ•°å»ºè®®</h3>
      
      <table>
        <tr><th>å‚æ•°</th><th>æ¨èå€¼</th><th>è¯´æ˜</th></tr>
        <tr><td>Î³</td><td>0.99</td><td>æŠ˜æ‰£å› å­</td></tr>
        <tr><td>Ï„</td><td>0.005</td><td>ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç³»æ•°</td></tr>
        <tr><td>æ‰¹é‡å¤§å°</td><td>256</td><td>è¾ƒå¤§æ‰¹æ¬¡æ›´ç¨³å®š</td></tr>
        <tr><td>å­¦ä¹ ç‡</td><td>3e-4</td><td>æ‰€æœ‰ç½‘ç»œä¸€è‡´</td></tr>
        <tr><td>ç›®æ ‡ç†µ</td><td>$-\text{dim}(A)$</td><td>é€šå¸¸å–åŠ¨ä½œç»´åº¦çš„è´Ÿå€¼</td></tr>
        <tr><td>å›æ”¾æ± å¤§å°</td><td>1e6</td><td>æ”¯æŒé•¿æœŸç»éªŒå¤ç”¨</td></tr>
      </table>

      <h3>åã€ä¸ºä»€ä¹ˆ SAC ç¨³å®šåˆå¼ºå¤§ï¼Ÿ</h3>

      <ul>
        <li><strong>æœ€å¤§ç†µç›®æ ‡</strong> - è‡ªåŠ¨å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨</li>
        <li><strong>åŒ Q ç½‘ç»œ</strong> - æŠ‘åˆ¶è¿‡ä¼°è®¡åå·®</li>
        <li><strong>ç›®æ ‡ç½‘ç»œ</strong> - å¹³æ»‘æ›´æ–°ç›®æ ‡ï¼Œç¨³å®šè®­ç»ƒ</li>
        <li><strong>Off-policy æ¶æ„</strong> - æé«˜æ ·æœ¬æ•ˆç‡</li>
        <li><strong>Î± è‡ªé€‚åº”è°ƒèŠ‚</strong> - å‡å°‘æ‰‹åŠ¨è°ƒå‚</li>
      </ul>

      <h3>åä¸€ã€ç®—æ³•å¯¹æ¯”</h3>

      <p><strong>PPO</strong> é™åˆ¶ç­–ç•¥å˜åŒ–ï¼ˆé˜²æ­¢å¤ªä¹±ï¼‰ï¼›<strong>SAC</strong> é¼“åŠ±ç­–ç•¥ä¿æŒéšæœºæ€§ï¼ˆé˜²æ­¢å¤ª"æ­»"ï¼‰ã€‚</p>

      <p><strong>PPO</strong> é‡‡ç”¨"å°å¿ƒå‰è¿›"çš„ç­–ç•¥ï¼›<strong>SAC</strong> é‡‡ç”¨"å¹¿æ³›æ¢ç´¢"çš„æ–¹æ³•ã€‚</p>

      <p>SAC çš„æœ€å¤§ç†µæ€æƒ³è®©æ™ºèƒ½ä½“åœ¨ä¸ç¡®å®šæ€§ä¸­ä¿æŒæ¢ç´¢æ€§ï¼Œæ—¢ä¸ä¼šè¿‡äºè´ªå©ªï¼Œä¹Ÿä¸ä¼šè¿‡äºä¿å®ˆã€‚</p>

      <h3>åäºŒã€æ€»ç»“</h3>

      <p><strong>Soft Actor-Critic = Off-policy Actor-Critic + åŒ Q + ç›®æ ‡ç½‘ç»œ + æœ€å¤§ç†µæ¢ç´¢ã€‚</strong></p>

      <p>å®ƒæŠŠ<strong>ç¨³å®šæ€§ã€æ•ˆç‡ä¸æ¢ç´¢æ€§</strong>èåˆåœ¨ä¸€èµ·ï¼Œ
      æˆä¸ºç›®å‰è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­æœ€å¼ºçš„ RL ç®—æ³•ä¹‹ä¸€ã€‚</p>

    </section>

    <section id="chapter10" class="chapter">
      <h2>ç¬¬åç« ï¼šç­–ç•¥æ¢¯åº¦å¢å¼ºç»„åˆ</h2>
      <p>å ä½ï¼šæœ¬ç« èŠ‚å†…å®¹å¾…è¡¥å……ï¼ˆç®€è¦ä¿ç•™æ ‡é¢˜ä»¥ä¿æŒç›®å½•ç»“æ„ï¼‰ã€‚</p>
    </section>

    <section id="chapter_summary" class="chapter">
      <h2>è¿›é˜¶æ€»ç»“ï¼šä»å…¥é—¨åˆ°ç²¾é€šçš„å®Œæ•´è·¯å¾„</h2>

      <p>æœ¬ç¯‡æ¶µç›–äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»ä»·å€¼å‡½æ•°æ”¹è¿›åˆ°ç­–ç•¥ä¼˜åŒ–çš„å®Œæ•´æ¼”è¿›å†ç¨‹ï¼ŒåŒ…å«åä¸ªæ ¸å¿ƒç®—æ³•çš„ç†è®ºåˆ†æä¸å®ç°ç»†èŠ‚ã€‚</p>

      <h3>ä¸€ã€ç®—æ³•æ¼”åŒ–åœ°å›¾</h3>
      
      <h4>ä»·å€¼å‡½æ•°ä¼˜åŒ–åˆ†æ”¯ï¼ˆç¬¬1-4ç« ï¼‰</h4>
      <p><strong>DQN â†’ Dueling DQN â†’ PER â†’ NoisyNet â†’ Rainbow</strong></p>
      
      <table>
        <tr><th>ç®—æ³•</th><th>æ ¸å¿ƒåˆ›æ–°</th><th>è§£å†³é—®é¢˜</th><th>é€‚ç”¨åœºæ™¯</th></tr>
        <tr><td><strong>Dueling DQN</strong></td><td>åŒæµæ¶æ„ V(s) + A(s,a)</td><td>çŠ¶æ€ä»·å€¼ä¼°è®¡ç²¾åº¦</td><td>åŠ¨ä½œä»·å€¼å·®å¼‚å°çš„ç¯å¢ƒ</td></tr>
        <tr><td><strong>PER</strong></td><td>ä¼˜å…ˆç»éªŒå›æ”¾</td><td>æ ·æœ¬åˆ©ç”¨æ•ˆç‡ä½</td><td>ç¨€ç–å¥–åŠ±ã€å¤æ‚æ¢ç´¢</td></tr>
        <tr><td><strong>NoisyNet</strong></td><td>å‚æ•°å™ªå£°æ¢ç´¢</td><td>Îµ-è´ªå©ªç­–ç•¥å±€é™</td><td>éœ€è¦æ·±åº¦æ¢ç´¢çš„ä»»åŠ¡</td></tr>
        <tr><td><strong>Rainbow</strong></td><td>é›†æˆå¤šç§æ”¹è¿›</td><td>å•ä¸€æŠ€æœ¯ç“¶é¢ˆ</td><td>ç»¼åˆåŸºå‡†æµ‹è¯•</td></tr>
      </table>

      <h4>è¿ç»­æ§åˆ¶åˆ†æ”¯ï¼ˆç¬¬5-6ç« ï¼‰</h4>
      <p><strong>DPG â†’ DDPG â†’ TD3</strong></p>
      
      <table>
        <tr><th>ç®—æ³•</th><th>æ ¸å¿ƒåˆ›æ–°</th><th>è§£å†³é—®é¢˜</th><th>å…³é”®æŠ€æœ¯</th></tr>
        <tr><td><strong>DDPG</strong></td><td>ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦</td><td>è¿ç»­åŠ¨ä½œç©ºé—´</td><td>Actor-Critic + ç›®æ ‡ç½‘ç»œ</td></tr>
        <tr><td><strong>TD3</strong></td><td>åŒ Critic + å»¶è¿Ÿæ›´æ–°</td><td>è¿‡ä¼°è®¡åå·®</td><td>ç›®æ ‡ç­–ç•¥å¹³æ»‘ + Clipped Q</td></tr>
      </table>

      <h4>ç­–ç•¥ä¼˜åŒ–åˆ†æ”¯ï¼ˆç¬¬7-9ç« ï¼‰</h4>
      <p><strong>TRPO â†’ PPO â†’ SAC</strong></p>
      
      <table>
        <tr><th>ç®—æ³•</th><th>æ ¸å¿ƒåˆ›æ–°</th><th>å“²å­¦æ€è·¯</th><th>å·¥ç¨‹ç‰¹ç‚¹</th></tr>
        <tr><td><strong>TRPO</strong></td><td>ä¿¡èµ–åŸŸçº¦æŸ</td><td>"ç¨³æ­¥å‰è¿›ï¼Œä¸èµ°å¼¯è·¯"</td><td>ç†è®ºä¿è¯ï¼Œå®ç°å¤æ‚</td></tr>
        <tr><td><strong>PPO</strong></td><td>æ¯”ç‡å‰ªåˆ‡</td><td>"å°å¿ƒç¿¼ç¿¼ï¼Œå±€éƒ¨off-policy"</td><td>ç®€å•é«˜æ•ˆï¼Œå¹¿æ³›åº”ç”¨</td></tr>
        <tr><td><strong>SAC</strong></td><td>æœ€å¤§ç†µæ¡†æ¶</td><td>"ä¿æŒå¥½å¥‡ï¼Œå¤šèµ°å‡ æ¡è·¯"</td><td>è‡ªé€‚åº”æ¢ç´¢ï¼Œè¿ç»­æ§åˆ¶ç‹è€…</td></tr>
      </table>

      <h3>äºŒã€ç®—æ³•é€‰æ‹©æŒ‡å—</h3>

      <h4>æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©</h4>
      <table>
        <tr><th>ä»»åŠ¡ç‰¹å¾</th><th>é¦–é€‰ç®—æ³•</th><th>æ›¿ä»£æ–¹æ¡ˆ</th><th>ç†ç”±</th></tr>
        <tr><td><strong>ç¦»æ•£åŠ¨ä½œ + ç¨ å¯†å¥–åŠ±</strong></td><td>Rainbow DQN</td><td>Dueling DQN + PER</td><td>æ ·æœ¬æ•ˆç‡é«˜ï¼Œæ¢ç´¢å……åˆ†</td></tr>
        <tr><td><strong>ç¦»æ•£åŠ¨ä½œ + ç¨€ç–å¥–åŠ±</strong></td><td>PPO + ICM</td><td>NoisyNet DQN</td><td>å†…åœ¨åŠ¨æœºï¼Œå‚æ•°å™ªå£°æ¢ç´¢</td></tr>
        <tr><td><strong>è¿ç»­æ§åˆ¶ + ç¨³å®šæ€§ä¼˜å…ˆ</strong></td><td>PPO</td><td>TRPO</td><td>å®ç°ç®€å•ï¼Œè®­ç»ƒç¨³å®š</td></tr>
        <tr><td><strong>è¿ç»­æ§åˆ¶ + æ•ˆç‡ä¼˜å…ˆ</strong></td><td>SAC</td><td>TD3</td><td>æ ·æœ¬æ•ˆç‡æé«˜ï¼Œè‡ªé€‚åº”æ¢ç´¢</td></tr>
        <tr><td><strong>é«˜ç»´è¿ç»­æ§åˆ¶</strong></td><td>SAC</td><td>TD3</td><td>æœ€å¤§ç†µé˜²æ­¢è¿‡æ—©æ”¶æ•›</td></tr>
        <tr><td><strong>å®æ—¶æ§åˆ¶ç³»ç»Ÿ</strong></td><td>DDPG / TD3</td><td>SAC</td><td>ç¡®å®šæ€§è¾“å‡ºï¼Œä½å»¶è¿Ÿ</td></tr>
      </table>

      <h4>æ ¹æ®è®¡ç®—èµ„æºé€‰æ‹©</h4>
      <table>
        <tr><th>èµ„æºæƒ…å†µ</th><th>æ¨èç®—æ³•</th><th>é…ç½®å»ºè®®</th></tr>
        <tr><td><strong>GPUå……è¶³</strong></td><td>SAC, Rainbow</td><td>å¤§æ‰¹é‡ï¼Œæ·±ç½‘ç»œï¼Œé•¿è®­ç»ƒ</td></tr>
        <tr><td><strong>GPUæœ‰é™</strong></td><td>PPO, TD3</td><td>ä¸­ç­‰æ‰¹é‡ï¼Œé€‚åº¦ç½‘ç»œæ·±åº¦</td></tr>
        <tr><td><strong>ä»…CPU</strong></td><td>PPO</td><td>å°æ‰¹é‡ï¼Œæµ…ç½‘ç»œï¼Œå¤šè¿›ç¨‹</td></tr>
        <tr><td><strong>è¾¹ç¼˜è®¾å¤‡</strong></td><td>DQN å˜ä½“</td><td>è½»é‡ç½‘ç»œï¼Œé‡åŒ–æ¨ç†</td></tr>
      </table>

      <h3>ä¸‰ã€å·¥ç¨‹å®è·µè·¯çº¿å›¾</h3>

      <h4>å…¥é—¨é˜¶æ®µï¼ˆ1-3ä¸ªæœˆï¼‰</h4>
      <ol>
        <li><strong>åŸºç¡€å®ç°</strong>ï¼šä» Dueling DQN å¼€å§‹ï¼ŒæŒæ¡ PyTorch/TensorFlow åŸºæœ¬æ¡†æ¶</li>
        <li><strong>ç¯å¢ƒç†Ÿæ‚‰</strong>ï¼šåœ¨ CartPole, LunarLander ç­‰ç®€å•ç¯å¢ƒè°ƒè¯•</li>
        <li><strong>æŒ‡æ ‡ç›‘æ§</strong>ï¼šå­¦ä¼šè§‚å¯Ÿ loss curves, episode rewards, exploration metrics</li>
        <li><strong>è¶…å‚è°ƒèŠ‚</strong>ï¼šç†è§£å­¦ä¹ ç‡ã€batch sizeã€ç½‘ç»œç»“æ„å¯¹æ€§èƒ½çš„å½±å“</li>
      </ol>

      <h4>è¿›é˜¶é˜¶æ®µï¼ˆ3-6ä¸ªæœˆï¼‰</h4>
      <ol>
        <li><strong>è¿ç»­æ§åˆ¶</strong>ï¼šæŒæ¡ DDPG â†’ TD3 â†’ SAC çš„å®ç°ä¸è°ƒä¼˜</li>
        <li><strong>å¤æ‚ç¯å¢ƒ</strong>ï¼šåœ¨ MuJoCo, PyBullet ç­‰ç¯å¢ƒéªŒè¯ç®—æ³•</li>
        <li><strong>ç¨³å®šæ€§æŠ€å·§</strong>ï¼šç›®æ ‡ç½‘ç»œã€ç»éªŒå›æ”¾ã€æ¢¯åº¦è£å‰ªç­‰å·¥ç¨‹ç»†èŠ‚</li>
        <li><strong>å¹¶è¡ŒåŒ–</strong>ï¼šå¤šè¿›ç¨‹é‡‡æ ·ã€GPU åŠ é€Ÿã€åˆ†å¸ƒå¼è®­ç»ƒ</li>
      </ol>

      <h4>ç²¾é€šé˜¶æ®µï¼ˆ6ä¸ªæœˆ+ï¼‰</h4>
      <ol>
        <li><strong>ç®—æ³•æ”¹è¿›</strong>ï¼šç»“åˆå…·ä½“ä»»åŠ¡ç‰¹ç‚¹ï¼Œèåˆå¤šç§æŠ€æœ¯</li>
        <li><strong>è‡ªå®šä¹‰ç¯å¢ƒ</strong>ï¼šæ„å»ºçœŸå®åº”ç”¨åœºæ™¯ï¼Œè§£å†³å®é™…é—®é¢˜</li>
        <li><strong>ç†è®ºæ·±åŒ–</strong>ï¼šç†è§£æ”¶æ•›æ€§ã€æ ·æœ¬å¤æ‚åº¦ã€æ³›åŒ–èƒ½åŠ›</li>
        <li><strong>å‰æ²¿è·Ÿè¸ª</strong>ï¼šå…³æ³¨æœ€æ–°ç ”ç©¶ï¼Œè´¡çŒ®å¼€æºç¤¾åŒº</li>
      </ol>

      <h3>å››ã€æ€§èƒ½åŸºå‡†ä¸æœŸæœ›</h3>

      <h4>ç»å…¸ç¯å¢ƒåŸºå‡†è¡¨ç°</h4>
      <table>
        <tr><th>ç¯å¢ƒ</th><th>ä»»åŠ¡ç±»å‹</th><th>PPO</th><th>SAC</th><th>TD3</th><th>Rainbow</th></tr>
        <tr><td><strong>CartPole-v1</strong></td><td>ç¦»æ•£æ§åˆ¶</td><td>500 (ç¨³å®š)</td><td>N/A</td><td>N/A</td><td>500 (å¿«é€Ÿ)</td></tr>
        <tr><td><strong>LunarLander-v2</strong></td><td>ç¦»æ•£æ§åˆ¶</td><td>200+</td><td>N/A</td><td>N/A</td><td>250+</td></tr>
        <tr><td><strong>HalfCheetah-v3</strong></td><td>è¿ç»­æ§åˆ¶</td><td>1000-3000</td><td>9000-12000</td><td>8000-10000</td><td>N/A</td></tr>
        <tr><td><strong>Walker2d-v3</strong></td><td>è¿ç»­æ§åˆ¶</td><td>1500-3000</td><td>4000-5500</td><td>3500-4500</td><td>N/A</td></tr>
        <tr><td><strong>Humanoid-v3</strong></td><td>é«˜ç»´æ§åˆ¶</td><td>1000-2000</td><td>5000-8000</td><td>4000-6000</td><td>N/A</td></tr>
      </table>

      <h4>è®­ç»ƒæ•ˆç‡å¯¹æ¯”</h4>
      <table>
        <tr><th>æŒ‡æ ‡</th><th>PPO</th><th>SAC</th><th>TD3</th><th>TRPO</th></tr>
        <tr><td><strong>æ ·æœ¬æ•ˆç‡</strong></td><td>ä¸­ç­‰</td><td>æé«˜</td><td>é«˜</td><td>ä½</td></tr>
        <tr><td><strong>è®­ç»ƒç¨³å®šæ€§</strong></td><td>æé«˜</td><td>é«˜</td><td>é«˜</td><td>æé«˜</td></tr>
        <tr><td><strong>å®ç°å¤æ‚åº¦</strong></td><td>ä½</td><td>ä¸­ç­‰</td><td>ä¸­ç­‰</td><td>é«˜</td></tr>
        <tr><td><strong>è°ƒå‚éš¾åº¦</strong></td><td>ä½</td><td>ä¸­ç­‰</td><td>ä¸­ç­‰</td><td>é«˜</td></tr>
        <tr><td><strong>æ”¶æ•›é€Ÿåº¦</strong></td><td>ä¸­ç­‰</td><td>å¿«</td><td>ä¸­ç­‰</td><td>æ…¢</td></tr>
      </table>

      <h3>äº”ã€å…³é”®è°ƒå‚ç»éªŒ</h3>

      <h4>é€šç”¨è¶…å‚æ•°å»ºè®®</h4>
      <table>
        <tr><th>å‚æ•°ç±»åˆ«</th><th>PPO</th><th>SAC</th><th>TD3</th><th>ç»éªŒæ³•åˆ™</th></tr>
        <tr><td><strong>å­¦ä¹ ç‡</strong></td><td>3e-4</td><td>3e-4</td><td>1e-3</td><td>è¿ç»­æ§åˆ¶å¯é€‚å½“å¢å¤§</td></tr>
        <tr><td><strong>Batch Size</strong></td><td>64-256</td><td>256-1024</td><td>256</td><td>Off-policy å¯ç”¨æ›´å¤§æ‰¹é‡</td></tr>
        <tr><td><strong>ç½‘ç»œæ·±åº¦</strong></td><td>2-3å±‚</td><td>2-4å±‚</td><td>2-3å±‚</td><td>é¿å…è¿‡æ·±ï¼Œé‡ç‚¹åœ¨å®½åº¦</td></tr>
        <tr><td><strong>Hidden Size</strong></td><td>64-256</td><td>256-512</td><td>256-400</td><td>å¤æ‚ä»»åŠ¡é€‚å½“å¢å¤§</td></tr>
        <tr><td><strong>Replay Buffer</strong></td><td>N/A</td><td>1M</td><td>1M</td><td>å†…å­˜å…è®¸å°½é‡å¤§</td></tr>
      </table>

      <h4>å¸¸è§é—®é¢˜è¯Šæ–­</h4>
      <table>
        <tr><th>é—®é¢˜ç°è±¡</th><th>å¯èƒ½åŸå› </th><th>è§£å†³æ–¹æ¡ˆ</th></tr>
        <tr><td><strong>è®­ç»ƒä¸æ”¶æ•›</strong></td><td>å­¦ä¹ ç‡è¿‡å¤§ / ç½‘ç»œè¿‡æ·±</td><td>é™ä½å­¦ä¹ ç‡ / ç®€åŒ–ç½‘ç»œ</td></tr>
        <tr><td><strong>æ€§èƒ½æŠ–åŠ¨å¤§</strong></td><td>æ¢ç´¢å™ªå£°è¿‡å¤§ / æ‰¹é‡å¤ªå°</td><td>è¡°å‡å™ªå£° / å¢å¤§batch size</td></tr>
        <tr><td><strong>æ”¶æ•›åé€€æ­¥</strong></td><td>è¿‡æ‹Ÿåˆ / ç¾éš¾æ€§é—å¿˜</td><td>æ—©åœ / ç»éªŒå›æ”¾</td></tr>
        <tr><td><strong>æ¢ç´¢ä¸å……åˆ†</strong></td><td>ç†µç³»æ•°è¿‡å° / å™ªå£°ä¸è¶³</td><td>å¢å¤§ç†µæƒé‡ / è°ƒæ•´å™ªå£°ç­–ç•¥</td></tr>
        <tr><td><strong>è®­ç»ƒé€Ÿåº¦æ…¢</strong></td><td>ç½‘ç»œè¿‡å¤§ / é‡‡æ ·æ•ˆç‡ä½</td><td>æ¨¡å‹å‹ç¼© / å¹¶è¡Œé‡‡æ ·</td></tr>
      </table>
  


      <h3>æ€»ç»“</h3>

      <p>æœ¬ç¯‡ç³»ç»Ÿåœ°ä»‹ç»äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒç®—æ³•ï¼Œä» Dueling DQN çš„æ¶æ„åˆ›æ–°åˆ° SAC çš„æœ€å¤§ç†µæ¢ç´¢ï¼Œæ¯ä¸ªç®—æ³•éƒ½é’ˆå¯¹ç‰¹å®šé—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>

      <p>æŒæ¡å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å…³é”®åœ¨äºç†è§£å…¶è®¾è®¡æ€è·¯å’Œé€‚ç”¨åœºæ™¯ï¼š</p>

      <ul>
        <li><strong>ç›®æ ‡å¯¼å‘æ€ç»´</strong> - æ˜ç¡®å¥–åŠ±å‡½æ•°ï¼Œè®¾è®¡åˆç†ç›®æ ‡</li>
        <li><strong>å¹³è¡¡æƒè¡¡æ„è¯†</strong> - æ¢ç´¢ä¸åˆ©ç”¨ï¼Œç¨³å®šä¸æ•ˆç‡ä¹‹é—´çš„å–èˆ</li>
        <li><strong>è¿­ä»£æ”¹è¿›ç²¾ç¥</strong> - æŒç»­ä¼˜åŒ–ç®—æ³•æ€§èƒ½</li>
        <li><strong>å®éªŒéªŒè¯ä¹ æƒ¯</strong> - ç†è®ºç»“åˆå®è·µï¼Œç”¨æ•°æ®éªŒè¯æ•ˆæœ</li>
        <li><strong>ç³»ç»Ÿæ€§æ€è€ƒ</strong> - ç»¼åˆè€ƒè™‘ç®—æ³•ã€ç¯å¢ƒã€åº”ç”¨çš„æ•´ä½“å› ç´ </li>
      </ul>

      <p>å¼ºåŒ–å­¦ä¹ ä»åœ¨å¿«é€Ÿå‘å±•ä¸­ï¼ŒæŒæ¡è¿™äº›ç»å…¸ç®—æ³•ä¸ºåç»­å­¦ä¹ æ›´å‰æ²¿çš„æŠ€æœ¯æ‰“ä¸‹äº†åšå®åŸºç¡€ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€‰æ‹©åˆé€‚çš„ç®—æ³•ã€åˆç†çš„è¶…å‚æ•°è®¾ç½®ï¼Œä»¥åŠå……åˆ†çš„å®éªŒéªŒè¯æ˜¯æˆåŠŸçš„å…³é”®ã€‚</p>
    </section>
  </main>
</div>

<footer>
  <p>Â© 2025 <span>Haoyun Tang</span> | Reinforcement Learning Advanced Notes</p>
</footer>

<script>
document.addEventListener('DOMContentLoaded', () => {
  document.querySelectorAll('pre[class*="language-"]').forEach(pre => {
    if (!pre.classList.contains('line-numbers')) {
      pre.classList.add('line-numbers');
    }
  });
  if (window.Prism) {
    Prism.highlightAll();
  }
});

window.addEventListener('load', () => {
  if (window.Prism) {
    Prism.highlightAll();
  }
});

const navLinks = document.querySelectorAll('nav a');
const asideLinks = document.querySelectorAll('aside a');
const combined = [...navLinks, ...asideLinks];

window.addEventListener('scroll', () => {
  const offset = window.scrollY + 160;
  combined.forEach(link => {
    const section = document.querySelector(link.getAttribute('href'));
    if (!section) return;
    if (section.offsetTop <= offset && section.offsetTop + section.offsetHeight > offset) {
      link.classList.add('active');
    } else {
      link.classList.remove('active');
    }
  });
});
</script>
</body>
</html>