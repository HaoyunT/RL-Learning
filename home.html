<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>强化学习学习笔记</title>
<!-- MathJax -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<style>
/* ===== 全局 ===== */
body {
  margin:0;
  font-family:'Segoe UI','Roboto',sans-serif;
  background-color:#121212;
  color:#e0e0e0;
  line-height:1.7;
}
header {
  background-color:#1e1e1e;
  padding:20px;
  text-align:center;
}
nav ul {
  display:flex;
  justify-content:center;
  list-style:none;
  padding:0;
  margin:0;
  flex-wrap: wrap;
}
nav li { margin:5px 15px; }
nav a { color:#e0e0e0; text-decoration:none; font-weight:bold; }
nav a:hover { color:#00bcd4; }
main { padding:20px; max-width:900px; margin:auto; }
.chapter {
  margin-bottom:40px;
  padding:25px;
  background-color:#1e1e1e;
  border-radius:10px;
}
.chapter h2 {
  color:#00bcd4;
  border-bottom:2px solid #00bcd4;
  padding-bottom:5px;
}
dl dt { font-weight:bold; margin-top:15px; font-size:1.1rem; }
dl dd { margin-left:20px; margin-bottom:10px; color:#cfcfcf; }
dl dd ul { margin-left:20px; }
pre {
  background:#2a2a2a;
  padding:15px;
  border-radius:8px;
  overflow-x:auto;
  font-family:'Fira Code', monospace;
  font-size:0.95rem;
}
em { color:#ff9800; font-style:normal; }
footer { text-align:center; padding:20px; background:#1e1e1e; margin-top:40px; }
</style>
</head>
<body>

<header>
  <h1>强化学习学习笔记</h1>
  <nav>
    <ul>
      <li><a href="#chapter0">环境 & GitHub</a></li>
      <li><a href="#chapter1">基本概念</a></li>
      <li><a href="#chapter2">MDP</a></li>
    </ul>
  </nav>
</header>

<main>
<!-- 第零章 -->
<section id="chapter0" class="chapter">
  <h2>第零章：环境配置 & GitHub 项目上传</h2>
  <p>创建项目文件夹，如 G:\test，然后配置 Anaconda 清华镜像：</p>
  <pre><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
conda config --set show_channel_urls yes</code></pre>
  <p>配置 pip 镜像：</p>
  <pre><code>pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></pre>
  <p>创建虚拟环境并安装核心包：</p>
  <pre><code>cd /d G:\test
conda create --prefix .\.venv python=3.11
conda activate G:\test\.venv
conda install numpy pandas matplotlib scikit-learn jupyterlab
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia</code></pre>
  <p>上传到 GitHub：</p>
  <pre><code>git init
git remote add origin https://github.com/HaoyunT/test.git
git checkout -b main
git add .
git commit -m "初始化项目"
git push -u origin main</code></pre>
</section>

<!-- 第一章 -->
<section id="chapter1" class="chapter">
  <h2>第一章：强化学习基本概念</h2>
  <p>强化学习（Reinforcement Learning, RL）核心概念：</p>
  <dl>
    <dt>智能体（Agent）</dt>
    <dd>决策执行主体。在每个时间步 \(t\) 观察状态 \(S_t\)，选择动作 \(A_t\)，并通过奖励信号调整策略。<br><em>示例：</em>游戏中的玩家角色。</dd>

    <dt>环境（Environment）</dt>
    <dd>智能体交互对象，定义状态空间、动作空间和状态转移规则。动作后返回状态 \(S_{t+1}\) 和奖励 \(R_t\)。<br><em>示例：</em>游戏关卡或物理模拟器。</dd>

    <dt>状态（State）</dt>
    <dd>描述环境在某一时刻的特征，满足马尔可夫性质。<br><em>示例：</em>角色位置、敌人位置。</dd>

    <dt>动作（Action）</dt>
    <dd>智能体可执行的操作，改变环境状态。<br><em>示例：</em>左右移动、跳跃。</dd>

    <dt>奖励（Reward）</dt>
    <dd>环境对动作的反馈，用于衡量优劣。强化学习目标最大化累计奖励：
      <br>\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k}\)
    </dd>

    <dt>策略（Policy）</dt>
    <dd>选择动作的规则，\(\pi(a|s)\) 或 \(a = \pi(s)\)。目标是找到最优策略 \(\pi^*\)。</dd>

    <dt>价值函数（Value Function）</dt>
    <dd>衡量状态或状态-动作对的长期回报：
      <ul>
        <li>状态价值：\(V^\pi(s) = \mathbb{E}[G_t | S_t=s]\)</li>
        <li>动作价值：\(Q^\pi(s,a) = \mathbb{E}[G_t | S_t=s, A_t=a]\)</li>
      </ul>
    </dd>
  </dl>
</section>

<!-- 第二章 -->
<section id="chapter2" class="chapter">
  <h2>第二章：马尔可夫决策过程（MDP）</h2>
  <p>MDP 数学基础：</p>
  <ul>
    <li>状态集合 \(S\)，动作集合 \(A\)</li>
    <li>状态转移概率 \(P(s'|s,a)\)</li>
    <li>奖励函数 \(R(s,a)\)</li>
    <li>折扣因子 \(\gamma\)</li>
    <li>回报 \(G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}\)</li>
  </ul>
</section>
<!-- 第二章 -->
<section id="chapter2" class="chapter">
  <h2>第二章：贝尔曼方程（Bellman Equation）</h2>

  <p>贝尔曼方程用于描述策略下状态或状态-动作的价值递归关系，是强化学习中**策略评估的核心工具**。</p>

  <dl>
    <dt><b>状态价值函数（State Value Function）</b></dt>
    <dd>
      对于策略 <code>π</code> 下的状态价值函数 <code>V^π(s)</code>：
      <br>
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>V</mi><mo>^</mo><mi>π</mi><mo>(</mo><mi>s</mi><mo>)</mo>
        <mo>=</mo>
        <mrow>
          <msubsup><mo>∑</mo><mi>a</mi><mi>A</mi></msubsup>
          <mi>π</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo>
          <msubsup><mo>∑</mo><mi>s'</mi><mi>S</mi></msubsup>
          <mi>P</mi><mo>(</mo><mi>s'</mi>|<mi>s</mi>,<mi>a</mi><mo>)</mo>
          <mo>[</mo><mi>R</mi><mo>(</mo><mi>s</mi>,<mi>a</mi><mo>) + γV^π(s')</mo><mo>]</mo>
        </mrow>
      </math>
      <br>
      说明：当前状态价值 = 即时奖励 + 折扣后的未来价值期望。
    </dd>

    <dt><b>状态-动作价值函数（Action-Value Function）</b></dt>
    <dd>
      对于状态-动作对的价值函数 <code>Q^π(s,a)</code>：
      <br>
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>Q</mi><mo>^</mo><mi>π</mi><mo>(</mo><mi>s</mi>,<mi>a</mi><mo>)</mo>
        <mo>=</mo>
        <mi>R</mi><mo>(</mo><mi>s</mi>,<mi>a</mi><mo>)</mo>
        <mo>+</mo>
        <mi>γ</mi>
        <msubsup><mo>∑</mo><mi>s'</mi><mi>S</mi></msubsup>
        <msubsup><mo>∑</mo><mi>a'</mi><mi>A</mi></msubsup>
        <mi>π</mi><mo>(</mo><mi>a'</mi>|<mi>s'</mi><mo>)</mo>
        <mi>Q</mi><mo>^</mo><mi>π</mi><mo>(</mo><mi>s'</mi>,<mi>a'</mi><mo>)</mo>
      </math>
    </dd>
  </dl>

  <p><strong>用途：</strong>策略评估、策略迭代、价值迭代的理论基础。</p>
</section>

<!-- 第三章 -->
<section id="chapter3" class="chapter">
  <h2>第三章：贝尔曼最优方程（Bellman Optimality Equation）</h2>

  <p>在寻找最优策略 <code>π*</code> 时，状态和状态-动作的价值函数满足**贝尔曼最优方程**，体现最优性原则。</p>

  <dl>
    <dt><b>最优状态价值函数</b></dt>
    <dd>
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>V</mi><mo>*</mo><mo>(</mo><mi>s</mi><mo>)</mo>
        <mo>=</mo>
        <max_{a \in A} 
        <mrow>
          <msubsup><mo>∑</mo><mi>s'</mi><mi>S</mi></msubsup>
          <mi>P</mi><mo>(</mo><mi>s'</mi>|<mi>s</mi>,<mi>a</mi><mo>)</mo>
          <mo>[</mo><mi>R</mi><mo>(</mo><mi>s</mi>,<mi>a</mi><mo>) + γV*(s')</mo><mo>]</mo>
        </mrow>
        </max_{a \in A}>
      </math>
    </dd>

    <dt><b>最优状态-动作价值函数</b></dt>
    <dd>
      <math xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>Q</mi><mo>*</mo><mo>(</mo><mi>s</mi>,<mi>a</mi><mo>)</mo>
        <mo>=</mo>
        <mi>R</mi><mo>(</mo><mi>s</mi>,<mi>a</mi><mo>)</mo>
        <mo>+</mo>
        <mi>γ</mi>
        <msubsup><mo>∑</mo><mi>s'</mi><mi>S</mi></msubsup>
        <max_{a' \in A} <mi>Q</mi>*<mo>(</mo><mi>s'</mi>,<mi>a'</mi><mo>)</mo>
      </math>
    </dd>
  </dl>

  <p><strong>关系总结：</strong></p>
  <ul>
    <li>第二章贝尔曼方程：给定策略 π → 计算 V^π 或 Q^π</li>
    <li>第三章贝尔曼最优方程：求最优策略 π* → V* 或 Q*</li>
  </ul>

  <p><strong>交互流程示意：</strong></p>
  <pre>
S_t (状态)
  │
  ▼
选择动作 A_t (策略 π)
  │
  ▼
环境返回 S_{t+1}, R_t
  │
  └─> 更新价值函数 V(s) / Q(s,a)
  </pre>
</section>

</main>
<footer>
  <p>© 2025 唐浩云 | Reinforcement Learning Journey</p>
</footer>

</body>
</html>
