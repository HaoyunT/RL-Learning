<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>å¼ºåŒ–å­¦ä¹ ç¬”è®° | Yun</title>

<!-- æ•°å­¦å…¬å¼æ¸²æŸ“ -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
 src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Prism.js æ ¸å¿ƒ + ä¸»é¢˜ + è¯­è¨€ -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-dracula.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
<!-- è¯­è¨€æ”¯æŒ -->
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-javascript.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-typescript.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-bash.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-json.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-css.min.js"></script>
<!-- è¡Œå·æ’ä»¶ -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>


<style>
/* ===== å…¨å±€æ ·å¼ ===== */
body {
  margin: 0;
  font-family: "Segoe UI", "Roboto", -apple-system, sans-serif;
  background: linear-gradient(135deg, #0f1419 0%, #1a1f2e 100%);
  color: #c8cdd3;
  line-height: 1.8;
  scroll-behavior: smooth;
}

/* èƒŒæ™¯ç²’å­ */
#trailCanvas {
  position: fixed;
  top: 0; left: 0;
  width: 100%; height: 100%;
  z-index: 0;
  pointer-events: none;
}

/* Header */
header {
  position: sticky;
  top: 0;
  z-index: 100;
  text-align: center;
  background: rgba(15, 20, 25, 0.95);
  padding: 25px 20px;
  backdrop-filter: blur(8px);
  border-bottom: 1px solid rgba(86, 171, 145, 0.2);
  box-shadow: 0 4px 12px rgba(0,0,0,0.4);
}
header h1 {
  font-size: 2.2rem;
  color: #56ab91;
  margin: 0;
  font-weight: 700;
  letter-spacing: 1px;
}

/* å¯¼èˆªæ  */
nav ul {
  list-style: none;
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  padding: 0;
  margin: 15px 0 0;
  gap: 8px;
}
nav li { margin: 0; }
nav a {
  color: #b0b5bb;
  text-decoration: none;
  font-weight: 600;
  font-size: 0.95rem;
  padding: 6px 14px;
  border-radius: 6px;
  transition: all 0.3s ease;
  background: rgba(86, 171, 145, 0.08);
  border: 1px solid rgba(86, 171, 145, 0.15);
}
nav a:hover, nav a.active {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.15);
  border-color: rgba(86, 171, 145, 0.4);
  box-shadow: 0 0 12px rgba(86, 171, 145, 0.2);
}

/* å®¹å™¨å¸ƒå±€ */
.container {
  display: flex;
  position: relative;
  z-index: 5;
  max-width: 1400px;
  margin: 30px auto;
  gap: 30px;
  padding: 0 20px;
}

/* ä¾§è¾¹æ  */
aside {
  position: sticky;
  top: 120px;
  width: 250px;
  height: fit-content;
  background: rgba(25, 30, 40, 0.8);
  border: 1px solid rgba(86, 171, 145, 0.15);
  border-radius: 10px;
  padding: 20px;
  backdrop-filter: blur(10px);
  flex-shrink: 0;
}

aside h3 {
  color: #56ab91;
  font-size: 1.1rem;
  margin: 0 0 15px 0;
  padding-bottom: 10px;
  border-bottom: 2px solid rgba(86, 171, 145, 0.3);
}

aside ul {
  list-style: none;
  padding: 0;
  margin: 0;
}

aside li {
  margin-bottom: 8px;
}

aside a {
  color: #b0b5bb;
  text-decoration: none;
  font-size: 0.9rem;
  display: block;
  padding: 8px 12px;
  border-radius: 6px;
  transition: all 0.2s ease;
  border-left: 3px solid transparent;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

aside a:hover, aside a.active {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
  border-left-color: #56ab91;
  padding-left: 15px;
}

/* å†…å®¹åŒº */
main {
  flex: 1;
  min-width: 0;
}

.chapter {
  background: rgba(25, 30, 40, 0.7);
  border: 1px solid rgba(86, 171, 145, 0.1);
  border-radius: 12px;
  padding: 35px;
  margin-bottom: 35px;
  transition: all 0.3s ease;
}

.chapter:hover {
  border-color: rgba(86, 171, 145, 0.25);
  box-shadow: 0 8px 24px rgba(86, 171, 145, 0.08);
  transform: translateY(-2px);
}

.chapter h2 {
  color: #56ab91;
  font-size: 1.8rem;
  margin: 0 0 20px 0;
  padding-bottom: 12px;
  border-bottom: 2px solid rgba(86, 171, 145, 0.2);
  font-weight: 700;
}

.chapter p {
  margin: 15px 0;
  color: #c8cdd3;
}

.chapter ul, .chapter ol {
  margin: 15px 0;
  padding-left: 30px;
}

.chapter li {
  margin-bottom: 8px;
  color: #c8cdd3;
}

/* ä»£ç å—æ ·å¼ */
pre[class*="language-"] {
  background: linear-gradient(135deg, #0a0e15 0%, #141a25 100%) !important;
  padding: 20px !important;
  border-radius: 10px !important;
  overflow-x: auto !important;
  font-size: 0.92rem !important;
  line-height: 1.6 !important;
  border: 1px solid rgba(86, 171, 145, 0.2) !important;
  position: relative;
  font-family: 'Fira Code', 'Courier New', monospace !important;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3) !important;
}

pre[class*="language-"]::before {
  content: attr(class);
  position: absolute;
  top: 8px;
  right: 12px;
  font-size: 0.7rem;
  color: rgba(86, 171, 145, 0.6);
  font-weight: bold;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

/* ä»£ç è¡Œå· */
pre.line-numbers {
  padding-left: 60px !important;
}

.line-numbers-rows {
  background: rgba(86, 171, 145, 0.08) !important;
  border-right: 2px solid rgba(86, 171, 145, 0.2) !important;
}

.line-numbers-rows > span:before {
  color: rgba(86, 171, 145, 0.6) !important;
  font-weight: bold;
}

pre code {
  color: inherit !important;
  background: none !important;
  font-family: inherit !important;
  font-size: inherit !important;
}

/* ä»£ç å—å†…å„å…ƒç´ é«˜äº® */
.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
  color: #6272a4;
}

.token.punctuation {
  color: #f8f8f2;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
  color: #ff79c6;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
  color: #50fa7b;
}

.token.operator,
.token.entity,
.token.url {
  color: #8be9fd;
}

.token.atrule,
.token.attr-value,
.token.keyword {
  color: #ff79c6;
}

.token.function,
.token.class-name {
  color: #8be9fd;
}

.token.regex,
.token.important,
.token.variable {
  color: #f1fa8c;
}

/* ä»£ç å—é¼ æ ‡æ‚¬åœæ•ˆæœ */
pre[class*="language-"]:hover {
  box-shadow: 
    0 12px 48px rgba(0, 188, 212, 0.25),
    inset 0 1px 0 rgba(255,255,255,0.15) !important;
  transform: translateY(-2px);
  transition: all 0.3s ease;
}

/* æ»šåŠ¨æ¡ç¾åŒ– */
pre[class*="language-"]::-webkit-scrollbar {
  height: 10px;
  background: rgba(0, 188, 212, 0.1);
}

pre[class*="language-"]::-webkit-scrollbar-thumb {
  background: rgba(0, 188, 212, 0.5);
  border-radius: 5px;
  transition: background 0.3s;
}

pre[class*="language-"]::-webkit-scrollbar-thumb:hover {
  background: rgba(0, 188, 212, 0.8);
}


/* è¡¨æ ¼ä¼˜åŒ– */
table {
  width: 100%;
  border-collapse: collapse;
  margin-top: 15px;
  background: rgba(255,255,255,0.05);
  border-radius: 8px;
  overflow: hidden;
}
table th, table td {
  padding: 10px 15px;
  border-bottom: 1px solid rgba(255,255,255,0.1);
}
table th {
  background: rgba(0,188,212,0.2);
  color: #00e5ff;
}

/* Footer */
footer {
  text-align: center;
  padding: 15px;
  margin: 40px 0 10px;
  font-size: 0.9rem;
  color: #aaa;
  background: rgba(20,20,20,0.7);
  border-radius: 8px;
}
footer span {
  color: #00bcd4;
}

/* ===== ç»Ÿä¸€ç« èŠ‚æ ·å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰ ===== */
.chapter h3 {
  padding-bottom: 8px;
  margin-top: 20px;
  border-bottom: 1px solid rgba(255,255,255,0.1);
  font-size: 1.1rem;
}

.chapter strong {
  font-weight: 600;
}

.chapter code {
  background: rgba(255,255,255,0.05);
  padding: 2px 6px;
  border-radius: 3px;
  font-size: 0.9em;
}

.chapter li {
  margin-bottom: 10px;
  line-height: 1.8;
  transition: all 0.2s ease;
}

.chapter li:hover {
  transform: translateX(3px);
}

.chapter table {
  border-collapse: collapse;
  width: 100%;
  margin: 15px 0;
}

.chapter table td, .chapter table th {
  border: 1px solid rgba(255,255,255,0.1);
  padding: 10px;
  text-align: left;
}

.chapter table th {
  background: rgba(255,255,255,0.05);
  font-weight: 600;
}

/* ===== ç¬¬ä¸€ç« ï¼ˆåŸºæœ¬æ¦‚å¿µï¼‰- ç¿ ç»¿ ===== */
#chapter1 h3, #chapter1 dt {
  color: #56ab91;
}

#chapter1 strong {
  color: #56ab91;
}

#chapter1 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬äºŒç« ï¼ˆè´å°”æ›¼æ–¹ç¨‹ï¼‰- ç¿ ç»¿ ===== */
#chapter2 h3, #chapter2 dt {
  color: #56ab91;
}

#chapter2 strong {
  color: #56ab91;
}

#chapter2 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬ä¸‰ç« ï¼ˆè´å°”æ›¼æœ€ä¼˜ï¼‰- ç¿ ç»¿ ===== */
#chapter3 h3, #chapter3 dt {
  color: #56ab91;
}

#chapter3 strong {
  color: #56ab91;
}

#chapter3 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬å››ç« ï¼ˆç­–ç•¥è¿­ä»£ï¼‰- ç¿ ç»¿ ===== */
#chapter4 h3 {
  color: #56ab91;
}

#chapter4 strong {
  color: #56ab91;
}

#chapter4 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬äº”ç« ï¼ˆå€¼è¿­ä»£ï¼‰- ç¿ ç»¿ ===== */
#chapter5 h3 {
  color: #56ab91;
}

#chapter5 strong {
  color: #56ab91;
}

#chapter5 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬å…­ç« ï¼ˆè’™ç‰¹å¡æ´›ï¼‰- ç¿ ç»¿ ===== */
#chapter6 h3 {
  color: #56ab91;
}

#chapter6 strong {
  color: #56ab91;
}

#chapter6 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

#chapter6 dt {
  color: #56ab91;
  font-weight: bold;
  margin-top: 10px;
  margin-bottom: 8px;
}

#chapter6 dd {
  background: rgba(86, 171, 145, 0.05);
  border-left: 3px solid #56ab91;
  padding: 10px 12px;
  margin-left: 0;
  margin-bottom: 12px;
  border-radius: 4px;
  font-size: 0.95rem;
}

/* ===== ç¬¬ä¸ƒç« ï¼ˆæ—¶é—´å·®åˆ†ï¼‰- ç¿ ç»¿ ===== */
#chapter7 h3 {
  color: #56ab91;
}

#chapter7 strong {
  color: #56ab91;
}

#chapter7 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬å…«ç« ï¼ˆQ-learningï¼‰- ç¿ ç»¿ ===== */
#chapter8 h3 {
  color: #56ab91;
}

#chapter8 strong {
  color: #56ab91;
}

#chapter8 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬ä¹ç« ï¼ˆDQNï¼‰- ç¿ ç»¿ ===== */
#chapter9 h3 {
  color: #56ab91;
}

#chapter9 strong {
  color: #56ab91;
}

#chapter9 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬åç« ï¼ˆDDQNï¼‰- ç¿ ç»¿ ===== */
#chapter10 h3 {
  color: #56ab91;
}

#chapter10 strong {
  color: #56ab91;
}

#chapter10 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬åä¸€ç« ï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰- ç¿ ç»¿ ===== */
#chapter11 h3 {
  color: #56ab91;
}

#chapter11 strong {
  color: #56ab91;
}

#chapter11 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬åäºŒç« ï¼ˆActor-Criticï¼‰- ç¿ ç»¿ ===== */
#chapter12 h3 {
  color: #56ab91;
}

#chapter12 strong {
  color: #56ab91;
}

#chapter12 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬åä¸‰ç« ï¼ˆA2C&A3Cï¼‰- ç¿ ç»¿ ===== */
#chapter13 h3 {
  color: #56ab91;
}

#chapter13 strong {
  color: #56ab91;
}

#chapter13 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

#chapter13 strong {
  color: #5dde95;
  font-weight: 600;
}

#chapter13 table td, #chapter13 table th {
  border-color: rgba(93, 222, 149, 0.3);
}

/* ===== å¯¹æ¯”è¡¨æ ¼æ ·å¼å¢å¼º ===== */
table {
  width: 100%;
  border-collapse: collapse;
  margin-top: 15px;
  background: linear-gradient(135deg, rgba(255,255,255,0.02) 0%, rgba(255,255,255,0.05) 100%);
  border-radius: 12px;
  overflow: hidden;
  box-shadow: 0 4px 20px rgba(0,0,0,0.3);
  transition: all 0.3s ease;
}

table:hover {
  box-shadow: 0 8px 30px rgba(0,0,0,0.4);
}

table th, table td {
  padding: 15px;
  border-bottom: 1px solid rgba(255,255,255,0.1);
  text-align: left;
}

table th {
  background: rgba(255, 255, 255, 0.05);
  color: #e0e0e0;
  font-weight: bold;
  font-size: 1.05rem;
}

table tr:hover {
  background: rgba(255, 255, 255, 0.05);
  transition: all 0.2s ease;
}

table td {
  color: #e0e0e0;
}

table tr:last-child th,
table tr:last-child td {
  border-bottom: none;
}

/* Footer */
footer {
  text-align: center;
  padding: 30px 20px;
  background: rgba(15, 20, 25, 0.95);
  border-top: 1px solid rgba(86, 171, 145, 0.15);
  color: #b0b5bb;
  margin-top: 60px;
  font-size: 0.9rem;
}

footer p {
  margin: 10px 0;
}

footer span {
  color: #56ab91;
  font-weight: 600;
}

/* å“åº”å¼è®¾è®¡ */
@media (max-width: 1024px) {
  .container {
    flex-direction: column;
    gap: 20px;
  }
  
  aside {
    position: relative;
    top: auto;
    width: 100%;
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
  }
  
  aside ul {
    grid-column: 1 / -1;
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
  }
  
  aside li {
    margin-bottom: 0;
  }
}

@media (max-width: 768px) {
  header h1 {
    font-size: 1.8rem;
  }
  
  nav ul {
    gap: 4px;
  }
  
  nav a {
    padding: 4px 10px;
    font-size: 0.85rem;
  }
  
  .container {
    padding: 0 10px;
  }
  
  .chapter {
    padding: 20px;
    margin-bottom: 20px;
  }
  
  aside h3 {
    font-size: 1rem;
  }
  
  aside a {
    font-size: 0.85rem;
  }
}

</style>
</head>

<body>
<canvas id="trailCanvas"></canvas>

<header>
  <h1>å¼ºåŒ–å­¦ä¹ ç¬”è®°</h1>
  <nav>
    <ul>
      <li><a href="#chapter0">ç¯å¢ƒé…ç½®</a></li>
      <li><a href="#chapter1">åŸºæœ¬æ¦‚å¿µ</a></li>
      <li><a href="#chapter2">è´å°”æ›¼æ–¹ç¨‹</a></li>
      <li><a href="#chapter3">è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹</a></li>
      <li><a href="#chapter4">ç­–ç•¥è¿­ä»£</a></li>
      <li><a href="#chapter5">å€¼è¿­ä»£</a></li>
      <li><a href="#chapter6">è’™ç‰¹å¡æ´›æ–¹æ³•</a></li>
      <li><a href="#chapter7">æ—¶é—´å·®åˆ†å­¦ä¹ </a></li>
      <li><a href="#chapter8">Q-learning</a></li>
      <li><a href="#chapter9">DQN</a></li>
      <li><a href="#chapter10">DDQN</a></li>
       <li><a href="#chapter11">ç­–ç•¥æ¢¯åº¦æ–¹æ³•</a></li>
       <li><a href="#chapter12">Actor-Critic</a></li>
       <li><a href="#chapter13">A2C&A3C</a></li>
    </ul>
  </nav>
</header>

<div class="container">
  <aside>
    <h3>ğŸ“š ç›®å½•</h3>
    <ul>
      <li><a href="#chapter0">ç¯å¢ƒé…ç½®</a></li>
      <li><a href="#chapter1">åŸºæœ¬æ¦‚å¿µ</a></li>
      <li><a href="#chapter2">è´å°”æ›¼æ–¹ç¨‹</a></li>
      <li><a href="#chapter3">è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹</a></li>
      <li><a href="#chapter4">ç­–ç•¥è¿­ä»£</a></li>
      <li><a href="#chapter5">å€¼è¿­ä»£</a></li>
      <li><a href="#chapter6">è’™ç‰¹å¡æ´›æ–¹æ³•</a></li>
      <li><a href="#chapter7">æ—¶é—´å·®åˆ†å­¦ä¹ </a></li>
      <li><a href="#chapter8">Q-learning</a></li>
      <li><a href="#chapter9">DQN</a></li>
      <li><a href="#chapter10">DDQN</a></li>
      <li><a href="#chapter11">ç­–ç•¥æ¢¯åº¦æ–¹æ³•</a></li>
      <li><a href="#chapter12">Actor-Critic</a></li>
      <li><a href="#chapter13">A2C&A3C</a></li>
    </ul>
  </aside>

  <main>
  <section id="chapter0" class="chapter">
    <h2>ç¬¬é›¶ç« ï¼šç¯å¢ƒé…ç½® & GitHub é¡¹ç›®ä¸Šä¼ </h2>
    <pre><code class="language-python"># é…ç½®é•œåƒæºï¼ˆç®¡ç†å‘˜èº«ä»½æ‰“å¼€Anaconda Promptï¼‰
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
conda config --set show_channel_urls yes

#é…ç½®Pipé•œåƒæº
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆä¸éœ€è¦ä¸ä½ æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´ï¼ŒæŒ‰ç…§è‡ªå·±éœ€æ±‚å°±è¡Œäº†ï¼‰
cd /d G:\test
conda create --prefix .\.venv python=3.11
conda activate G:\test\.venv

#æ‰“å¼€powershellæŸ¥è¯¢ä¸€ä¸‹è‡ªå·±çš„å‹å·ï¼ˆåç»­å®‰è£…pytorchç‰ˆæœ¬éœ€è¦ï¼Œæˆ‘çš„æ˜¯12.4ï¼Œå› æ­¤æˆ‘é€‰çš„12.1å°±å¯ä»¥å…¼å®¹ï¼‰
conda activate G:\test\.venv
nvidia-smi 

# å®‰è£…ä¾èµ–
conda install numpy pandas matplotlib pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia

# ä¸Šä¼ åˆ° GitHub
git init
git remote add origin https://github.com/HaoyunT/test.git
git checkout -b main
git add .
git commit -m "åˆå§‹åŒ–é¡¹ç›®"
git push -u origin main</code></pre>
  </section>

<!-- ç¬¬ä¸€ç«  -->
<section id="chapter1" class="chapter">
  <h2>ç¬¬ä¸€ç« ï¼šå¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ</h2>
  <p>å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰æ ¸å¿ƒæ¦‚å¿µï¼š</p>
  <dl>
    <dt>æ™ºèƒ½ä½“ï¼ˆAgentï¼‰</dt>
    <dd>å†³ç­–æ‰§è¡Œä¸»ä½“ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ \(t\) è§‚å¯ŸçŠ¶æ€ \(S_t\)ï¼Œé€‰æ‹©åŠ¨ä½œ \(A_t\)ï¼Œå¹¶é€šè¿‡å¥–åŠ±ä¿¡å·è°ƒæ•´ç­–ç•¥ã€‚<br><em>ç¤ºä¾‹ï¼š</em>æ¸¸æˆä¸­çš„ç©å®¶è§’è‰²ã€‚</dd>

    <dt>ç¯å¢ƒï¼ˆEnvironmentï¼‰</dt>
    <dd>æ™ºèƒ½ä½“äº¤äº’å¯¹è±¡ï¼Œå®šä¹‰çŠ¶æ€ç©ºé—´ã€åŠ¨ä½œç©ºé—´å’ŒçŠ¶æ€è½¬ç§»è§„åˆ™ã€‚åŠ¨ä½œåè¿”å›çŠ¶æ€ \(S_{t+1}\) å’Œå¥–åŠ± \(R_t\)ã€‚<br><em>ç¤ºä¾‹ï¼š</em>æ¸¸æˆå…³å¡æˆ–ç‰©ç†æ¨¡æ‹Ÿå™¨ã€‚</dd>

    <dt>çŠ¶æ€ï¼ˆStateï¼‰</dt>
    <dd>æè¿°ç¯å¢ƒåœ¨æŸä¸€æ—¶åˆ»çš„ç‰¹å¾ï¼Œæ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨ã€‚<br><em>ç¤ºä¾‹ï¼š</em>è§’è‰²ä½ç½®ã€æ•Œäººä½ç½®ã€‚</dd>

    <dt>åŠ¨ä½œï¼ˆActionï¼‰</dt>
    <dd>æ™ºèƒ½ä½“å¯æ‰§è¡Œçš„æ“ä½œï¼Œæ”¹å˜ç¯å¢ƒçŠ¶æ€ã€‚<br><em>ç¤ºä¾‹ï¼š</em>å·¦å³ç§»åŠ¨ã€è·³è·ƒã€‚</dd>

    <dt>å¥–åŠ±ï¼ˆRewardï¼‰</dt>
    <dd>ç¯å¢ƒå¯¹åŠ¨ä½œçš„åé¦ˆï¼Œç”¨äºè¡¡é‡ä¼˜åŠ£ã€‚å¼ºåŒ–å­¦ä¹ ç›®æ ‡æœ€å¤§åŒ–ç´¯è®¡å¥–åŠ±ï¼š
      <br>\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k}\)
    </dd>

    <dt>ç­–ç•¥ï¼ˆPolicyï¼‰</dt>
    <dd>é€‰æ‹©åŠ¨ä½œçš„è§„åˆ™ï¼Œ\(\pi(a|s)\) æˆ– \(a = \pi(s)\)ã€‚ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ \(\pi^*\)ã€‚</dd>

    <dt>ä»·å€¼å‡½æ•°ï¼ˆValue Functionï¼‰</dt>
    <dd>è¡¡é‡çŠ¶æ€æˆ–çŠ¶æ€-åŠ¨ä½œå¯¹çš„é•¿æœŸå›æŠ¥ï¼š
      <ul>
        <li>çŠ¶æ€ä»·å€¼ï¼š\(V^\pi(s) = \mathbb{E}[G_t | S_t=s]\)</li>
        <li>åŠ¨ä½œä»·å€¼ï¼š\(Q^\pi(s,a) = \mathbb{E}[G_t | S_t=s, A_t=a]\)</li>
      </ul>
    </dd>
  </dl>
</section>

<!-- ç¬¬äºŒç«  -->
<section id="chapter2" class="chapter">
  <h2>ç¬¬äºŒç« ï¼šè´å°”æ›¼æ–¹ç¨‹ï¼ˆBellman Equationï¼‰</h2>
  <p>è´å°”æ›¼æ–¹ç¨‹ç”¨äºæè¿°ç­–ç•¥ä¸‹çŠ¶æ€æˆ–çŠ¶æ€-åŠ¨ä½œçš„ä»·å€¼é€’å½’å…³ç³»ï¼Œæ˜¯å¼ºåŒ–å­¦ä¹ ä¸­<strong>ç­–ç•¥è¯„ä¼°çš„æ ¸å¿ƒå·¥å…·</strong>ã€‚</p>

  <dl>
    <dt><b>çŠ¶æ€ä»·å€¼å‡½æ•°ï¼ˆState Value Functionï¼‰</b></dt>
    <dd>
      å¯¹äºç­–ç•¥ <code>Ï€</code> ä¸‹çš„çŠ¶æ€ä»·å€¼å‡½æ•° <code>V^Ï€(s)</code>ï¼š<br>
      <p>$$ V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s,a) \big[ R(s,a) + \gamma V^\pi(s') \big] $$</p>
      è¯´æ˜ï¼šå½“å‰çŠ¶æ€ä»·å€¼ = å³æ—¶å¥–åŠ± + æŠ˜æ‰£åçš„æœªæ¥ä»·å€¼æœŸæœ›ã€‚
    </dd>

    <dt><b>çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°ï¼ˆAction-Value Functionï¼‰</b></dt>
    <dd>
      å¯¹äºçŠ¶æ€-åŠ¨ä½œå¯¹çš„ä»·å€¼å‡½æ•° <code>Q^Ï€(s,a)</code>ï¼š<br>
      <p>$$ Q^\pi(s,a) = R(s,a) + \gamma \sum_{s' \in S} \sum_{a' \in A} \pi(a'|s') Q^\pi(s',a') $$</p>
    </dd>

    <dt><b>çŠ¶æ€ä»·å€¼å‡½æ•°ä¸çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°çš„å…³ç³»</b></dt>
    <dd>
      çŠ¶æ€ä»·å€¼å‡½æ•°å¯ä»¥ç”±çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°å¾—åˆ°ï¼š<br>
      <p>$$ V^\pi(s) = \sum_{a \in A} \pi(a|s) Q^\pi(s,a) $$</p>
      è§£é‡Šï¼šåœ¨çŠ¶æ€ <code>s</code> ä¸‹ï¼Œä»·å€¼å‡½æ•° <code>V^Ï€(s)</code> æ˜¯ç­–ç•¥é€‰æ‹©åŠ¨ä½œåçš„æœŸæœ› <code>Q^Ï€(s,a)</code>ã€‚
    </dd>
  </dl>

  <p><strong>ç”¨é€”ï¼š</strong>ç­–ç•¥è¯„ä¼°ã€ç­–ç•¥è¿­ä»£ã€ä»·å€¼è¿­ä»£çš„ç†è®ºåŸºç¡€ã€‚</p>
</section>


<!-- ç¬¬ä¸‰ç«  -->
<section id="chapter3" class="chapter">
  <h2>ç¬¬ä¸‰ç« ï¼šè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼ˆBellman Optimality Equationï¼‰</h2>

  <p>åœ¨å¯»æ‰¾æœ€ä¼˜ç­–ç•¥ <code>Ï€*</code> æ—¶ï¼ŒçŠ¶æ€å’ŒçŠ¶æ€-åŠ¨ä½œçš„ä»·å€¼å‡½æ•°æ»¡è¶³<strong>è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹</strong>ï¼Œä½“ç°æœ€ä¼˜æ€§åŸåˆ™ã€‚</p>

  <dl>
    <dt><b>æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•°</b></dt>
    <dd>
      <p>$$
      V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s,a) \big[ R(s,a) + \gamma V^*(s') \big]
      $$</p>
    </dd>

    <dt><b>æœ€ä¼˜çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°</b></dt>
    <dd>
      <p>$$
      Q^*(s,a) = R(s,a) + \gamma \sum_{s' \in S} \max_{a' \in A} Q^*(s',a')
      $$</p>
    </dd>
  </dl>

  <p><strong>å…³ç³»æ€»ç»“ï¼š</strong></p>
  <ul>
    <li>ç¬¬äºŒç« è´å°”æ›¼æ–¹ç¨‹ï¼šç»™å®šç­–ç•¥ Ï€ â†’ è®¡ç®— V^Ï€ æˆ– Q^Ï€</li>
    <li>ç¬¬ä¸‰ç« è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼šæ±‚æœ€ä¼˜ç­–ç•¥ Ï€* â†’ V* æˆ– Q*</li>
  </ul>
</section>
<!-- ç¬¬å››ç«  -->
<section id="chapter4" class="chapter">
  <h2>ç¬¬å››ç« ï¼šç­–ç•¥è¿­ä»£ï¼ˆPolicy Iterationï¼‰</h2>

  <p>ç­–ç•¥è¿­ä»£é€šè¿‡äº¤æ›¿è¿›è¡Œç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›æ¥æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚</p>

  <ol>
    <li><strong>åˆå§‹åŒ–ç­–ç•¥ï¼š</strong>é€‰æ‹©åˆå§‹ç­–ç•¥ <code>Ï€_0</code></li>
    <li><strong>ç­–ç•¥è¯„ä¼°ï¼š</strong>è®¡ç®—çŠ¶æ€ä»·å€¼å‡½æ•°ï¼š
      <p>$$
      v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}
      $$</p>
    </li>
    <li><strong>ç­–ç•¥æ”¹è¿›ï¼š</strong>æ›´æ–°ç­–ç•¥ï¼š
      <p>$$
      \pi_{k+1} = \arg\max_\pi (r_\pi + \gamma P_\pi v_{\pi_k})
      $$</p>
    </li>
    <li>é‡å¤è¯„ä¼°å’Œæ”¹è¿›ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›ã€‚</li>
  </ol>
</section>


<!-- ç¬¬äº”ç«  -->
<section id="chapter5" class="chapter">
  <h2>ç¬¬äº”ç« ï¼šå€¼è¿­ä»£ï¼ˆValue Iterationï¼‰</h2>

  <p>å€¼è¿­ä»£ç›´æ¥è¿­ä»£çŠ¶æ€ä»·å€¼å‡½æ•°ï¼Œé€šè¿‡è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ”¶æ•›åˆ°æœ€ä¼˜å€¼å‡½æ•°ï¼Œç„¶åå¯¼å‡ºæœ€ä¼˜ç­–ç•¥ã€‚</p>

  <ol>
    <li><strong>åˆå§‹åŒ–ä»·å€¼å‡½æ•°ï¼š</strong>é€‰æ‹©åˆå§‹å€¼ <code>v_0</code></li>
    <li><strong>è¿­ä»£æ›´æ–°ï¼š</strong>ä½¿ç”¨è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼š
      <p>$$
      v_{k+1} = \max_\pi (r_\pi + \gamma P_\pi v_k)
      $$</p>
    </li>
    <li><strong>ç­–ç•¥å¯¼å‡ºï¼š</strong>æ”¶æ•›åé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼š
      <p>$$
      \pi^*(s) = \arg\max_a \sum_{s'} P(s'|s,a) \big[R(s,a) + \gamma v^*(s') \big]
      $$</p>
    </li>
  </ol>
</section>


<!-- å¯¹æ¯”-->
<section id="chapterX" class="chapter">
  <h2>ç­–ç•¥è¿­ä»£ä¸å€¼è¿­ä»£çš„å¯¹æ¯”</h2>

  <table border="1" cellpadding="8" cellspacing="0">
    <thead>
      <tr>
        <th></th>
        <th>Policy Iteration</th>
        <th>Value Iteration</th>
        <th>Comments</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>(1)Policy</td>
        <td>\(\pi_0\)</td>
        <td>N/A</td>
        <td></td>
      </tr>
      <tr>
        <td>(2)Value</td>
        <td>\(v_{\pi_0} = r_{\pi_0} + \gamma P_{\pi_0} v_{\pi_0}\)</td>
        <td>\(v_0 := v_{\pi_0}\)</td>
        <td></td>
      </tr>
      <tr>
        <td>(3)Policy</td>
        <td>\(\pi_1 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_{\pi_0} )\)</td>
        <td>\(\pi_1 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_0 )\)</td>
        <td>The two policies are the same</td>
      </tr>
      <tr>
        <td>(4)Value</td>
        <td>\(v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}\)</td>
        <td>\(v_1 = r_{\pi_1} + \gamma P_{\pi_1} v_0\)</td>
        <td>\(v_{\pi_1} \ge v_1 \text{ since } v_{\pi_1} \ge v_{\pi_0}\)</td>
      </tr>
      <tr>
        <td>5) Policy</td>
        <td>\(\pi_2 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_{\pi_1} )\)</td>
        <td>\(\pi'_2 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_1 )\)</td>
        <td></td>
      </tr>
    </tbody>
  </table>
</section>

 <!-- ç¬¬å…­ç«  -->
<section id="chapter6" class="chapter">
  <h2>ç¬¬å…­ç« ï¼šè’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆMonte Carlo Methodsï¼‰</h2>

  <p>è’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆMonte Carloï¼Œç®€ç§° <b>MC</b>ï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ä¸€ç§åŸºäº<strong>é‡‡æ ·</strong>çš„ç­–ç•¥è¯„ä¼°ä¸ä¼˜åŒ–æ–¹æ³•ã€‚
  ä¸åŠ¨æ€è§„åˆ’ä¸åŒï¼ŒMC ä¸ä¾èµ–ç¯å¢ƒçš„çŠ¶æ€è½¬ç§»æ¦‚ç‡æ¨¡å‹ <code>P(s'|s,a)</code>ï¼Œè€Œæ˜¯é€šè¿‡åå¤ä»ç­–ç•¥ <code>Ï€</code> ç”Ÿæˆå®Œæ•´å›åˆï¼ˆEpisodeï¼‰ï¼Œ
  æ ¹æ®å®é™…è·å¾—çš„å›æŠ¥ä¼°è®¡çŠ¶æ€æˆ–çŠ¶æ€-åŠ¨ä½œä»·å€¼ã€‚</p>

  <h3>ä¸€ã€åŸºæœ¬æ€æƒ³</h3>
  <p>ç»™å®šç­–ç•¥ \( \pi \)ï¼Œæ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’å¤šæ¬¡ï¼Œå¾—åˆ°è‹¥å¹²å®Œæ•´å›åˆï¼š</p>
  <p>$$
  (S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_T)
  $$</p>
  <p>å¯¹äºæ¯ä¸ªå›åˆï¼Œå®šä¹‰ä»æ—¶é—´æ­¥ \(t\) å¼€å§‹çš„ç´¯è®¡å›æŠ¥ï¼ˆReturnï¼‰ï¼š</p>
  <p>$$
  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}
  $$</p>
  <p>MC æ–¹æ³•é€šè¿‡å¤šæ¬¡é‡‡æ ·ï¼Œåˆ©ç”¨è¿™äº› \(G_t\) çš„å¹³å‡å€¼æ¥è¿‘ä¼¼ä»·å€¼å‡½æ•°ã€‚</p>

  <dl>
    <dt><b>çŠ¶æ€ä»·å€¼ä¼°è®¡ï¼š</b></dt>
    <dd>$$
    V(s) = \mathbb{E}_\pi[G_t \mid S_t = s] \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G^{(i)}(s)
    $$</dd>

    <dt><b>åŠ¨ä½œä»·å€¼ä¼°è®¡ï¼š</b></dt>
    <dd>$$
    Q(s,a) = \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a] \approx \frac{1}{N(s,a)} \sum_{i=1}^{N(s,a)} G^{(i)}(s,a)
    $$</dd>
  </dl>

  <h3>äºŒã€é¦–æ¬¡è®¿é—®ä¸æ¯æ¬¡è®¿é—®</h3>
  <ul>
    <li><b>First-Visit MCï¼š</b> ä»…åœ¨ä¸€ä¸ªå›åˆä¸­ç¬¬ä¸€æ¬¡è®¿é—®çŠ¶æ€ï¼ˆæˆ–çŠ¶æ€-åŠ¨ä½œå¯¹ï¼‰æ—¶è¿›è¡Œæ›´æ–°ã€‚</li>
    <li><b>Every-Visit MCï¼š</b> å¯¹å›åˆä¸­æ¯æ¬¡è®¿é—®çŠ¶æ€ï¼ˆæˆ–çŠ¶æ€-åŠ¨ä½œå¯¹ï¼‰éƒ½è¿›è¡Œæ›´æ–°ã€‚</li>
  </ul>
  <p>äºŒè€…æœ€ç»ˆéƒ½ä¼šæ”¶æ•›åˆ°çœŸå®çš„ä»·å€¼å‡½æ•°ï¼Œä½†é¦–æ¬¡è®¿é—® MC æ–¹å·®ç•¥å°ã€‚</p>

  <h3>ä¸‰ã€è’™ç‰¹å¡æ´›ç­–ç•¥è¯„ä¼°ç®—æ³•</h3>
  <p><strong>åŸºæœ¬æ€æƒ³ï¼š</strong>ç»™å®šç­–ç•¥ Ï€ï¼Œè¯„ä¼°å…¶çŠ¶æ€ä»·å€¼å‡½æ•° V(s)</p>
  <ol>
    <li><strong>åˆå§‹åŒ–ï¼š</strong>å¯¹æ‰€æœ‰çŠ¶æ€ sï¼Œè®¾ V(s) = 0ï¼ŒåŒæ—¶è®°å½•æ¯ä¸ªçŠ¶æ€çš„å›æŠ¥åˆ—è¡¨ Returns(s)</li>
    <li><strong>é‡‡é›†æ•°æ®ï¼š</strong>ä¸ç¯å¢ƒäº¤äº’å¤šæ¬¡ï¼Œæ¯æ¬¡äº§ç”Ÿä¸€ä¸ªå®Œæ•´å›åˆ (Sâ‚€, Aâ‚€, Râ‚, Sâ‚, ..., S_T)</li>
    <li><strong>è®¡ç®—å›æŠ¥ï¼š</strong>å¯¹æ¯ä¸ªå›åˆï¼Œä»æœ«å°¾å‘å‰è®¡ç®—ç´¯è®¡å›æŠ¥ G_t = R_{t+1} + Î³R_{t+2} + ...</li>
    <li><strong>ç­–ç•¥è¯„ä¼°ï¼ˆé¦–æ¬¡è®¿é—®ï¼‰ï¼š</strong>å¯¹æ¯ä¸ªçŠ¶æ€ï¼Œä»…åœ¨è¯¥å›åˆä¸­ç¬¬ä¸€æ¬¡å‡ºç°æ—¶ï¼š
      <ul>
        <li>å°†è¯¥æ—¶åˆ»çš„å›æŠ¥ G_t åŠ å…¥ Returns(S_t)</li>
        <li>æ›´æ–°ä»·å€¼å‡½æ•°ï¼šV(S_t) = Returns(S_t) çš„å¹³å‡å€¼</li>
      </ul>
    </li>
    <li><strong>é‡å¤ï¼š</strong>é‡å¤é‡‡é›†å’Œæ›´æ–°è¿‡ç¨‹ï¼ŒV(s) é€æ¸é€¼è¿‘çœŸå®æœŸæœ›å›æŠ¥</li>
  </ol>
  <p style="margin-top: 15px;">è¿™ä¸ªç®—æ³•æ˜¯<strong>æ— æ¨¡å‹ã€åŸºäºé‡‡æ ·</strong>çš„æ–¹æ³•â€”â€”åªéœ€è¦å®é™…å›åˆæ•°æ®ï¼Œä¸éœ€è¦çŸ¥é“ç¯å¢ƒçš„çŠ¶æ€è½¬ç§»æ¨¡å‹ã€‚</p>

  <h3>å››ã€è’™ç‰¹å¡æ´›æ§åˆ¶ï¼ˆMC Controlï¼‰</h3>
  <p>åœ¨ç­–ç•¥è¯„ä¼°çš„åŸºç¡€ä¸Šï¼ŒMC è¿˜å¯ä»¥å®ç°<strong>ç­–ç•¥æ”¹è¿›</strong>ï¼Œå½¢æˆä¸€ä¸ªä¸ç­–ç•¥è¿­ä»£ç±»ä¼¼çš„è¿‡ç¨‹ï¼š</p>
  <ol>
    <li>è¯„ä¼°å½“å‰ç­–ç•¥ \(\pi\)ï¼šä½¿ç”¨ MC æ–¹æ³•ä¼°è®¡ \(Q^\pi(s,a)\)</li>
    <li>æ”¹è¿›ç­–ç•¥ï¼šé‡‡ç”¨è´ªå©ªæˆ– Îµ-è´ªå©ªæ–¹å¼æ›´æ–°ç­–ç•¥
      <p>$$
      \pi'(s) = \arg\max_a Q(s,a)
      $$</p>
    </li>
    <li>é‡å¤ä»¥ä¸Šè¿‡ç¨‹ï¼Œç›´åˆ°æ”¶æ•›</li>
  </ol>

  <h3>äº”ã€Îµ-è´ªå©ªï¼ˆÎµ-Greedyï¼‰æ¢ç´¢ç­–ç•¥</h3>
  <p>ä¸ºäº†é¿å…è¿‡æ—©é™·å…¥æ¬¡ä¼˜ç­–ç•¥ï¼ŒMC æ§åˆ¶ä¸­å¸¸ç”¨ <b>Îµ-è´ªå©ªç­–ç•¥</b> è¿›è¡Œæ¢ç´¢ï¼š</p>
  <p>$$
  \pi(a|s) = 
  \begin{cases}
  1 - \varepsilon + \frac{\varepsilon}{|A(s)|}, & a = \arg\max_{a'} Q(s,a') \\
  \frac{\varepsilon}{|A(s)|}, & \text{å¦åˆ™}
  \end{cases}
  $$</p>
  <p>å…¶ä¸­ \(\varepsilon \in [0,1]\) è¡¨ç¤ºéšæœºæ¢ç´¢æ¦‚ç‡ã€‚</p>

  <h3>å…­ã€MC æ§åˆ¶ç¤ºä¾‹ä»£ç </h3>
<pre><code class="language-python">
import numpy as np
from collections import defaultdict

class MCControl:
    def __init__(self, num_states, num_actions, gamma=0.99, epsilon=0.1, alpha=0.1):
        self.num_states = num_states
        self.num_actions = num_actions
        self.gamma = gamma
        self.epsilon = epsilon
        self.alpha = alpha
        self.Q = defaultdict(lambda: np.zeros(num_actions))
        self.returns = defaultdict(list)  # å­˜å‚¨æ¯ä¸ª(s,a)çš„å›æŠ¥åˆ—è¡¨
        
    def epsilon_greedy(self, state):
        """Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.num_actions)
        else:
            return np.argmax(self.Q[state])
    
    def train_episode(self, env):
        """è¿è¡Œä¸€ä¸ªå®Œæ•´å›åˆå¹¶æ›´æ–°Qå€¼"""
        trajectory = []
        state = env.reset()
        done = False
        
        # æ”¶é›†å›åˆè½¨è¿¹
        while not done:
            action = self.epsilon_greedy(state)
            next_state, reward, done, _ = env.step(action)
            trajectory.append((state, action, reward))
            state = next_state
        
        # ä»åå‘å‰è®¡ç®—å›æŠ¥å¹¶æ›´æ–°Qå€¼
        G = 0
        visited_pairs = set()
        for t in range(len(trajectory) - 1, -1, -1):
            state, action, reward = trajectory[t]
            G = reward + self.gamma * G
            
            # åªåœ¨çŠ¶æ€-åŠ¨ä½œå¯¹ç¬¬ä¸€æ¬¡å‡ºç°æ—¶æ›´æ–°ï¼ˆFirst-Visit MCï¼‰
            if (state, action) not in visited_pairs:
                visited_pairs.add((state, action))
                self.returns[(state, action)].append(G)
                self.Q[state][action] = np.mean(self.returns[(state, action)])
    
    def train(self, env, num_episodes=1000):
        """è®­ç»ƒè¿‡ç¨‹"""
        rewards = []
        for episode in range(num_episodes):
            episode_reward = sum(r for _, _, r in self.collect_episode(env))
            rewards.append(episode_reward)
            self.train_episode(env)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        return rewards
    
    def get_policy(self):
        """è·å–è´ªå©ªç­–ç•¥"""
        policy = {}
        for state in self.Q:
            policy[state] = np.argmax(self.Q[state])
        return policy
  </code></pre>


  <h3>ä¸ƒã€MC æ–¹æ³•çš„ç‰¹ç‚¹ä¸å±€é™</h3>
  <table border="1" cellpadding="8" cellspacing="0">
    <thead>
      <tr><th>ä¼˜ç‚¹</th><th>ç¼ºç‚¹</th></tr>
    </thead>
    <tbody>
      <tr>
        <td>æ— éœ€çŸ¥é“ç¯å¢ƒæ¨¡å‹ï¼ˆPã€Rï¼‰</td>
        <td>å¿…é¡»ç­‰å¾…å®Œæ•´å›åˆç»“æŸï¼Œä¸èƒ½ç”¨äºæŒç»­ä»»åŠ¡</td>
      </tr>
      <tr>
        <td>å®ç°ç®€å•ï¼Œæ¦‚å¿µç›´è§‚</td>
        <td>æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ï¼Œæ–¹å·®å¤§</td>
      </tr>
      <tr>
        <td>é€‚åˆç¦»çº¿æ¨¡æ‹Ÿç¯å¢ƒï¼ˆå¦‚æ¸¸æˆï¼‰</td>
        <td>å¯¹é•¿æœŸä»»åŠ¡æˆ–é«˜ç»´çŠ¶æ€ç©ºé—´ä¸é€‚ç”¨</td>
      </tr>
    </tbody>
  </table>

  <h3>å…«ã€å°ç»“</h3>
  <ul>
    <li>MC æ˜¯åŸºäºé‡‡æ ·çš„<strong>ç­–ç•¥è¯„ä¼°ä¸æ§åˆ¶</strong>æ–¹æ³•ã€‚</li>
    <li>ä¾èµ–å›åˆç»“æŸçš„å®é™…å›æŠ¥ï¼Œè€Œéä¼°è®¡çš„ä¸‹ä¸€æ­¥ä»·å€¼ã€‚</li>
    <li>ä¸åŠ¨æ€è§„åˆ’ç›¸æ¯”ï¼Œä¸éœ€çŸ¥é“æ¨¡å‹ï¼Œä½†æ”¶æ•›æ…¢ã€‚</li>
    <li>æ˜¯ç†è§£æ—¶åºå·®åˆ†ï¼ˆTDï¼‰å­¦ä¹ çš„é‡è¦è¿‡æ¸¡ã€‚</li>
  </ul>

</section>
<!-- ç¬¬ä¸ƒç«  -->
<section id="chapter7" class="chapter">
  <h2>ç¬¬ä¸ƒç« ï¼šæ—¶é—´å·®åˆ†å­¦ä¹ ï¼ˆTD Learningï¼‰</h2>

  <p>æ—¶é—´å·®åˆ†å­¦ä¹ ï¼ˆTemporal Differenceï¼Œç®€ç§° <b>TD</b>ï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ç»“åˆäº†
  <strong>è’™ç‰¹å¡æ´›æ–¹æ³•</strong>ä¸<strong>åŠ¨æ€è§„åˆ’</strong>çš„ç­–ç•¥è¯„ä¼°ä¸æ§åˆ¶æ–¹æ³•ã€‚
  ä¸è’™ç‰¹å¡æ´›ä¸åŒï¼ŒTD å¯ä»¥åœ¨å›åˆæœªç»“æŸæ—¶å°±æ›´æ–°çŠ¶æ€ä»·å€¼ï¼Œå› æ­¤å±äº<strong>åœ¨çº¿æ›´æ–°</strong>æ–¹æ³•ã€‚</p>

  <h3>ä¸€ã€TD(0)æ ¸å¿ƒå…¬å¼</h3>
  <p>å•æ­¥TDæ›´æ–°çŠ¶æ€ä»·å€¼å‡½æ•°å…¬å¼ï¼š</p>
  <p>$$
  V(s_t) \leftarrow V(s_t) + \alpha \big[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \big]
  $$</p>
  <p>å…¶ä¸­ï¼š</p>
  <ul>
    <li><code>V(s_t)</code>ï¼šå½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡</li>
    <li><code>r_{t+1}</code>ï¼šå½“å‰åŠ¨ä½œçš„å³æ—¶å¥–åŠ±</li>
    <li><code>Î³</code>ï¼šæŠ˜æ‰£å› å­</li>
    <li><code>Î±</code>ï¼šå­¦ä¹ ç‡</li>
    <li><code>Î´_t = r_{t+1} + Î³ V(s_{t+1}) - V(s_t)</code>ï¼šTDè¯¯å·®</li>
  </ul>

  <h3>äºŒã€TD(0)ç®—æ³•æµç¨‹</h3>
  <p><strong>TD ä¸ MC æœ€å¤§çš„åŒºåˆ«ï¼šä¸ç­‰å¾…å›åˆç»“æŸï¼Œåœ¨æ¯ä¸€æ­¥å°±è¿›è¡Œæ›´æ–°ï¼</strong></p>
  <ol>
    <li><strong>åˆå§‹åŒ–ï¼š</strong>å¯¹æ‰€æœ‰çŠ¶æ€ sï¼Œè®¾ V(s) = 0</li>
    <li><strong>äº¤äº’ä¸€æ­¥ï¼š</strong>åœ¨çŠ¶æ€ s_t æ‰§è¡ŒåŠ¨ä½œ a_tï¼Œè§‚å¯Ÿå³æ—¶å¥–åŠ± r_{t+1} å’Œä¸‹ä¸€çŠ¶æ€ s_{t+1}</li>
    <li><strong>è®¡ç®— TD è¯¯å·®ï¼š</strong>Î´_t = r_{t+1} + Î³Â·V(s_{t+1}) - V(s_t)</li>
    <li><strong>æ›´æ–°ä»·å€¼å‡½æ•°ï¼š</strong>V(s_t) â† V(s_t) + Î±Â·Î´_tï¼ˆä½¿ç”¨å­¦ä¹ ç‡ Î±ï¼‰</li>
    <li><strong>ç»§ç»­äº¤äº’ï¼š</strong>s_t â† s_{t+1}ï¼Œé‡å¤æ­¥éª¤ 2-4ï¼Œä¸éœ€è¦ç­‰å¾…å›åˆç»“æŸ</li>
  </ol>
  <p style="margin-top: 15px;"><strong>å…³é”®ä¼˜åŠ¿ï¼š</strong></p>
  <ul>
    <li>âœ“ å¯ä»¥è¿›è¡Œ <strong>åœ¨çº¿å­¦ä¹ </strong>ï¼Œå›åˆä¸­å®æ—¶æ›´æ–°</li>
    <li>âœ“ å¯ä»¥ç”¨äº <strong>æŒç»­ä»»åŠ¡</strong>ï¼ˆæ— ç»ˆæ­¢çŠ¶æ€çš„ä»»åŠ¡ï¼‰</li>
    <li>âœ“ æ”¶æ•›é€Ÿåº¦æ¯” MC æ›´å¿«</li>
    <li>âœ“ åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­æ›´ç¨³å®š</li>
  </ul>

  <h3>ä¸‰ã€TDå­¦ä¹ ä¸MCæ–¹æ³•å¯¹æ¯”</h3>
  <table>
    <tr><th>æ–¹æ³•</th><th>æ›´æ–°æ—¶æœº</th><th>æ˜¯å¦ä¾èµ–ç¯å¢ƒæ¨¡å‹</th><th>åå·®/æ–¹å·®</th></tr>
    <tr><td>è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰</td><td>å›åˆç»“æŸ</td><td>å¦</td><td>æ— åï¼Œæ–¹å·®å¤§</td></tr>
    <tr><td>TD(0)</td><td>æ¯ä¸€æ­¥</td><td>å¦</td><td>æœ‰åï¼Œæ–¹å·®å°</td></tr>
  </table>
  <p>MCä½¿ç”¨å®Œæ•´å›æŠ¥ <code>G_t</code> æ›´æ–°ä»·å€¼ï¼Œè€ŒTDä½¿ç”¨ä¸‹ä¸€çŠ¶æ€ä¼°è®¡ <code>V(s_{t+1})</code> å¼•å¯¼å½“å‰æ›´æ–°ã€‚</p>

  <h3>å››ã€TD(Î»)ä¸èµ„æ ¼è¿¹</h3>
  <p>TD(Î»)å¼•å…¥äº† <b>èµ„æ ¼è¿¹ï¼ˆeligibility traceï¼‰</b>ï¼Œå¯ä»¥ç»“åˆå¤šæ­¥ä¿¡æ¯æ›´æ–°ä»·å€¼ï¼š</p>
  <p>$$
  V(s) \leftarrow V(s) + \alpha \delta_t e(s)
  $$</p>
  <ul>
    <li><code>e(s)</code>ï¼šçŠ¶æ€çš„èµ„æ ¼è¿¹ï¼Œè®°å½•è¿‡å»æ—¶é—´æ­¥çš„é‡è¦æ€§</li>
    <li>Î» âˆˆ [0,1] æ§åˆ¶å¤šæ­¥ä¿¡æ¯è¡°å‡ï¼šÎ»=0 â†’ TD(0)ï¼ŒÎ»=1 â†’ æ¥è¿‘MC</li>
  </ul>

  <h3>äº”ã€TDæ§åˆ¶ä¸Îµ-è´ªå©ªç­–ç•¥</h3>
  <p>åœ¨ç­–ç•¥è¯„ä¼°åŸºç¡€ä¸Šï¼ŒTDä¹Ÿå¯è¿›è¡Œç­–ç•¥æ”¹è¿›ï¼Œå®ç°TDæ§åˆ¶ï¼š</p>
  <ol>
    <li>è¯„ä¼°å½“å‰ç­–ç•¥ï¼šä½¿ç”¨TDä¼°è®¡ Q(s,a)</li>
    <li>æ”¹è¿›ç­–ç•¥ï¼šé‡‡ç”¨Îµ-è´ªå©ªé€‰æ‹©åŠ¨ä½œ
      <p>$$
      \pi(a|s) = 
      \begin{cases}
      1 - \varepsilon + \frac{\varepsilon}{|A(s)|}, & a = \arg\max_{a'} Q(s,a') \\
      \frac{\varepsilon}{|A(s)|}, & \text{å¦åˆ™}
      \end{cases}
      $$</p>
    </li>
    <li>é‡å¤ä»¥ä¸Šè¿‡ç¨‹ç›´åˆ°æ”¶æ•›</li>
  </ol>

  <h3>å…­ã€TD æ§åˆ¶ç¤ºä¾‹ä»£ç ï¼ˆSARSAï¼‰</h3>
  <pre><code class="language-python">
import numpy as np
from collections import defaultdict

class SARSA:
    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.num_states = num_states
        self.num_actions = num_actions
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = defaultdict(lambda: np.zeros(num_actions))
    
    def epsilon_greedy(self, state):
        """Îµ-è´ªå©ªç­–ç•¥"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.num_actions)
        else:
            return np.argmax(self.Q[state])
    
    def train_step(self, state, action, reward, next_state, next_action, done):
        """å•æ­¥SARSAæ›´æ–°"""
        if done:
            target = reward
        else:
            target = reward + self.gamma * self.Q[next_state][next_action]
        
        # TDè¯¯å·®
        td_error = target - self.Q[state][action]
        
        # Qå€¼æ›´æ–°
        self.Q[state][action] += self.alpha * td_error
    
    def train_episode(self, env):
        """è¿è¡Œä¸€ä¸ªå®Œæ•´å›åˆ"""
        state = env.reset()
        action = self.epsilon_greedy(state)
        done = False
        episode_reward = 0
        
        while not done:
            next_state, reward, done, _ = env.step(action)
            next_action = self.epsilon_greedy(next_state)
            
            # æ›´æ–°Qå€¼
            self.train_step(state, action, reward, next_state, next_action, done)
            
            episode_reward += reward
            state = next_state
            action = next_action
        
        return episode_reward
    
    def train(self, env, num_episodes=1000):
        """è®­ç»ƒè¿‡ç¨‹"""
        rewards = []
        for episode in range(num_episodes):
            reward = self.train_episode(env)
            rewards.append(reward)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        return rewards
  </code></pre>

  <h3>ä¸ƒã€Bootstrappingï¼ˆè‡ªä¸¾/å¼•å¯¼ï¼‰è§£é‡Š</h3>
  <p><b>Bootstrapping</b> æ˜¯TDæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³ï¼š</p>
  <p>TDä¸æ˜¯ç­‰åˆ°å®Œæ•´å›åˆè·å¾—çœŸå®å›æŠ¥ <code>G_t</code> æ‰æ›´æ–°ä»·å€¼ï¼Œè€Œæ˜¯ä½¿ç”¨å¯¹ä¸‹ä¸€çŠ¶æ€ä»·å€¼çš„ä¼°è®¡ <code>V(s_{t+1})</code> æ¥å¼•å¯¼å½“å‰çŠ¶æ€ä»·å€¼æ›´æ–°ï¼š</p>
  <p>$$
  V(s_t) \leftarrow V(s_t) + \alpha \big[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \big]
  $$</p>
  <p>è¿™ç§æ–¹å¼ç§°ä¸ºâ€œè‡ªä¸¾â€ï¼Œç›´è§‚ç†è§£ä¸ºï¼š</p>
  <ul>
    <li>ç”¨å½“å‰å·²æœ‰çš„ä¼°è®¡å»æ”¹è¿›è‡ªèº«</li>
    <li>å®ç°åœ¨çº¿ã€é€æ­¥æ›´æ–°ï¼Œä¸ä¾èµ–æ•´å›åˆå®Œæ•´ä¿¡æ¯</li>
    <li>ä¼˜ç‚¹ï¼šæ”¶æ•›å¿«ã€å¯åœ¨çº¿å­¦ä¹ ï¼›ç¼ºç‚¹ï¼šåˆå§‹ä¼°è®¡ä¸å‡†å¯èƒ½å¼•å…¥åå·®</li>
  </ul>

</section>
<!-- ç¬¬å…«ç«  -->
<section id="chapter8" class="chapter">
  <h2>ç¬¬å…«ç« ï¼šQ-learning</h2>

  <p>Q-learning æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­æœ€ç»å…¸çš„ <strong>å€¼è¿­ä»£ç®—æ³•</strong>ï¼Œç”¨äºå­¦ä¹ æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•° <code>Q*(s,a)</code>ï¼Œä»è€Œé—´æ¥å¾—åˆ°æœ€ä¼˜ç­–ç•¥ï¼š
  </p>
  <p>
  $$
  \pi^*(s) = \arg\max_a Q^*(s,a)
  $$
  </p>
  <p>ç‰¹ç‚¹ï¼š</p>
  <ul>
    <li>å±äº <strong>off-policy</strong> ç®—æ³•ï¼šæ›´æ–° Q å€¼æ—¶ä½¿ç”¨æœ€å¤§åŒ–æœªæ¥ Q å€¼ï¼Œè€Œè¡Œä¸ºç­–ç•¥å¯ä»¥å¸¦æ¢ç´¢</li>
    <li>ç›®æ ‡æ˜¯æ‰¾åˆ°èƒ½è·å¾—æœ€å¤§ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±çš„åŠ¨ä½œ</li>
  </ul>

  <h3>ä¸€ã€æ ¸å¿ƒå…¬å¼</h3>
  <p>Q-learning æ ¸å¿ƒæ›´æ–°å…¬å¼ï¼š</p>
  <p>
  $$
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \big[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \big]
  $$
  </p>
  <table>
    <tr><th>ç¬¦å·</th><th>å«ä¹‰</th></tr>
    <tr><td>s<sub>t</sub>, a<sub>t</sub></td><td>å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œ</td></tr>
    <tr><td>r<sub>t+1</sub></td><td>å³æ—¶å¥–åŠ±</td></tr>
    <tr><td>s<sub>t+1</sub></td><td>ä¸‹ä¸€çŠ¶æ€</td></tr>
    <tr><td>Î±</td><td>å­¦ä¹ ç‡ï¼ˆ0.1~0.5ï¼Œä¸€èˆ¬è®¾ç½®ï¼‰</td></tr>
    <tr><td>Î³</td><td>æŠ˜æ‰£å› å­ï¼ˆ0.9~0.99ï¼Œä¸€èˆ¬è®¾ç½®ï¼‰</td></tr>
    <tr><td>max Q(s<sub>t+1</sub>,a')</td><td>æœªæ¥çŠ¶æ€æœ€ä¼˜åŠ¨ä½œä»·å€¼</td></tr>
  </table>

  <h3>äºŒã€ç®—æ³•æµç¨‹</h3>
  <p><strong>Q-learning æ˜¯ Off-policy æ–¹æ³•ï¼Œè¡Œä¸ºç­–ç•¥å¯ä»¥æ¢ç´¢ï¼Œä½†å­¦ä¹ çš„æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚</strong></p>
  <ol>
    <li><strong>åˆå§‹åŒ–ï¼š</strong>å¯¹æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹ (s,a)ï¼Œè®¾ Q(s,a) = 0</li>
    <li><strong>ç­–ç•¥é€‰æ‹©ï¼š</strong>ä½¿ç”¨ Îµ-greedy ç­–ç•¥ä»çŠ¶æ€ s é€‰æ‹©åŠ¨ä½œ a
      <ul>
        <li>ä»¥æ¦‚ç‡ Îµ éšæœºé€‰æ‹©ï¼ˆæ¢ç´¢ï¼‰</li>
        <li>ä»¥æ¦‚ç‡ 1-Îµ é€‰æ‹© Q å€¼æœ€å¤§çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰</li>
      </ul>
    </li>
    <li><strong>æ‰§è¡Œå¹¶è§‚å¯Ÿï¼š</strong>æ‰§è¡ŒåŠ¨ä½œ aï¼Œå¾—åˆ°å¥–åŠ± r å’Œä¸‹ä¸€çŠ¶æ€ s'</li>
    <li><strong>è®¡ç®—ç›®æ ‡ Q å€¼ï¼š</strong>y = r + Î³Â·max_{a'} Q(s',a')
      <ul>
        <li>è™½ç„¶æˆ‘ä»¬ç”¨ Îµ-greedy æ¢ç´¢ï¼Œä½†è¿™é‡Œå– max è¯´æ˜æˆ‘ä»¬æœç€æœ€ä¼˜ç­–ç•¥å­¦ä¹ </li>
      </ul>
    </li>
    <li><strong>æ›´æ–° Q å€¼ï¼š</strong>Q(s,a) â† Q(s,a) + Î±Â·(y - Q(s,a))</li>
    <li><strong>é‡å¤ï¼š</strong>s â† s'ï¼Œç»§ç»­äº¤äº’å’Œæ›´æ–°</li>
  </ol>
  <p style="margin-top: 15px;"><strong>å­¦ä¹ ç‡ Î±ã€æŠ˜æ‰£å› å­ Î³ã€æ¢ç´¢ç‡ Îµ çš„ä½œç”¨ï¼š</strong></p>
  <ul>
    <li>Î±ï¼ˆå­¦ä¹ ç‡ï¼‰ï¼š0.1~0.5ï¼Œæ§åˆ¶å¯¹æ–°ä¿¡æ¯çš„å¸æ”¶ç¨‹åº¦</li>
    <li>Î³ï¼ˆæŠ˜æ‰£å› å­ï¼‰ï¼š0.9~0.99ï¼Œå†³å®šå¯¹æœªæ¥å¥–åŠ±çš„é‡è§†ç¨‹åº¦</li>
    <li>Îµï¼ˆæ¢ç´¢ç‡ï¼‰ï¼šé€šå¸¸ä» 1.0 è¡°å‡åˆ° 0.01~0.1ï¼Œé€æ­¥ä»æ¢ç´¢è½¬å‘åˆ©ç”¨</li>
  </ul>

  <h3>ä¸‰ã€ç‰¹ç‚¹ä¸æ³¨æ„äº‹é¡¹</h3>
  <ul>
    <li>é€‚ç”¨åœºæ™¯ï¼šç¦»æ•£çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å°çš„ä»»åŠ¡</li>
    <li>æ¢ç´¢ç­–ç•¥ï¼šÎµ-greedy æˆ– Boltzmannï¼Œå¯éšè®­ç»ƒè¡°å‡ Îµ</li>
    <li>å­¦ä¹ ç‡ Î±ï¼šä¸€èˆ¬ 0.1~0.5ï¼Œå¤ªå¤§ä¸ç¨³å®šï¼Œå¤ªå°æ”¶æ•›æ…¢</li>
    <li>æŠ˜æ‰£å› å­ Î³ï¼šä¸€èˆ¬ 0.9~0.99</li>
    <li>æ”¶æ•›æ€§ï¼šæ¯ä¸ªçŠ¶æ€åŠ¨ä½œå¯¹è¢«å……åˆ†è®¿é—®ï¼Œä¸” Î± è¡°å‡æ—¶å¯æ”¶æ•›</li>
  </ul>

  <h3>å››ã€Python ç¤ºä¾‹</h3>
  <pre><code class="language-python">
import numpy as np
import gym

class QLearning:
    """Q-learning ç®—æ³•å®ç°"""
    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.n_states = n_states
        self.n_actions = n_actions
        self.alpha = alpha  # å­¦ä¹ ç‡
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # æ¢ç´¢ç‡
        self.Q = np.zeros((n_states, n_actions))
    
    def select_action(self, state):
        """Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)
        else:
            return np.argmax(self.Q[state])
    
    def update(self, state, action, reward, next_state, done):
        """Q å€¼æ›´æ–°"""
        target = reward + self.gamma * np.max(self.Q[next_state]) * (1 - done)
        td_error = target - self.Q[state, action]
        self.Q[state, action] += self.alpha * td_error
    
    def train(self, env, n_episodes=2000, max_steps=100):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        for episode in range(n_episodes):
            state = env.reset()
            if isinstance(state, tuple):
                state = state[0]  # å…¼å®¹æ–°ç‰ˆ gym
            done = False
            
            for _ in range(max_steps):
                action = self.select_action(state)
                result = env.step(action)
                next_state = result[0]
                reward = result[1]
                done = result[2]
                
                self.update(state, action, reward, next_state, done)
                state = next_state
                if done:
                    break
        
        return self.Q


# ä½¿ç”¨ç¤ºä¾‹
env = gym.make('FrozenLake-v1', is_slippery=False)
n_states = env.observation_space.n
n_actions = env.action_space.n

agent = QLearning(n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1)
Q_table = agent.train(env, n_episodes=2000, max_steps=100)

print("è®­ç»ƒå®Œæˆï¼")
print("Q è¡¨å½¢çŠ¶ï¼š", Q_table.shape)
print("Q è¡¨ï¼š\n", Q_table)
  </code></pre>

  <h3>äº”ã€å‚æ•°è¯´æ˜</h3>
  <table>
    <tr><th>å‚æ•°</th><th>å»ºè®®èŒƒå›´</th><th>è¯´æ˜</th></tr>
    <tr><td>Î±ï¼ˆå­¦ä¹ ç‡ï¼‰</td><td>0.1~0.5</td><td>æ§åˆ¶ Q æ›´æ–°å¹…åº¦</td></tr>
    <tr><td>Î³ï¼ˆæŠ˜æ‰£å› å­ï¼‰</td><td>0.9~0.99</td><td>é•¿æœŸå¥–åŠ±é‡è¦æ€§</td></tr>
    <tr><td>Îµï¼ˆæ¢ç´¢ç‡ï¼‰</td><td>0.05~0.2</td><td>Îµ-greedy æ¢ç´¢æ¦‚ç‡ï¼Œå¯éšè®­ç»ƒè¡°å‡</td></tr>
    <tr><td>n_episodes</td><td>500~5000</td><td>æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´</td></tr>
    <tr><td>max_steps</td><td>100~1000</td><td>æ¯å›åˆæœ€å¤§æ­¥æ•°ï¼Œé¿å…æ— é™å¾ªç¯</td></tr>
  </table>

</section>
<!-- ç¬¬ä¹ç«  -->
<section id="chapter9" class="chapter">
  <h2>ç¬¬ä¹ç« ï¼šDQNï¼ˆDeep Q-Networkï¼‰</h2>

  <p>DQNï¼ˆDeep Q-Networkï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ç”¨äºé«˜ç»´çŠ¶æ€ç©ºé—´çš„ç»å…¸ç®—æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ç”¨<strong>æ·±åº¦ç¥ç»ç½‘ç»œ</strong>é€¼è¿‘åŠ¨ä½œä»·å€¼å‡½æ•° Q(s,a;Î¸)ï¼Œä»è€Œé—´æ¥å¾—åˆ°æœ€ä¼˜ç­–ç•¥ã€‚</p>
  
  <h3>ä¸€ã€èƒŒæ™¯ä¸åŸç†</h3>
  <ul>
    <li>ä¼ ç»Ÿ Q-learning åœ¨å¤§æˆ–è¿ç»­çŠ¶æ€ç©ºé—´ä¸­æ— æ³•ä½¿ç”¨ Q è¡¨</li>
    <li>DQN ä½¿ç”¨ç¥ç»ç½‘ç»œå°†çŠ¶æ€æ˜ å°„åˆ°åŠ¨ä½œä»·å€¼</li>
    <li>é€‚ç”¨äºé«˜ç»´è¾“å…¥ï¼Œå¦‚å›¾åƒç¯å¢ƒï¼ˆAtari æ¸¸æˆï¼‰</li>
    <li>æ ¸å¿ƒæŒ‘æˆ˜ï¼šè®­ç»ƒç¨³å®šæ€§ï¼Œéœ€è¦ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œ</li>
  </ul>

  <h3>äºŒã€æ ¸å¿ƒå…¬å¼</h3>
  <p>DQN æœ€å°åŒ– TD è¯¯å·®æŸå¤±ï¼š</p>
  <p>$$
  L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \Big[ \big( y - Q(s,a;\theta) \big)^2 \Big]
  $$</p>
  <p>å…¶ä¸­ç›®æ ‡ Q å€¼ä¸ºï¼š</p>
  <p>$$
  y = r + \gamma \max_{a'} Q(s',a'; \theta^-)
  $$</p>
  <ul>
    <li>Q(s,a;Î¸)ï¼šå½“å‰ Q ç½‘ç»œé¢„æµ‹</li>
    <li>yï¼šç›®æ ‡ Q å€¼ï¼Œç”±ç›®æ ‡ç½‘ç»œ Î¸â» æä¾›</li>
    <li>ğ’Ÿï¼šç»éªŒå›æ”¾æ± </li>
    <li>Î³ï¼šæŠ˜æ‰£å› å­</li>
  </ul>

  <h3>ä¸‰ã€è®­ç»ƒæµç¨‹</h3>
  <ol>
    <li>åˆå§‹åŒ– Q ç½‘ç»œ Q(s,a;Î¸) å’Œç›®æ ‡ç½‘ç»œ Q(s,a;Î¸â»)</li>
    <li>ä¸ç¯å¢ƒäº¤äº’ï¼ŒæŒ‰ Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œï¼Œå­˜å…¥ç»éªŒå›æ”¾æ± </li>
    <li>ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·å°æ‰¹é‡æ•°æ®è®­ç»ƒ Q ç½‘ç»œï¼Œæœ€å°åŒ– TD loss</li>
    <li>å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ Î¸â» â† Î¸</li>
    <li>é‡å¤ä»¥ä¸Šæ­¥éª¤ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›</li>
  </ol>

  <h3>å››ã€æ ¸å¿ƒæŠ€æœ¯ç‚¹</h3>
  <table>
    <tr><th>æŠ€æœ¯</th><th>ä½œç”¨</th></tr>
    <tr><td>ç»éªŒå›æ”¾ Replay Buffer</td><td>æ‰“ç ´æ•°æ®ç›¸å…³æ€§ï¼Œæé«˜æ ·æœ¬åˆ©ç”¨ç‡ï¼Œç¨³å®šè®­ç»ƒ</td></tr>
    <tr><td>ç›®æ ‡ç½‘ç»œ Target Network</td><td>å‡ç¼“è®­ç»ƒéœ‡è¡ï¼Œä¿è¯ç›®æ ‡ç¨³å®š</td></tr>
    <tr><td>Îµ-greedy ç­–ç•¥</td><td>å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨</td></tr>
    <tr><td>å½’ä¸€åŒ– / å›¾åƒé¢„å¤„ç†</td><td>é™ä½é«˜ç»´è¾“å…¥ç»´åº¦ï¼Œæå‡è®­ç»ƒæ•ˆç‡</td></tr>
  </table>

  <h3>äº”ã€DQN ç¤ºä¾‹ä»£ç </h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple

Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))

class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.memory = deque(maxlen=capacity)
    
    def add(self, state, action, reward, next_state, done):
        self.memory.append(Transition(state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        return zip(*batch)
    
    def __len__(self):
        return len(self.memory)

class DQN:
    def __init__(self, state_dim, action_dim, gamma=0.99, epsilon=1.0, alpha=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.min_epsilon = 0.01
        self.epsilon_decay = 0.995
        
        # Qç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.Q = QNetwork(state_dim, action_dim)
        self.Q_target = QNetwork(state_dim, action_dim)
        self.Q_target.load_state_dict(self.Q.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.Q.parameters(), lr=alpha)
        self.replay_buffer = ReplayBuffer(capacity=10000)
        self.loss_fn = nn.MSELoss()
    
    def select_action(self, state):
        """Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            with torch.no_grad():
                q_values = self.Q(state_tensor)
            return q_values.argmax(dim=1).item()
    
    def train_step(self, batch_size):
        """ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·å¹¶è®­ç»ƒ"""
        if len(self.replay_buffer) < batch_size:
            return
        
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
        
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(np.array(actions))
        rewards = torch.FloatTensor(np.array(rewards))
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(np.array(dones))
        
        # è®¡ç®—å½“å‰Qå€¼
        q_values = self.Q(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            max_next_q = self.Q_target(next_states).max(dim=1)[0]
            target_q = rewards + self.gamma * max_next_q * (1 - dones)
        
        # è®¡ç®—æŸå¤±
        loss = self.loss_fn(q_values, target_q)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, env, num_episodes=1000, batch_size=32, target_update_freq=500):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        rewards = []
        step_count = 0
        
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            episode_reward = 0
            
            while not done:
                # é€‰æ‹©åŠ¨ä½œ
                action = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                # å­˜å‚¨ç»éªŒ
                self.replay_buffer.add(state, action, reward, next_state, done)
                
                # è®­ç»ƒ
                self.train_step(batch_size)
                
                episode_reward += reward
                state = next_state
                step_count += 1
                
                # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
                if step_count % target_update_freq == 0:
                    self.Q_target.load_state_dict(self.Q.state_dict())
            
            rewards.append(episode_reward)
            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Epsilon: {self.epsilon:.3f}")
        
        return rewards
  </code></pre>

  <h3>å…­ã€å‚æ•°è¯´æ˜</h3>
  <table>
    <tr><th>å‚æ•°</th><th>å»ºè®®èŒƒå›´</th><th>è¯´æ˜</th></tr>
    <tr><td>Î³ï¼ˆæŠ˜æ‰£å› å­ï¼‰</td><td>0.95~0.99</td><td>é•¿æœŸå¥–åŠ±æƒé‡</td></tr>
    <tr><td>å­¦ä¹ ç‡ lr</td><td>1e-4~1e-3</td><td>Adam æˆ– RMSProp ä¼˜åŒ–å™¨</td></tr>
    <tr><td>Îµ åˆå€¼</td><td>1.0</td><td>åˆæœŸé«˜æ¢ç´¢</td></tr>
    <tr><td>Îµ æœ€å°å€¼</td><td>0.01~0.1</td><td>ä¿æŒä¸€å®šæ¢ç´¢</td></tr>
    <tr><td>Îµ è¡°å‡ç‡</td><td>0.995~0.999</td><td>é€æ­¥é™ä½ Îµ</td></tr>
    <tr><td>ç»éªŒå›æ”¾å¤§å°</td><td>1e4~1e6</td><td>æ ¹æ®ç¯å¢ƒå¤§å°é€‰æ‹©</td></tr>
    <tr><td>æ‰¹é‡å¤§å° batch_size</td><td>32~64</td><td>æ¯æ¬¡è®­ç»ƒé‡‡æ ·é‡</td></tr>
    <tr><td>ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡</td><td>500~10000 steps</td><td>å®šæœŸåŒæ­¥ Q ç½‘ç»œåˆ°ç›®æ ‡ç½‘ç»œ</td></tr>
  </table>

  <h3>ä¸ƒã€è®­ç»ƒå®Œæˆåˆ¤æ–­</h3>
  <ul>
    <li><strong>ç´¯è®¡å¥–åŠ±ç¨³å®šï¼š</strong>è¿ç»­è‹¥å¹²å›åˆå¹³å‡å¥–åŠ±å˜åŒ–å¾ˆå°</li>
    <li><strong>æµ‹è¯•ç¯å¢ƒè¡¨ç°ï¼š</strong>ä½¿ç”¨çº¯åˆ©ç”¨ç­–ç•¥æµ‹è¯•å¹³å‡æˆåŠŸç‡æˆ–å®Œæˆç‡è¾¾åˆ°ç›®æ ‡</li>
    <li><strong>æŸå¤±æ”¶æ•›ï¼š</strong>è§‚å¯Ÿ TD loss åŸºæœ¬ç¨³å®š</li>
    <li><strong>Îµ-greedy æ¢ç´¢ç‡è¾ƒä½ï¼š</strong>ç­–ç•¥ä¸»è¦ä¾èµ–ç½‘ç»œä¼°è®¡åŠ¨ä½œï¼Œæµ‹è¯•è¡¨ç°ç¨³å®š</li>
  </ul>
  <pre><code class="language-python">
# æ¯éš” N å›åˆæµ‹è¯•ç­–ç•¥
if episode % test_interval == 0:
    total_reward = 0
    for _ in range(test_episodes):
        state = env.reset()
        done = False
        while not done:
            action = np.argmax(Q.predict(state))
            state, reward, done, _ = env.step(action)
            total_reward += reward
    avg_reward = total_reward / test_episodes
    if avg_reward >= target_reward:
        print("è®­ç»ƒå®Œæˆï¼Œç­–ç•¥æ”¶æ•›")
        break
  </code></pre>

</section>

<!-- ç¬¬åç«  -->
<section id="chapter10" class="chapter">
  <h2>ç¬¬åç« ï¼šDDQNï¼ˆDouble Deep Q-Networkï¼‰</h2>

  <p>DDQN æ˜¯ DQN çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç”¨äºè§£å†³ DQN åœ¨è®­ç»ƒä¸­å­˜åœ¨çš„<strong>Q å€¼è¿‡ä¼°è®¡ï¼ˆOverestimationï¼‰</strong>é—®é¢˜ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å°†åŠ¨ä½œé€‰æ‹©ä¸åŠ¨ä½œè¯„ä»·åˆ†å¼€ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚</p>

  <h3>ä¸€ã€èƒŒæ™¯ä¸åŸç†</h3>
  <ul>
    <li>DQN ä½¿ç”¨ max(Q) åŒæ—¶é€‰æ‹©æœ€ä¼˜åŠ¨ä½œå’Œä¼°è®¡åŠ¨ä½œä»·å€¼ï¼Œå®¹æ˜“é«˜ä¼° Q å€¼ã€‚</li>
    <li>DDQN å°†åŠ¨ä½œé€‰æ‹©å’ŒåŠ¨ä½œè¯„ä»·åˆ†å¼€ï¼š<strong>ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ</strong>ï¼Œ<strong>ç›®æ ‡ç½‘ç»œè¯„ä»·åŠ¨ä½œä»·å€¼</strong>ã€‚</li>
    <li>è¿™æ ·å¯ä»¥é™ä½ max æ“ä½œå¯¹å¶ç„¶é«˜ä¼°çš„æ”¾å¤§æ•ˆåº”ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚</li>
  </ul>

  <h3>äºŒã€æ ¸å¿ƒå…¬å¼</h3>
  <p>DDQN ç›®æ ‡ Q å€¼è®¡ç®—ï¼š</p>
  <p>$$
  y^{DDQN} = r + \gamma Q(s', \arg\max_{a'} Q(s', a'; \theta), \theta^-)
  $$</p>
  <ul>
    <li>åŠ¨ä½œé€‰æ‹©ä½¿ç”¨ä¸»ç½‘ç»œå‚æ•° <code>Î¸</code></li>
    <li>åŠ¨ä½œè¯„ä»·ä½¿ç”¨ç›®æ ‡ç½‘ç»œå‚æ•° <code>Î¸^-</code></li>
    <li>æŸå¤±å‡½æ•°ä¸ DQN ä¸€è‡´ï¼š
      <p>$$
      L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \Big[ (y^{DDQN} - Q(s,a;\theta))^2 \Big]
      $$</p>
    </li>
  </ul>

  <h3>ä¸‰ã€è®­ç»ƒæµç¨‹</h3>
  <ol>
    <li>åˆå§‹åŒ–ä¸»ç½‘ç»œ Q(s,a;Î¸) å’Œç›®æ ‡ç½‘ç»œ Q(s,a;Î¸â»)</li>
    <li>ä¸ç¯å¢ƒäº¤äº’ï¼Œä½¿ç”¨ Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œï¼Œå­˜å…¥ç»éªŒå›æ”¾æ± </li>
    <li>ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·å°æ‰¹é‡è®­ç»ƒï¼Œä½¿ç”¨ DDQN ç›®æ ‡æ›´æ–°ä¸»ç½‘ç»œå‚æ•° Î¸</li>
    <li>å®šæœŸåŒæ­¥ç›®æ ‡ç½‘ç»œ Î¸â» â† Î¸</li>
    <li>é‡å¤ä»¥ä¸Šæ­¥éª¤ï¼Œç›´åˆ°è®­ç»ƒå®Œæˆ</li>
  </ol>

  <h3>å››ã€DDQN ä¸ DQN åŒºåˆ«</h3>
  <table>
    <tr><th>ç‰¹æ€§</th><th>DQN</th><th>DDQN</th></tr>
    <tr><td>ç›®æ ‡è®¡ç®—</td><td>y = r + Î³ max_a Q(s',a;Î¸â»)</td><td>y = r + Î³ Q(s', argmax_a Q(s',a;Î¸), Î¸â»)</td></tr>
    <tr><td>é€‰æ‹©åŠ¨ä½œ</td><td>ç›®æ ‡ç½‘ç»œ</td><td>ä¸»ç½‘ç»œ</td></tr>
    <tr><td>è¯„ä»·åŠ¨ä½œ</td><td>ç›®æ ‡ç½‘ç»œ</td><td>ç›®æ ‡ç½‘ç»œ</td></tr>
    <tr><td>è¿‡ä¼°è®¡é—®é¢˜</td><td>å­˜åœ¨</td><td>å¤§å¹…å‡è½»</td></tr>
    <tr><td>è®­ç»ƒç¨³å®šæ€§</td><td>ä¸€èˆ¬</td><td>æ›´ç¨³</td></tr>
  </table>

  <h3>äº”ã€DDQN ç¤ºä¾‹ä»£ç </h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

class DDQN:
    def __init__(self, state_dim, action_dim, gamma=0.99, epsilon=1.0, alpha=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.min_epsilon = 0.01
        self.epsilon_decay = 0.995
        
        # Qç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.Q = self._build_network(state_dim, action_dim)
        self.Q_target = self._build_network(state_dim, action_dim)
        self.Q_target.load_state_dict(self.Q.state_dict())
        
        self.optimizer = optim.Adam(self.Q.parameters(), lr=alpha)
        self.replay_buffer = deque(maxlen=10000)
        self.loss_fn = nn.MSELoss()
    
    def _build_network(self, state_dim, action_dim):
        return nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def select_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            q_values = self.Q(state_tensor)
        return q_values.argmax(dim=1).item()
    
    def train_step(self, batch_size):
        if len(self.replay_buffer) < batch_size:
            return
        
        indices = np.random.choice(len(self.replay_buffer), batch_size, replace=False)
        batch = [self.replay_buffer[i] for i in indices]
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(np.array(actions))
        rewards = torch.FloatTensor(np.array(rewards))
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(np.array(dones))
        
        # å½“å‰Qå€¼
        q_values = self.Q(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # DDQNæ ¸å¿ƒï¼šç”¨Qç½‘ç»œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼Œç”¨Q_targetç½‘ç»œè¯„ä¼°ä»·å€¼
        with torch.no_grad():
            # ç”¨Qç½‘ç»œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            next_actions = self.Q(next_states).argmax(dim=1)
            # ç”¨Q_targetç½‘ç»œè¯„ä¼°è¿™ä¸ªåŠ¨ä½œçš„ä»·å€¼
            max_next_q = self.Q_target(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
            target_q = rewards + self.gamma * max_next_q * (1 - dones)
        
        # è®¡ç®—æŸå¤±
        loss = self.loss_fn(q_values, target_q)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, env, num_episodes=1000, batch_size=32, target_update_freq=500):
        rewards = []
        step_count = 0
        
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            episode_reward = 0
            
            while not done:
                action = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                self.replay_buffer.append((state, action, reward, next_state, done))
                self.train_step(batch_size)
                
                episode_reward += reward
                state = next_state
                step_count += 1
                
                if step_count % target_update_freq == 0:
                    self.Q_target.load_state_dict(self.Q.state_dict())
            
            rewards.append(episode_reward)
            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        
        return rewards
  </code></pre>

  <h3>å…­ã€å‚æ•°è¯´æ˜ï¼ˆä¸€èˆ¬è®¾ç½®ï¼‰</h3>
  <table>
    <tr><th>å‚æ•°</th><th>å»ºè®®èŒƒå›´</th><th>è¯´æ˜</th></tr>
    <tr><td>Î³ï¼ˆæŠ˜æ‰£å› å­ï¼‰</td><td>0.95~0.99</td><td>é•¿æœŸå¥–åŠ±æƒé‡</td></tr>
    <tr><td>å­¦ä¹ ç‡ lr</td><td>1e-4~1e-3</td><td>Adam æˆ– RMSProp ä¼˜åŒ–å™¨</td></tr>
    <tr><td>Îµ åˆå€¼</td><td>1.0</td><td>åˆæœŸé«˜æ¢ç´¢</td></tr>
    <tr><td>Îµ æœ€å°å€¼</td><td>0.01~0.1</td><td>ä¿æŒä¸€å®šæ¢ç´¢</td></tr>
    <tr><td>Îµ è¡°å‡ç‡</td><td>0.995~0.999</td><td>é€æ­¥é™ä½ Îµ</td></tr>
    <tr><td>ç»éªŒå›æ”¾å¤§å°</td><td>1e4~1e6</td><td>æ ¹æ®ç¯å¢ƒå¤§å°é€‰æ‹©</td></tr>
    <tr><td>æ‰¹é‡å¤§å° batch_size</td><td>32~64</td><td>æ¯æ¬¡è®­ç»ƒé‡‡æ ·é‡</td></tr>
    <tr><td>ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡</td><td>500~10000 steps</td><td>å®šæœŸåŒæ­¥ Q ç½‘ç»œåˆ°ç›®æ ‡ç½‘ç»œ</td></tr>
  </table>

  <h3>ä¸ƒã€è®­ç»ƒå®Œæˆåˆ¤æ–­</h3>
  åŒä¸Š
  <h3>å…«ã€Q ç½‘ç»œå‚æ•°æ›´æ–°ä¸è¿‡ä¼°è®¡åˆ†æ</h3>
  <ul>
    <li><strong>Q ç½‘ç»œå‚æ•° Î¸ å®æ—¶æ›´æ–°ï¼š</strong>ç”¨äºé€‰æ‹©åŠ¨ä½œï¼Œéšç€æ¢¯åº¦ä¸‹é™ä¸æ–­è®­ç»ƒ</li>
    <li><strong>ç›®æ ‡ç½‘ç»œå‚æ•° Î¸â» éš”ä¸€æ®µæ—¶é—´æ›´æ–°ï¼š</strong>ç”¨äºè¯„ä»·åŠ¨ä½œä»·å€¼ï¼Œä¿æŒç¨³å®šé¿å… max æ“ä½œæŠŠå¶ç„¶é«˜ä¼°æ”¾å¤§</li>
    <li><strong>è®­ç»ƒé€»è¾‘ï¼š</strong>ä¸»ç½‘ç»œå®æ—¶å­¦ â†’ é€‰æ‹©åŠ¨ä½œï¼›ç›®æ ‡ç½‘ç»œç¨³å®š â†’ è¯„ä»·åŠ¨ä½œï¼›å®šæœŸåŒæ­¥ â†’ Î¸â» â† Î¸</li>
    <li>è¿™æ ·åšçš„ç›´è§‰ï¼šä¸»ç½‘ç»œåƒâ€œå†²åŠ¨çš„ç©å®¶â€ï¼Œç›®æ ‡ç½‘ç»œåƒâ€œç¨³é‡çš„è£åˆ¤â€ï¼Œé™ä½ Q å€¼è¿‡ä¼°è®¡ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§</li>
  </ul>

</section>
<!-- ç¬¬åä¸€ç«  -->
<section id="chapter11" class="chapter">
  <h2>ç¬¬åä¸€ç« ï¼šç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆPolicy Gradient, PGï¼‰</h2>

  <p>ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç›´æ¥å¯¹ç­–ç•¥ <code>&pi;(a|s;Î¸)</code> å‚æ•°åŒ–ä¼˜åŒ–ï¼Œè€Œä¸æ˜¯å…ˆä¼°è®¡ Q å€¼å†é€‰åŠ¨ä½œã€‚é€‚ç”¨äºåŠ¨ä½œç©ºé—´è¿ç»­æˆ–å¤æ‚çš„ç¯å¢ƒã€‚</p>

  <h3>ä¸€ã€ç­–ç•¥æ¢¯åº¦ç›®æ ‡</h3>
  <p>æœ€å¤§åŒ–æœŸæœ›ç´¯è®¡å¥–åŠ±ï¼š</p>
  <p>$$
  J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]
  $$</p>
  <ul>
    <li><code>Î¸</code>ï¼šç­–ç•¥å‚æ•°</li>
    <li><code>Ï„</code>ï¼šä»ç­–ç•¥ç”Ÿæˆçš„è½¨è¿¹ï¼ˆstate-actionåºåˆ—ï¼‰</li>
    <li><code>R(Ï„)</code>ï¼šè½¨è¿¹æ€»å¥–åŠ±</li>
  </ul>

  <h3>äºŒã€ç­–ç•¥æ¢¯åº¦å®šç†</h3>
  <p>ç­–ç•¥æ¢¯åº¦å…¬å¼ï¼š</p>
  <p>$$
  \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [ \nabla_\theta \log \pi_\theta(a|s) \, Q^\pi(s,a) ]
  $$</p>
  <ul>
    <li><code>âˆ‡Î¸ log Ï€Î¸(a|s)</code>ï¼šè°ƒæ•´ç­–ç•¥å‚æ•° Î¸ æ”¹å˜åŠ¨ä½œæ¦‚ç‡çš„æ–¹å‘</li>
    <li><code>QÏ€(s,a)</code>ï¼šåŠ¨ä½œä»·å€¼ï¼Œç”¨æ¥è¡¡é‡åŠ¨ä½œå¥½å</li>
  </ul>

  <h3>ä¸‰ã€æ ¸å¿ƒç›´è§‰</h3>
  <ul>
    <li>å¦‚æœåŠ¨ä½œ a çš„å›æŠ¥ Q é«˜ â†’ å¢åŠ  Ï€(a|s) çš„æ¦‚ç‡</li>
    <li>å¦‚æœåŠ¨ä½œ a çš„å›æŠ¥ Q ä½ â†’ å‡å°‘ Ï€(a|s) çš„æ¦‚ç‡</li>
  </ul>
  <p>æ•°å­¦æ¥æºï¼šæ›´æ–°å…¬å¼ <code>Î”Î¸ âˆ âˆ‡Î¸ log Ï€(a|s) * Q(s,a)</code>ï¼ŒQ(s,a) å¤§ â†’ å¼ºçƒˆå¢åŠ æ¦‚ç‡ï¼ŒQ(s,a) å° â†’ å¯èƒ½å‡å°‘æ¦‚ç‡ã€‚</p>
  <p>æ¦‚ç‡è§’åº¦ï¼šlog Ï€ æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•è°ƒèŠ‚ Î¸ï¼Œä½¿é€‰æ‹©åŠ¨ä½œæ¦‚ç‡å¢å¤§/å‡å°ï¼›ä¹˜ä¸Š Q(s,a) è®©è°ƒæ•´å’ŒåŠ¨ä½œå¥½åæŒ‚é’©ã€‚</p>

  <h3>å››ã€REINFORCE ç®—æ³•ï¼ˆMonte Carlo Policy Gradientï¼‰</h3>
  <pre><code class="language-python">
for episode in range(num_episodes):
    states, actions, rewards = run_episode(env, policy)
    G = 0
    returns = []
    # è®¡ç®—å›åˆç´¯è®¡å›æŠ¥
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    
    # æ›´æ–°ç­–ç•¥
    for s, a, Gt in zip(states, actions, returns):
        theta += alpha * grad_log_pi(s, a) * Gt
  </code></pre>


  <h3>äº”ã€ç­–ç•¥æ¢¯åº¦è®­ç»ƒå®Œæˆåˆ¤æ–­</h3>
  <ul>
    <li>ç­–ç•¥ç¨³å®šæ€§ï¼šè¿ç»­è‹¥å¹²å›åˆç­–ç•¥é€‰æ‹©åŠ¨ä½œæ¦‚ç‡å˜åŒ–ä¸å¤§</li>
    <li>å¹³å‡ç´¯è®¡å¥–åŠ±æ”¶æ•›ï¼šæµ‹è¯•ç¯å¢ƒä¸‹å¹³å‡å¥–åŠ±è¾¾åˆ°ç›®æ ‡</li>
  </ul>

  
</section>
<!-- ç¬¬åäºŒç«  -->
<section id="chapter12" class="chapter">
  <h2>ç¬¬åäºŒç« ï¼šActor-Critic (AC) æ–¹æ³•</h2>

  <p>Actor-Critic æ–¹æ³•æ˜¯ç­–ç•¥æ¢¯åº¦å®¶æ—çš„ä¸€ç§æ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆä»·å€¼å‡½æ•°ï¼ˆCriticï¼‰æ¥é™ä½ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚</p>

  <h3>ä¸€ã€REINFORCE ä¸ Actor-Critic å¯¹æ¯”</h3>
  <table>
    <tr>
      <th>æ–¹æ³•</th>
      <th>å›æŠ¥ä¼°è®¡æ–¹å¼</th>
      <th>ç­–ç•¥æ›´æ–°ä¾æ®</th>
      <th>æ›´æ–°é¢‘ç‡</th>
      <th>æ–¹å·®</th>
    </tr>
    <tr>
      <td>REINFORCE</td>
      <td>å®Œæ•´å›åˆç´¯è®¡å›æŠ¥ G_t</td>
      <td>G_t * âˆ‡ log Ï€(a|s)</td>
      <td>å›åˆç»“æŸ</td>
      <td>é«˜</td>
    </tr>
    <tr>
      <td>Actor-Critic (AC)</td>
      <td>TD è¯¯å·® Î´_t = r + Î³ V(s') - V(s)</td>
      <td>Î´_t * âˆ‡ log Ï€(a|s)</td>
      <td>æ¯ä¸€æ­¥ï¼ˆåœ¨çº¿ï¼‰</td>
      <td>ä¸­ä½</td>
    </tr>
  </table>
  <p>ç›´è§‚ç†è§£ï¼šREINFORCE ç­‰æ•´å›åˆç»“æŸæ‰æ‰“åˆ†ï¼Œè€Œ AC æ¯ä¸€æ­¥éƒ½æœ‰ Critic æŒ‡å¯¼ Actorï¼Œå› æ­¤è®­ç»ƒæ›´å¿«æ›´ç¨³ã€‚</p>

  <h3>äºŒã€æ ¸å¿ƒæ€æƒ³</h3>
  <ul>
    <li><b>Actor</b>ï¼šç”ŸæˆåŠ¨ä½œç­–ç•¥ Ï€(a|s;Î¸_actor)ï¼Œç›´æ¥è¾“å‡ºåŠ¨ä½œæ¦‚ç‡</li>
    <li><b>Critic</b>ï¼šè¯„ä¼°çŠ¶æ€ä»·å€¼ V(s;Î¸_critic)ï¼Œæä¾› TD è¯¯å·® Î´_t æ¥æŒ‡å¯¼ Actor æ›´æ–°</li>
  </ul>
  <p>ç›´è§‚ç†è§£ï¼šActor æ˜¯â€œå†³ç­–è€…â€ï¼ŒCritic æ˜¯â€œè£åˆ¤â€ï¼Œè£åˆ¤å‘Šè¯‰å†³ç­–è€…åŠ¨ä½œå¥½åï¼ŒActor æ ¹æ®åé¦ˆè°ƒæ•´ç­–ç•¥ã€‚</p>

  <h3>ä¸‰ã€æ ¸å¿ƒå…¬å¼</h3>
  <ul>
    <li><b>TD è¯¯å·®ï¼ˆCriticï¼‰</b>ï¼š
      <p>$$
      \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
      $$</p>
    </li>
    <li><b>Actor æ›´æ–°</b>ï¼š
      <p>$$
      \theta_\text{Actor} \gets \theta_\text{Actor} + \alpha_\text{Actor} \nabla_\theta \log \pi_\theta(a|s) \, \delta_t
      $$</p>
    </li>
    <li><b>Critic æ›´æ–°</b>ï¼š
      <p>$$
      \theta_\text{Critic} \gets \theta_\text{Critic} + \alpha_\text{Critic} \delta_t \nabla_\theta V_\theta(s)
      $$</p>
    </li>
  </ul>

  <h3>å››ã€è®­ç»ƒæµç¨‹</h3>
  <ol>
    <li>åˆå§‹åŒ– Actor å’Œ Critic ç½‘ç»œ</li>
    <li>ä¸ç¯å¢ƒäº¤äº’ï¼Œé€‰æ‹©åŠ¨ä½œ a_t âˆ¼ Ï€(a|s_t)</li>
    <li>æ‰§è¡ŒåŠ¨ä½œï¼Œè·å¾— r_{t+1}, s_{t+1}</li>
    <li>Critic è®¡ç®— TD è¯¯å·® Î´_t</li>
    <li>Actor ä½¿ç”¨ Î´_t æ›´æ–°ç­–ç•¥</li>
    <li>Critic ä½¿ç”¨ Î´_t æ›´æ–°ä»·å€¼å‡½æ•°</li>
    <li>é‡å¤ä»¥ä¸Šæ­¥éª¤ç›´åˆ°è®­ç»ƒå®Œæˆ</li>
  </ol>

  <h3>äº”ã€Actor-Critic ç¤ºä¾‹ä»£ç </h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

class ActorNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(ActorNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.softmax(self.fc3(x), dim=-1)

class CriticNetwork(nn.Module):
    def __init__(self, state_dim):
        super(CriticNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 1)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class ActorCritic:
    def __init__(self, state_dim, action_dim, gamma=0.99, actor_lr=1e-3, critic_lr=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        
        self.actor = ActorNetwork(state_dim, action_dim)
        self.critic = CriticNetwork(state_dim)
        
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)
    
    def select_action(self, state):
        """é‡‡æ ·åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            action_probs = self.actor(state_tensor)
        dist = Categorical(action_probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)
    
    def train_step(self, state, action, reward, next_state, done, log_prob):
        """å•æ­¥è®­ç»ƒ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
        
        # Criticè¯„ä¼°
        with torch.no_grad():
            value = self.critic(state_tensor).squeeze()
            next_value = self.critic(next_state_tensor).squeeze()
            if done:
                target_value = reward
            else:
                target_value = reward + self.gamma * next_value
        
        # è®¡ç®—TDè¯¯å·®ï¼ˆä¼˜åŠ¿å‡½æ•°ï¼‰
        advantage = target_value - value
        
        # æ›´æ–°Actorï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰
        actor_loss = -log_prob * advantage.detach()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # æ›´æ–°Criticï¼ˆå€¼å‡½æ•°ï¼‰
        critic_loss = (advantage ** 2).mean()
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        return actor_loss.item(), critic_loss.item()
    
    def train_episode(self, env):
        """è¿è¡Œä¸€ä¸ªå®Œæ•´å›åˆ"""
        state = env.reset()
        done = False
        episode_reward = 0
        
        while not done:
            action, log_prob = self.select_action(state)
            next_state, reward, done, _ = env.step(action)
            
            self.train_step(state, action, reward, next_state, done, log_prob)
            
            episode_reward += reward
            state = next_state
        
        return episode_reward
    
    def train(self, env, num_episodes=1000):
        """è®­ç»ƒè¿‡ç¨‹"""
        rewards = []
        for episode in range(num_episodes):
            reward = self.train_episode(env)
            rewards.append(reward)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        return rewards
  </code></pre>

  <h3>å…­ã€è®­ç»ƒå®Œæˆåˆ¤æ–­</h3>
  <ul>
    <li>å¹³å‡ç´¯è®¡å¥–åŠ±ç¨³å®šå¹¶æ”¶æ•›</li>
    <li>Actor ç­–ç•¥æ¦‚ç‡å˜åŒ–ä¸å¤§</li>
    <li>Critic TD loss æ”¶æ•›</li>
  </ul>

  <h3>ä¸ƒã€ç›´è§‚ç†è§£æ€»ç»“</h3>
  <ul>
    <li>Actor å†³ç­–ï¼ŒCritic è¯„ä»·</li>
    <li>TD è¯¯å·®æŒ‡å¯¼ Actor æ›´æ–°ï¼Œæ¯” REINFORCE é«˜æ•ˆã€ä½æ–¹å·®</li>
    <li>å¯åœ¨çº¿é€æ­¥æ›´æ–°ç­–ç•¥ï¼Œä¸å¿…ç­‰æ•´å›åˆç»“æŸ</li>
  </ul>

</section>
<!-- ç¬¬åä¸‰ç«  -->
<section id="chapter13" class="chapter">
  <h2>ç¬¬åä¸‰ç« ï¼šAdvantage Actor-Critic (A2C) ä¸ Asynchronous Advantage Actor-Critic (A3C)</h2>

  <p>A2C å’Œ A3C æ˜¯ Actor-Critic ç³»åˆ—æ–¹æ³•çš„æ”¹è¿›ï¼Œæ ¸å¿ƒéƒ½æ˜¯ä½¿ç”¨ä¼˜åŠ¿å‡½æ•° <code>A(s,a) = Q(s,a) - V(s)</code> æ¥é™ä½ç­–ç•¥æ¢¯åº¦æ–¹å·®ï¼Œå¹¶å¼•å…¥å¤šçº¿ç¨‹æœºåˆ¶æé«˜è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚</p>

  <h3>ä¸€ã€Advantage Actor-Critic (A2C)</h3>
  <p><b>æ ¸å¿ƒå…¬å¼ï¼š</b></p>
  <ul>
    <li>Actor æ›´æ–°ï¼š
      <p>$$
      \theta_\text{Actor} \gets \theta_\text{Actor} + \alpha_\text{Actor} \nabla_\theta \log \pi_\theta(a|s) \, A(s,a)
      $$</p>
    </li>
    <li>Critic æ›´æ–°ï¼š
      <p>$$
      \theta_\text{Critic} \gets \theta_\text{Critic} + \alpha_\text{Critic} \delta_t \nabla_\theta V_\theta(s)
      $$</p>
    </li>
  </ul>

  <p><b>å¤šçº¿ç¨‹æœºåˆ¶ï¼š</b>åŒæ­¥å¤šç¯å¢ƒè®­ç»ƒã€‚å¤šä¸ªç¯å¢ƒåŒæ—¶é‡‡æ ·çŠ¶æ€-åŠ¨ä½œåºåˆ—ï¼Œå°†æ¢¯åº¦ç´¯ç§¯ååŒæ­¥æ›´æ–°å…¨å±€ç½‘ç»œã€‚</p>
  <ul>
    <li>ä¸å¼‚æ­¥ï¼Œæ‰€æœ‰ç¯å¢ƒåœ¨åŒä¸€æ—¶åˆ»å®Œæˆä¸€ä¸ªæ‰¹é‡æ›´æ–°</li>
    <li>å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œé‡‡æ ·</li>
    <li>ä¼˜ç‚¹ï¼šç¨³å®šã€æ–¹å·®ä½ï¼Œè®­ç»ƒæ•ˆç‡é«˜</li>
  </ul>

  <h3>A2C ç¤ºä¾‹ä»£ç </h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
from multiprocessing import Process, Queue

class A2CNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(A2CNetwork, self).__init__()
        self.fc = nn.Linear(state_dim, 256)
        
        # Actorå¤´
        self.actor_head = nn.Sequential(
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )
        
        # Criticå¤´
        self.critic_head = nn.Sequential(
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        shared = self.fc(x)
        action_probs = self.actor_head(shared)
        value = self.critic_head(shared)
        return action_probs, value

class A2C:
    def __init__(self, state_dim, action_dim, num_envs=8, gamma=0.99, lr=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.num_envs = num_envs
        self.gamma = gamma
        
        self.network = A2CNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.network.to(self.device)
    
    def compute_gae(self, rewards, values, dones, gamma=0.99, gae_lambda=0.95):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGeneralized Advantage Estimationï¼‰"""
        advantages = []
        gae = 0
        next_value = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_nonterminal = 1.0 - dones[t]
                next_value_t = next_value
            else:
                next_nonterminal = 1.0 - dones[t]
                next_value_t = values[t + 1]
            
            delta = rewards[t] + gamma * next_value_t * next_nonterminal - values[t]
            gae = delta + gamma * gae_lambda * next_nonterminal * gae
            advantages.insert(0, gae)
        
        return np.array(advantages)
    
    def train_iteration(self, envs, max_steps=2048):
        """æ‰§è¡Œä¸€æ¬¡è¿­ä»£ï¼ˆå¤šç¯å¢ƒåŒæ­¥é‡‡æ ·ï¼‰"""
        batch_states = []
        batch_actions = []
        batch_rewards = []
        batch_dones = []
        batch_values = []
        batch_log_probs = []
        
        states = [env.reset() for env in envs]
        
        for step in range(max_steps):
            state_tensors = torch.FloatTensor(np.array(states)).to(self.device)
            
            with torch.no_grad():
                action_probs, values = self.network(state_tensors)
            
            # ä»æ¯ä¸ªç¯å¢ƒé‡‡æ ·åŠ¨ä½œ
            actions = []
            log_probs = []
            for i, (probs, val) in enumerate(zip(action_probs, values)):
                dist = Categorical(probs)
                action = dist.sample()
                log_prob = dist.log_prob(action)
                actions.append(action.item())
                log_probs.append(log_prob)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            rewards = []
            dones = []
            next_states = []
            for i, env in enumerate(envs):
                next_state, reward, done, _ = env.step(actions[i])
                rewards.append(reward)
                dones.append(done)
                if done:
                    next_state = env.reset()
                next_states.append(next_state)
            
            batch_states.extend(states)
            batch_actions.extend(actions)
            batch_rewards.extend(rewards)
            batch_dones.extend(dones)
            batch_values.extend([v.item() for v in values])
            batch_log_probs.extend(log_probs)
            
            states = next_states
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        advantages = self.compute_gae(batch_rewards, batch_values, batch_dones, self.gamma)
        returns = advantages + np.array(batch_values)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # æ›´æ–°ç½‘ç»œ
        batch_size = 32
        for epoch in range(3):
            indices = np.random.permutation(len(batch_states))
            for i in range(0, len(batch_states), batch_size):
                batch_indices = indices[i:i+batch_size]
                
                states_batch = torch.FloatTensor(np.array(batch_states)[batch_indices]).to(self.device)
                actions_batch = torch.LongTensor(np.array(batch_actions)[batch_indices]).to(self.device)
                advantages_batch = torch.FloatTensor(advantages[batch_indices]).to(self.device)
                returns_batch = torch.FloatTensor(returns[batch_indices]).to(self.device)
                
                action_probs, values = self.network(states_batch)
                
                # è®¡ç®—ç­–ç•¥æŸå¤±
                dist = Categorical(action_probs)
                log_probs = dist.log_prob(actions_batch)
                policy_loss = -(log_probs * advantages_batch).mean()
                
                # è®¡ç®—ä»·å€¼æŸå¤±
                value_loss = ((values.squeeze() - returns_batch) ** 2).mean()
                
                # æ€»æŸå¤±
                loss = policy_loss + 0.5 * value_loss - 0.01 * dist.entropy().mean()
                
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
                self.optimizer.step()
    
    def train(self, env_fn, num_iterations=1000):
        """è®­ç»ƒè¿‡ç¨‹"""
        envs = [env_fn() for _ in range(self.num_envs)]
        
        for iteration in range(num_iterations):
            self.train_iteration(envs)
            
            if (iteration + 1) % 100 == 0:
                print(f"Iteration {iteration+1} completed")
  </code></pre>

  <h3>äºŒã€Asynchronous Advantage Actor-Critic (A3C)</h3>
  <p><b>æ ¸å¿ƒå…¬å¼ï¼š</b>ä¸ A2C ç›¸åŒï¼Œä½¿ç”¨ä¼˜åŠ¿å‡½æ•°æ›´æ–° Actorï¼ŒTD è¯¯å·®æ›´æ–° Criticã€‚</p>

  <p><b>å¤šçº¿ç¨‹æœºåˆ¶ï¼š</b>å¼‚æ­¥å¤šçº¿ç¨‹è®­ç»ƒï¼Œæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹é‡‡æ ·ç¯å¢ƒå¹¶ç»´æŠ¤å±€éƒ¨ Actor-Critic å‰¯æœ¬ï¼Œè®¡ç®—æ¢¯åº¦åç«‹å³å¼‚æ­¥æ›´æ–°å…¨å±€ç½‘ç»œï¼š</p>
  <ul>
    <li>å¢å¼ºæ¢ç´¢æ€§ï¼šå„çº¿ç¨‹é‡‡æ ·çš„çŠ¶æ€åˆ†å¸ƒä¸åŒ</li>
    <li>é¿å…å•çº¿ç¨‹é™·å…¥å±€éƒ¨æœ€ä¼˜</li>
    <li>è®­ç»ƒæ•ˆç‡æ›´é«˜ï¼Œæ”¶æ•›ç¨³å®šæ€§æ›´å¥½</li>
    <li>ç¼ºç‚¹ï¼šæ¢¯åº¦éšæœºæ€§è¾ƒå¤§ï¼Œéœ€è¦è¾ƒå°å­¦ä¹ ç‡</li>
  </ul>

  <h3>A3C ç¤ºä¾‹ä»£ç </h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import threading
from multiprocessing import Process, Queue
import time

class A3CNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(A3CNetwork, self).__init__()
        self.fc = nn.Linear(state_dim, 256)
        
        self.actor_head = nn.Sequential(
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )
        
        self.critic_head = nn.Sequential(
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        shared = self.fc(x)
        return self.actor_head(shared), self.critic_head(shared)

class A3CWorker(threading.Thread):
    def __init__(self, global_network, global_optimizer, env, worker_id, max_steps=100):
        super(A3CWorker, self).__init__()
        self.global_network = global_network
        self.global_optimizer = global_optimizer
        self.env = env
        self.worker_id = worker_id
        self.max_steps = max_steps
        self.daemon = True
        
        # æœ¬åœ°ç½‘ç»œå‰¯æœ¬
        self.local_network = A3CNetwork(
            self.global_network.fc.in_features,
            self.global_network.actor_head[-2].out_features
        )
        self.local_network.load_state_dict(self.global_network.state_dict())
        self.local_optimizer = optim.Adam(self.local_network.parameters(), lr=1e-3)
    
    def run(self):
        """å¼‚æ­¥è®­ç»ƒå¾ªç¯"""
        state = self.env.reset()
        
        while True:
            # é‡‡é›†æœ¬åœ°è½¨è¿¹
            batch_states = []
            batch_actions = []
            batch_rewards = []
            batch_values = []
            
            for step in range(self.max_steps):
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                
                with torch.no_grad():
                    action_probs, value = self.local_network(state_tensor)
                
                # é‡‡æ ·åŠ¨ä½œ
                dist = Categorical(action_probs.squeeze())
                action = dist.sample()
                
                next_state, reward, done, _ = self.env.step(action.item())
                
                batch_states.append(state)
                batch_actions.append(action)
                batch_rewards.append(reward)
                batch_values.append(value.item())
                
                state = next_state if not done else self.env.reset()
            
            # è®¡ç®—TDç›®æ ‡
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            with torch.no_grad():
                _, bootstrap_value = self.local_network(state_tensor)
            bootstrap_value = bootstrap_value.item()
            
            # åå‘è®¡ç®—ç´¯è®¡å¥–åŠ±
            returns = []
            G = bootstrap_value
            for reward in reversed(batch_rewards):
                G = reward + 0.99 * G
                returns.insert(0, G)
            returns = np.array(returns)
            
            # æ ‡å‡†åŒ–ä¼˜åŠ¿
            advantages = returns - np.array(batch_values)
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # æ›´æ–°æœ¬åœ°ç½‘ç»œ
            for state, action, advantage, ret in zip(batch_states, batch_actions, advantages, returns):
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                action_probs, value = self.local_network(state_tensor)
                
                # è®¡ç®—æŸå¤±
                dist = Categorical(action_probs)
                log_prob = dist.log_prob(action)
                
                policy_loss = -(log_prob * advantage)
                value_loss = ((value.squeeze() - ret) ** 2)
                entropy_bonus = -0.01 * dist.entropy()
                
                loss = policy_loss + 0.5 * value_loss + entropy_bonus
                
                self.local_optimizer.zero_grad()
                loss.backward()
                self.local_optimizer.step()
            
            # å¼‚æ­¥æ›´æ–°å…¨å±€ç½‘ç»œ
            self._async_update_global()
            
            # åŒæ­¥æœ¬åœ°ç½‘ç»œ
            self.local_network.load_state_dict(self.global_network.state_dict())
    
    def _async_update_global(self):
        """å¼‚æ­¥æ›´æ–°å…¨å±€ç½‘ç»œ"""
        # å°†æœ¬åœ°æ¢¯åº¦ç´¯ç§¯åˆ°å…¨å±€ç½‘ç»œ
        for local_param, global_param in zip(
            self.local_network.parameters(),
            self.global_network.parameters()
        ):
            if global_param.grad is not None:
                global_param.grad.data += local_param.grad.data

class A3C:
    def __init__(self, state_dim, action_dim, num_workers=8):
        self.global_network = A3CNetwork(state_dim, action_dim)
        self.global_optimizer = optim.Adam(self.global_network.parameters(), lr=1e-3)
        self.num_workers = num_workers
    
    def train(self, env_fn, num_iterations=10000):
        """å¯åŠ¨å¤šä¸ªå¼‚æ­¥çº¿ç¨‹è®­ç»ƒ"""
        workers = []
        for i in range(self.num_workers):
            worker = A3CWorker(
                self.global_network,
                self.global_optimizer,
                env_fn(),
                worker_id=i
            )
            worker.start()
            workers.append(worker)
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for worker in workers:
            worker.join()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # env_fn = lambda: gym.make('CartPole-v1')
    # a3c = A3C(state_dim=4, action_dim=2, num_workers=8)
    # a3c.train(env_fn, num_iterations=10000)
    pass
  </code></pre>

  <h3>ä¸‰ã€è®­ç»ƒå®Œæˆåˆ¤æ–­</h3>
  <ul>
    <li>å¹³å‡ç´¯è®¡å¥–åŠ±ç¨³å®šå¹¶æ”¶æ•›</li>
    <li>Actor ç­–ç•¥æ¦‚ç‡å˜åŒ–ä¸å¤§</li>
    <li>Critic TD loss æ”¶æ•›</li>
    <li>å¤šçº¿ç¨‹è®­ç»ƒä¸‹ï¼Œæ¢¯åº¦æ›´æ–°ç¨³å®šï¼Œå¥–åŠ±æ›²çº¿å¹³æ»‘</li>
  </ul>

  <h3>å››ã€A2C ä¸ A3C å¯¹æ¯”</h3>
  <table>
    <tr><th>ç‰¹æ€§</th><th>A2C</th><th>A3C</th></tr>
    <tr><td>å¤šçº¿ç¨‹ç±»å‹</td><td>åŒæ­¥å¤šç¯å¢ƒ</td><td>å¼‚æ­¥å¤šçº¿ç¨‹</td></tr>
    <tr><td>æ¢¯åº¦æ›´æ–°</td><td>æ‰€æœ‰ç¯å¢ƒæ¢¯åº¦ç´¯ç§¯ååŒæ­¥æ›´æ–°</td><td>æ¯ä¸ªçº¿ç¨‹è®¡ç®—æ¢¯åº¦åç«‹å³å¼‚æ­¥æ›´æ–°å…¨å±€ç½‘ç»œ</td></tr>
    <tr><td>æ¢ç´¢æ€§</td><td>è¾ƒå¥½</td><td>æ›´é«˜</td></tr>
    <tr><td>æ”¶æ•›ç¨³å®šæ€§</td><td>é«˜</td><td>é«˜ï¼Œä½†éšæœºæ€§å¤§</td></tr>
    <tr><td>å®ç°éš¾åº¦</td><td>ä¸­</td><td>é«˜</td></tr>
  </table>

</section>

  </main>
</div>

<footer>
  <p>Â© 2025 <span>Haoyun Tang</span> | Reinforcement Learning Notes</p>
</footer>

<!-- ===== åŠ¨æ€ç²’å­ç‰¹æ•ˆ ===== -->
<script>
const canvas = document.getElementById("trailCanvas");
const ctx = canvas.getContext("2d");
let particles = [];
function resize() {
  canvas.width = window.innerWidth;
  canvas.height = window.innerHeight;
}
window.addEventListener("resize", resize);
resize();
document.addEventListener("mousemove", e => {
  for (let i = 0; i < 2; i++) {
    particles.push({x: e.clientX, y: e.clientY, alpha: 1, r: Math.random()*3+1});
  }
});
function animate() {
  ctx.fillStyle = "rgba(0,0,0,0.2)";
  ctx.fillRect(0, 0, canvas.width, canvas.height);
  particles.forEach(p => {
    p.y -= 0.3;
    p.alpha -= 0.01;
    ctx.beginPath();
    ctx.arc(p.x, p.y, p.r, 0, Math.PI*2);
    ctx.fillStyle = `rgba(0,188,212,${p.alpha})`;
    ctx.fill();
  });
  particles = particles.filter(p => p.alpha > 0);
  requestAnimationFrame(animate);
}
animate();
</script>

<!-- ===== ä»£ç é«˜äº®åˆå§‹åŒ– ===== -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  // è‡ªåŠ¨æ·»åŠ è¡Œå·ç±»å
  document.querySelectorAll('pre[class*="language-"]').forEach(pre => {
    if (!pre.classList.contains('line-numbers')) {
      pre.classList.add('line-numbers');
    }
  });
  
  // è§¦å‘ Prism é«˜äº®
  if (window.Prism) {
    Prism.highlightAll();
  }
});

// é¡µé¢åŠ è½½åé‡æ–°é«˜äº®
window.addEventListener('load', function() {
  if (window.Prism) {
    Prism.highlightAll();
  }
});
</script>

<!-- ===== å¯¼èˆªè‡ªåŠ¨é«˜äº® ===== -->
<script>
const navLinks = document.querySelectorAll("nav a");
const asideLinks = document.querySelectorAll("aside a");
const allLinks = [...navLinks, ...asideLinks];

window.addEventListener("scroll", () => {
  let fromTop = window.scrollY + 150;
  
  allLinks.forEach(link => {
    const section = document.querySelector(link.getAttribute("href"));
    if (section && section.offsetTop <= fromTop && section.offsetTop + section.offsetHeight > fromTop) {
      link.classList.add("active");
    } else {
      link.classList.remove("active");
    }
  });
});
</script>
</body>
</html>
