<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>å¼ºåŒ–å­¦ä¹ ç¬”è®° | Yun</title>

<!-- æ•°å­¦å…¬å¼æ¸²æŸ“ -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script id="MathJax-script" async
 src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Prism.js æ ¸å¿ƒ + ä¸»é¢˜ + è¯­è¨€ -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-dracula.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
<!-- è¯­è¨€æ”¯æŒ -->
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-javascript.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-typescript.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-bash.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-json.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-css.min.js"></script>
<!-- è¡Œå·æ’ä»¶ -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>


<style>
/* ===== å…¨å±€æ ·å¼ ===== */
body {
  margin: 0;
  font-family: "Segoe UI", "Roboto", -apple-system, sans-serif;
  background: linear-gradient(135deg, #0f1419 0%, #1a1f2e 100%);
  color: #c8cdd3;
  line-height: 1.8;
  scroll-behavior: smooth;
}

/* èƒŒæ™¯ç²’å­ */
#trailCanvas {
  position: fixed;
  top: 0; left: 0;
  width: 100%; height: 100%;
  z-index: 0;
  pointer-events: none;
}

/* Header */
header {
  position: sticky;
  top: 0;
  z-index: 100;
  text-align: center;
  background: rgba(15, 20, 25, 0.95);
  padding: 25px 20px;
  backdrop-filter: blur(8px);
  border-bottom: 1px solid rgba(86, 171, 145, 0.2);
  box-shadow: 0 4px 12px rgba(0,0,0,0.4);
}
header h1 {
  font-size: 2.2rem;
  color: #56ab91;
  margin: 0;
  font-weight: 700;
  letter-spacing: 1px;
}

/* å¯¼èˆªæ  */
nav ul {
  list-style: none;
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  padding: 0;
  margin: 15px 0 0;
  gap: 8px;
}
nav li { margin: 0; }
nav a {
  color: #b0b5bb;
  text-decoration: none;
  font-weight: 600;
  font-size: 0.95rem;
  padding: 6px 14px;
  border-radius: 6px;
  transition: all 0.3s ease;
  background: rgba(86, 171, 145, 0.08);
  border: 1px solid rgba(86, 171, 145, 0.15);
}
nav a:hover, nav a.active {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.15);
  border-color: rgba(86, 171, 145, 0.4);
  box-shadow: 0 0 12px rgba(86, 171, 145, 0.2);
}

/* å®¹å™¨å¸ƒå±€ */
.container {
  display: flex;
  position: relative;
  z-index: 5;
  max-width: 1400px;
  margin: 30px auto;
  gap: 30px;
  padding: 0 20px;
}

/* ä¾§è¾¹æ  */
aside {
  position: sticky;
  top: 120px;
  width: 250px;
  height: fit-content;
  background: rgba(25, 30, 40, 0.8);
  border: 1px solid rgba(86, 171, 145, 0.15);
  border-radius: 10px;
  padding: 20px;
  backdrop-filter: blur(10px);
  flex-shrink: 0;
}

aside h3 {
  color: #56ab91;
  font-size: 1.1rem;
  margin: 0 0 15px 0;
  padding-bottom: 10px;
  border-bottom: 2px solid rgba(86, 171, 145, 0.3);
}

aside ul {
  list-style: none;
  padding: 0;
  margin: 0;
}

aside li {
  margin-bottom: 8px;
}

aside a {
  color: #b0b5bb;
  text-decoration: none;
  font-size: 0.9rem;
  display: block;
  padding: 8px 12px;
  border-radius: 6px;
  transition: all 0.2s ease;
  border-left: 3px solid transparent;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

aside a:hover, aside a.active {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
  border-left-color: #56ab91;
  padding-left: 15px;
}

/* å†…å®¹åŒº */
main {
  flex: 1;
  min-width: 0;
}

.chapter {
  background: rgba(25, 30, 40, 0.7);
  border: 1px solid rgba(86, 171, 145, 0.1);
  border-radius: 12px;
  padding: 35px;
  margin-bottom: 35px;
  transition: all 0.3s ease;
}

.chapter:hover {
  border-color: rgba(86, 171, 145, 0.25);
  box-shadow: 0 8px 24px rgba(86, 171, 145, 0.08);
  transform: translateY(-2px);
}

.chapter h2 {
  color: #56ab91;
  font-size: 1.8rem;
  margin: 0 0 20px 0;
  padding-bottom: 12px;
  border-bottom: 2px solid rgba(86, 171, 145, 0.2);
  font-weight: 700;
}

.chapter p {
  margin: 15px 0;
  color: #c8cdd3;
}

.chapter ul, .chapter ol {
  margin: 15px 0;
  padding-left: 30px;
}

.chapter li {
  margin-bottom: 8px;
  color: #c8cdd3;
}

/* ä»£ç å—æ ·å¼ */
pre[class*="language-"] {
  background: linear-gradient(135deg, #0a0e15 0%, #141a25 100%) !important;
  padding: 20px !important;
  border-radius: 10px !important;
  overflow-x: auto !important;
  font-size: 0.92rem !important;
  line-height: 1.6 !important;
  border: 1px solid rgba(86, 171, 145, 0.2) !important;
  position: relative;
  font-family: 'Fira Code', 'Courier New', monospace !important;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3) !important;
}

pre[class*="language-"]::before {
  content: attr(class);
  position: absolute;
  top: 8px;
  right: 12px;
  font-size: 0.7rem;
  color: rgba(86, 171, 145, 0.6);
  font-weight: bold;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

/* ä»£ç è¡Œå· */
pre.line-numbers {
  padding-left: 60px !important;
}

.line-numbers-rows {
  background: rgba(86, 171, 145, 0.08) !important;
  border-right: 2px solid rgba(86, 171, 145, 0.2) !important;
}

.line-numbers-rows > span:before {
  color: rgba(86, 171, 145, 0.6) !important;
  font-weight: bold;
}

pre code {
  color: inherit !important;
  background: none !important;
  font-family: inherit !important;
  font-size: inherit !important;
}

/* ä»£ç å—å†…å„å…ƒç´ é«˜äº® */
.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
  color: #6272a4;
}

.token.punctuation {
  color: #f8f8f2;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
  color: #ff79c6;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
  color: #50fa7b;
}

.token.operator,
.token.entity,
.token.url {
  color: #8be9fd;
}

.token.atrule,
.token.attr-value,
.token.keyword {
  color: #ff79c6;
}

.token.function,
.token.class-name {
  color: #8be9fd;
}

.token.regex,
.token.important,
.token.variable {
  color: #f1fa8c;
}

/* ä»£ç å—é¼ æ ‡æ‚¬åœæ•ˆæœ */
pre[class*="language-"]:hover {
  box-shadow: 
    0 12px 48px rgba(0, 188, 212, 0.25),
    inset 0 1px 0 rgba(255,255,255,0.15) !important;
  transform: translateY(-2px);
  transition: all 0.3s ease;
}

/* æ»šåŠ¨æ¡ç¾åŒ– */
pre[class*="language-"]::-webkit-scrollbar {
  height: 10px;
  background: rgba(0, 188, 212, 0.1);
}

pre[class*="language-"]::-webkit-scrollbar-thumb {
  background: rgba(0, 188, 212, 0.5);
  border-radius: 5px;
  transition: background 0.3s;
}

pre[class*="language-"]::-webkit-scrollbar-thumb:hover {
  background: rgba(0, 188, 212, 0.8);
}


/* è¡¨æ ¼ä¼˜åŒ– */
table {
  width: 100%;
  border-collapse: collapse;
  margin-top: 15px;
  background: rgba(255,255,255,0.05);
  border-radius: 8px;
  overflow: hidden;
}
table th, table td {
  padding: 10px 15px;
  border-bottom: 1px solid rgba(255,255,255,0.1);
}
table th {
  background: rgba(0,188,212,0.2);
  color: #00e5ff;
}

/* Footer */
footer {
  text-align: center;
  padding: 15px;
  margin: 40px 0 10px;
  font-size: 0.9rem;
  color: #aaa;
  background: rgba(20,20,20,0.7);
  border-radius: 8px;
}
footer span {
  color: #00bcd4;
}

/* ===== ç»Ÿä¸€ç« èŠ‚æ ·å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰ ===== */
.chapter h3 {
  padding-bottom: 8px;
  margin-top: 20px;
  border-bottom: 1px solid rgba(255,255,255,0.1);
  font-size: 1.1rem;
}

.chapter strong {
  font-weight: 600;
}

.chapter code {
  background: rgba(255,255,255,0.05);
  padding: 2px 6px;
  border-radius: 3px;
  font-size: 0.9em;
}

.chapter li {
  margin-bottom: 10px;
  line-height: 1.8;
  transition: all 0.2s ease;
}

.chapter li:hover {
  transform: translateX(3px);
}

.chapter table {
  border-collapse: collapse;
  width: 100%;
  margin: 15px 0;
}

.chapter table td, .chapter table th {
  border: 1px solid rgba(255,255,255,0.1);
  padding: 10px;
  text-align: left;
}

.chapter table th {
  background: rgba(255,255,255,0.05);
  font-weight: 600;
}

/* ===== ç¬¬ä¸€ç« ï¼ˆåŸºæœ¬æ¦‚å¿µï¼‰- ç¿ ç»¿ ===== */
#chapter1 h3, #chapter1 dt {
  color: #56ab91;
}

#chapter1 strong {
  color: #56ab91;
}

#chapter1 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬äºŒç« ï¼ˆè´å°”æ›¼æ–¹ç¨‹ï¼‰- ç¿ ç»¿ ===== */
#chapter2 h3, #chapter2 dt {
  color: #56ab91;
}

#chapter2 strong {
  color: #56ab91;
}

#chapter2 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬ä¸‰ç« ï¼ˆè´å°”æ›¼æœ€ä¼˜ï¼‰- ç¿ ç»¿ ===== */
#chapter3 h3, #chapter3 dt {
  color: #56ab91;
}

#chapter3 strong {
  color: #56ab91;
}

#chapter3 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬å››ç« ï¼ˆç­–ç•¥è¿­ä»£ï¼‰- ç¿ ç»¿ ===== */
#chapter4 h3 {
  color: #56ab91;
}

#chapter4 strong {
  color: #56ab91;
}

#chapter4 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬äº”ç« ï¼ˆå€¼è¿­ä»£ï¼‰- ç¿ ç»¿ ===== */
#chapter5 h3 {
  color: #56ab91;
}

#chapter5 strong {
  color: #56ab91;
}

#chapter5 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬å…­ç« ï¼ˆè’™ç‰¹å¡æ´›ï¼‰- ç¿ ç»¿ ===== */
#chapter6 h3 {
  color: #56ab91;
}

#chapter6 strong {
  color: #56ab91;
}

#chapter6 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

#chapter6 dt {
  color: #56ab91;
  font-weight: bold;
  margin-top: 10px;
  margin-bottom: 8px;
}

#chapter6 dd {
  background: rgba(86, 171, 145, 0.05);
  border-left: 3px solid #56ab91;
  padding: 10px 12px;
  margin-left: 0;
  margin-bottom: 12px;
  border-radius: 4px;
  font-size: 0.95rem;
}

/* ===== ç¬¬ä¸ƒç« ï¼ˆæ—¶é—´å·®åˆ†ï¼‰- ç¿ ç»¿ ===== */
#chapter7 h3 {
  color: #56ab91;
}

#chapter7 strong {
  color: #56ab91;
}

#chapter7 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬å…«ç« ï¼ˆQ-learningï¼‰- ç¿ ç»¿ ===== */
#chapter8 h3 {
  color: #56ab91;
}

#chapter8 strong {
  color: #56ab91;
}

#chapter8 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬ä¹ç« ï¼ˆDQNï¼‰- ç¿ ç»¿ ===== */
#chapter9 h3 {
  color: #56ab91;
}

#chapter9 strong {
  color: #56ab91;
}

#chapter9 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬åç« ï¼ˆDDQNï¼‰- ç¿ ç»¿ ===== */
#chapter10 h3 {
  color: #56ab91;
}

#chapter10 strong {
  color: #56ab91;
}

#chapter10 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬åä¸€ç« ï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰- ç¿ ç»¿ ===== */
#chapter11 h3 {
  color: #56ab91;
}

#chapter11 strong {
  color: #56ab91;
}

#chapter11 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬åäºŒç« ï¼ˆActor-Criticï¼‰- ç¿ ç»¿ ===== */
#chapter12 h3 {
  color: #56ab91;
}

#chapter12 strong {
  color: #56ab91;
}

#chapter12 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== ç¬¬åä¸‰ç« ï¼ˆA2C&A3Cï¼‰- ç¿ ç»¿ ===== */
#chapter13 h3 {
  color: #56ab91;
}

#chapter13 strong {
  color: #56ab91;
}

#chapter13 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

#chapter13 strong {
  color: #5dde95;
  font-weight: 600;
}

#chapter13 table td, #chapter13 table th {
  border-color: rgba(93, 222, 149, 0.3);
}

/* ===== å¯¹æ¯”è¡¨æ ¼æ ·å¼å¢å¼º ===== */
table {
  width: 100%;
  border-collapse: collapse;
  margin-top: 15px;
  background: linear-gradient(135deg, rgba(255,255,255,0.02) 0%, rgba(255,255,255,0.05) 100%);
  border-radius: 12px;
  overflow: hidden;
  box-shadow: 0 4px 20px rgba(0,0,0,0.3);
  transition: all 0.3s ease;
}

table:hover {
  box-shadow: 0 8px 30px rgba(0,0,0,0.4);
}

table th, table td {
  padding: 15px;
  border-bottom: 1px solid rgba(255,255,255,0.1);
  text-align: left;
}

table th {
  background: rgba(255, 255, 255, 0.05);
  color: #e0e0e0;
  font-weight: bold;
  font-size: 1.05rem;
}

table tr:hover {
  background: rgba(255, 255, 255, 0.05);
  transition: all 0.2s ease;
}

table td {
  color: #e0e0e0;
}

table tr:last-child th,
table tr:last-child td {
  border-bottom: none;
}

/* Footer */
footer {
  text-align: center;
  padding: 30px 20px;
  background: rgba(15, 20, 25, 0.95);
  border-top: 1px solid rgba(86, 171, 145, 0.15);
  color: #b0b5bb;
  margin-top: 60px;
  font-size: 0.9rem;
}

footer p {
  margin: 10px 0;
}

footer span {
  color: #56ab91;
  font-weight: 600;
}

/* å“åº”å¼è®¾è®¡ */
@media (max-width: 1024px) {
  .container {
    flex-direction: column;
    gap: 20px;
  }
  
  aside {
    position: relative;
    top: auto;
    width: 100%;
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
  }
  
  aside ul {
    grid-column: 1 / -1;
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
  }
  
  aside li {
    margin-bottom: 0;
  }
}

@media (max-width: 768px) {
  header h1 {
    font-size: 1.8rem;
  }
  
  nav ul {
    gap: 4px;
  }
  
  nav a {
    padding: 4px 10px;
    font-size: 0.85rem;
  }
  
  .container {
    padding: 0 10px;
  }
  
  .chapter {
    padding: 20px;
    margin-bottom: 20px;
  }
  
  aside h3 {
    font-size: 1rem;
  }
  
  aside a {
    font-size: 0.85rem;
  }
}

</style>
</head>

<body>
<canvas id="trailCanvas"></canvas>

<header>
  <h1>å¼ºåŒ–å­¦ä¹ ç¬”è®°ï¼ˆåŸºç¡€ç¯‡ï¼‰</h1>
  <nav>
    <ul>
      <li><a href="index.html">é¦–é¡µ</a></li>
      <li><a href="home.html" class="active">åŸºç¡€ç†è®º</a></li>
      <li><a href="advanced.html">è¿›é˜¶ç®—æ³•</a></li>
      <li><a href="MARL.html">å¤šæ™ºèƒ½ä½“å­¦ä¹ </a></li>
      <li><a href="papers.html">è®ºæ–‡ç ”è¯»</a></li>
    </ul>
  </nav>
</header>

<div class="container">
  <aside>
    <h3>ğŸ“š ç›®å½•</h3>
    <ul>
      <li><a href="#chapter0">ç¯å¢ƒé…ç½®</a></li>
      <li><a href="#chapter1">åŸºæœ¬æ¦‚å¿µ</a></li>
      <li><a href="#chapter2">è´å°”æ›¼æ–¹ç¨‹</a></li>
      <li><a href="#chapter3">è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹</a></li>
      <li><a href="#chapter4">ç­–ç•¥è¿­ä»£</a></li>
      <li><a href="#chapter5">å€¼è¿­ä»£</a></li>
      <li><a href="#chapter6">è’™ç‰¹å¡æ´›æ–¹æ³•</a></li>
      <li><a href="#chapter7">æ—¶é—´å·®åˆ†å­¦ä¹ </a></li>
      <li><a href="#chapter8">Q-learning</a></li>
      <li><a href="#chapter9">DQN</a></li>
      <li><a href="#chapter10">DDQN</a></li>
      <li><a href="#chapter11">ç­–ç•¥æ¢¯åº¦æ–¹æ³•</a></li>
      <li><a href="#chapter12">Actor-Critic</a></li>
      <li><a href="#chapter13">A2C&A3C</a></li>
      <li><a href="#summary">ç®—æ³•æ€»ç»“</a></li>
    </ul>
  </aside>

  <main>
  <section id="chapter0" class="chapter">
    <h2>ç¬¬é›¶ç« ï¼šç¯å¢ƒé…ç½® & GitHub é¡¹ç›®ä¸Šä¼ </h2>
    <pre><code class="language-python"># é…ç½®é•œåƒæºï¼ˆç®¡ç†å‘˜èº«ä»½æ‰“å¼€Anaconda Promptï¼‰
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
conda config --set show_channel_urls yes

#é…ç½®Pipé•œåƒæº
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆä¸éœ€è¦ä¸ä½ æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´ï¼ŒæŒ‰ç…§è‡ªå·±éœ€æ±‚å°±è¡Œäº†ï¼‰
cd /d G:\test
conda create --prefix .\.venv python=3.11
conda activate G:\test\.venv

#æ‰“å¼€powershellæŸ¥è¯¢ä¸€ä¸‹è‡ªå·±çš„å‹å·ï¼ˆåç»­å®‰è£…pytorchç‰ˆæœ¬éœ€è¦ï¼Œæˆ‘çš„æ˜¯12.4ï¼Œå› æ­¤æˆ‘é€‰çš„12.1å°±å¯ä»¥å…¼å®¹ï¼‰
conda activate G:\test\.venv
nvidia-smi 

# å®‰è£…ä¾èµ–
conda install numpy pandas matplotlib pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia

# ä¸Šä¼ åˆ° GitHub
git init
git remote add origin https://github.com/HaoyunT/test.git
git checkout -b main
git add .
git commit -m "åˆå§‹åŒ–é¡¹ç›®"
git push -u origin main</code></pre>

    <h3>æœ¬ç¬”è®°å­¦ä¹ è·¯å¾„å¯¼å›¾</h3>
    <ul>
      <li><strong>ç¬¬1-3ç« ï¼šç†è®ºåŸºç¡€</strong>
        <ul>
          <li>ç¬¬1ç« ï¼šå¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µï¼ˆæ™ºèƒ½ä½“ã€ç¯å¢ƒã€çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä»·å€¼å‡½æ•°ï¼‰</li>
          <li>ç¬¬2ç« ï¼šè´å°”æ›¼æ–¹ç¨‹ï¼ˆä»·å€¼å‡½æ•°çš„é€’æ¨å…³ç³»ï¼‰</li>
          <li>ç¬¬3ç« ï¼šè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼ˆæœ€ä¼˜ç­–ç•¥çš„åˆ»ç”»ï¼‰</li>
        </ul>
      </li>
      <li><strong>ç¬¬4-5ç« ï¼šåŠ¨æ€è§„åˆ’æ–¹æ³•ï¼ˆéœ€çŸ¥é“ç¯å¢ƒæ¨¡å‹ï¼‰</strong>
        <ul>
          <li>ç¬¬4ç« ï¼šç­–ç•¥è¿­ä»£ï¼ˆè¯„ä¼° + æ”¹è¿›çš„å¾ªç¯ï¼‰</li>
          <li>ç¬¬5ç« ï¼šå€¼è¿­ä»£ï¼ˆç›´æ¥ä¼˜åŒ–ä»·å€¼å‡½æ•°ï¼‰</li>
        </ul>
      </li>
      <li><strong>ç¬¬6-7ç« ï¼šé‡‡æ ·æ–¹æ³•ï¼ˆæ— æ¨¡å‹å­¦ä¹ çš„åŸºç¡€ï¼‰</strong>
        <ul>
          <li>ç¬¬6ç« ï¼šè’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆå›åˆåæ›´æ–°ï¼Œé«˜æ–¹å·®ï¼‰</li>
          <li>ç¬¬7ç« ï¼šæ—¶é—´å·®åˆ†å­¦ä¹ ï¼ˆåœ¨çº¿æ›´æ–°ï¼Œä½æ–¹å·®ï¼Œè‡ªä¸¾æ€æƒ³ï¼‰</li>
        </ul>
      </li>
      <li><strong>ç¬¬8-10ç« ï¼šQå­¦ä¹ ç³»åˆ—ï¼ˆç¦»æ•£åŠ¨ä½œç©ºé—´ï¼‰</strong>
        <ul>
          <li>ç¬¬8ç« ï¼šQ-learningï¼ˆè¡¨æ ¼å½¢å¼çš„æ—¶å·®å­¦ä¹ ï¼‰</li>
          <li>ç¬¬9ç« ï¼šDQNï¼ˆç”¨ç¥ç»ç½‘ç»œé€¼è¿‘Qå‡½æ•°ï¼Œé€‚åº”é«˜ç»´çŠ¶æ€ï¼‰</li>
          <li>ç¬¬10ç« ï¼šDDQNï¼ˆåŒç½‘ç»œè§£å†³Qå€¼è¿‡ä¼°è®¡ï¼‰</li>
        </ul>
      </li>
      <li><strong>ç¬¬11-13ç« ï¼šç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œæ”¯æŒè¿ç»­åŠ¨ä½œï¼‰</strong>
        <ul>
          <li>ç¬¬11ç« ï¼šç­–ç•¥æ¢¯åº¦ï¼ˆç­–ç•¥å‚æ•°ç©ºé—´ä¸­çš„æ¢¯åº¦ä¸Šå‡ï¼‰</li>
          <li>ç¬¬12ç« ï¼šActor-Criticï¼ˆç»“åˆä»·å€¼å‡½æ•°é™ä½æ–¹å·®ï¼‰</li>
          <li>ç¬¬13ç« ï¼šA3C &amp; A2Cï¼ˆå¼‚æ­¥å¹¶è¡Œ â†’ åŒæ­¥æ”¹è¿›ï¼‰</li>
        </ul>
      </li>
    </ul>

    <p><strong>å…³é”®æ´å¯Ÿé“¾è·¯ï¼š</strong></p>
    <ol>
      <li>è´å°”æ›¼æ–¹ç¨‹ â†’ åŠ¨æ€è§„åˆ’æ–¹æ³•ï¼ˆç†è®ºæœ€ä¼˜ä½†éœ€è¦æ¨¡å‹ï¼‰</li>
      <li>è’™ç‰¹å¡æ´›æ–¹æ³• + æ—¶é—´å·®åˆ† â†’ æ— æ¨¡å‹å­¦ä¹ ï¼ˆé‡‡æ ·+è‡ªä¸¾ï¼‰</li>
      <li>Q-learningï¼ˆè¡¨æ ¼ï¼‰â†’ DQNï¼ˆç¥ç»ç½‘ç»œï¼‰â†’ DDQNï¼ˆç¨³å®šè®­ç»ƒï¼‰</li>
      <li>ç­–ç•¥æ¢¯åº¦ = ç»•è¿‡Qå‡½æ•°ï¼Œç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°</li>
      <li>Actor-Critic = ç”¨V(s)è¾…åŠ©ï¼ˆCriticï¼‰æ¥åŠ é€Ÿç­–ç•¥æ¢¯åº¦ï¼ˆActorï¼‰å­¦ä¹ </li>
      <li>A3C/A2C = Næ­¥TD + å¹¶è¡ŒåŒ–æå‡é‡‡æ ·å’Œè®­ç»ƒæ•ˆç‡ï¼ˆA3Cå¼‚æ­¥ â†’ A2CåŒæ­¥ï¼‰</li>
    </ol>

  </section>

<!-- ç¬¬ä¸€ç«  -->
<section id="chapter1" class="chapter">
  <h2>ç¬¬ä¸€ç« ï¼šå¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ</h2>
  <p>å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰æ ¸å¿ƒæ¦‚å¿µï¼š</p>
  <dl>
    <dt>æ™ºèƒ½ä½“ï¼ˆAgentï¼‰</dt>
    <dd>å†³ç­–æ‰§è¡Œä¸»ä½“ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ \(t\) è§‚å¯ŸçŠ¶æ€ \(S_t\)ï¼Œé€‰æ‹©åŠ¨ä½œ \(A_t\)ï¼Œå¹¶é€šè¿‡å¥–åŠ±ä¿¡å·è°ƒæ•´ç­–ç•¥ã€‚<br><em>ç¤ºä¾‹ï¼š</em>æ¸¸æˆä¸­çš„ç©å®¶è§’è‰²ã€‚</dd>

    <dt>ç¯å¢ƒï¼ˆEnvironmentï¼‰</dt>
    <dd>æ™ºèƒ½ä½“äº¤äº’å¯¹è±¡ï¼Œå®šä¹‰çŠ¶æ€ç©ºé—´ã€åŠ¨ä½œç©ºé—´å’ŒçŠ¶æ€è½¬ç§»è§„åˆ™ã€‚åŠ¨ä½œåè¿”å›çŠ¶æ€ \(S_{t+1}\) å’Œå¥–åŠ± \(R_t\)ã€‚<br><em>ç¤ºä¾‹ï¼š</em>æ¸¸æˆå…³å¡æˆ–ç‰©ç†æ¨¡æ‹Ÿå™¨ã€‚</dd>

    <dt>çŠ¶æ€ï¼ˆStateï¼‰</dt>
    <dd>æè¿°ç¯å¢ƒåœ¨æŸä¸€æ—¶åˆ»çš„ç‰¹å¾ï¼Œæ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨ã€‚<br><em>ç¤ºä¾‹ï¼š</em>è§’è‰²ä½ç½®ã€æ•Œäººä½ç½®ã€‚</dd>

    <dt>åŠ¨ä½œï¼ˆActionï¼‰</dt>
    <dd>æ™ºèƒ½ä½“å¯æ‰§è¡Œçš„æ“ä½œï¼Œæ”¹å˜ç¯å¢ƒçŠ¶æ€ã€‚<br><em>ç¤ºä¾‹ï¼š</em>å·¦å³ç§»åŠ¨ã€è·³è·ƒã€‚</dd>

    <dt>å¥–åŠ±ï¼ˆRewardï¼‰</dt>
    <dd>ç¯å¢ƒå¯¹åŠ¨ä½œçš„åé¦ˆï¼Œç”¨äºè¡¡é‡ä¼˜åŠ£ã€‚å¼ºåŒ–å­¦ä¹ ç›®æ ‡æœ€å¤§åŒ–ç´¯è®¡å¥–åŠ±ï¼š
      <br>\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k}\)
    </dd>

    <dt>ç­–ç•¥ï¼ˆPolicyï¼‰</dt>
    <dd>é€‰æ‹©åŠ¨ä½œçš„è§„åˆ™ï¼Œ\(\pi(a|s)\) æˆ– \(a = \pi(s)\)ã€‚ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ \(\pi^*\)ã€‚</dd>

    <dt>ä»·å€¼å‡½æ•°ï¼ˆValue Functionï¼‰</dt>
    <dd>è¡¡é‡çŠ¶æ€æˆ–çŠ¶æ€-åŠ¨ä½œå¯¹çš„é•¿æœŸå›æŠ¥ï¼š
      <ul>
        <li>çŠ¶æ€ä»·å€¼ï¼š\(V^\pi(s) = \mathbb{E}[G_t | S_t=s]\)</li>
        <li>åŠ¨ä½œä»·å€¼ï¼š\(Q^\pi(s,a) = \mathbb{E}[G_t | S_t=s, A_t=a]\)</li>
      </ul>
    </dd>
  </dl>
</section>

<!-- ç¬¬äºŒç«  -->
<section id="chapter2" class="chapter">
  <h2>ç¬¬äºŒç« ï¼šè´å°”æ›¼æ–¹ç¨‹ï¼ˆBellman Equationï¼‰</h2>
  <p>è´å°”æ›¼æ–¹ç¨‹ç”¨äºæè¿°ç­–ç•¥ä¸‹çŠ¶æ€æˆ–çŠ¶æ€-åŠ¨ä½œçš„ä»·å€¼é€’å½’å…³ç³»ï¼Œæ˜¯å¼ºåŒ–å­¦ä¹ ä¸­<strong>ç­–ç•¥è¯„ä¼°çš„æ ¸å¿ƒå·¥å…·</strong>ã€‚å®ƒå°†<strong>ä»·å€¼å‡½æ•°</strong>åˆ†è§£ä¸º<strong>å³æ—¶å¥–åŠ±</strong>å’Œ<strong>æœªæ¥ä»·å€¼çš„æŠ˜æ‰£æœŸæœ›</strong>ä¸¤éƒ¨åˆ†ï¼Œå¥ å®šäº†ä»ç¬¬3åˆ°ç¬¬10ç« æ‰€æœ‰ä»·å€¼å‡½æ•°æ–¹æ³•çš„ç†è®ºåŸºç¡€ã€‚</p>

  <dl>
    <dt><b>çŠ¶æ€ä»·å€¼å‡½æ•°ï¼ˆState Value Functionï¼‰</b></dt>
    <dd>
      å¯¹äºç­–ç•¥ <code>Ï€</code> ä¸‹çš„çŠ¶æ€ä»·å€¼å‡½æ•° <code>V^Ï€(s)</code>ï¼š<br>
      <p>$$ V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s,a) \big[ R(s,a) + \gamma V^\pi(s') \big] $$</p>
      è¯´æ˜ï¼šå½“å‰çŠ¶æ€ä»·å€¼ = å³æ—¶å¥–åŠ± + æŠ˜æ‰£åçš„æœªæ¥ä»·å€¼æœŸæœ›ã€‚
    </dd>

    <dt><b>çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°ï¼ˆAction-Value Functionï¼‰</b></dt>
    <dd>
      å¯¹äºçŠ¶æ€-åŠ¨ä½œå¯¹çš„ä»·å€¼å‡½æ•° <code>Q^Ï€(s,a)</code>ï¼š<br>
      <p>$$ Q^\pi(s,a) = R(s,a) + \gamma \sum_{s' \in S} \sum_{a' \in A} \pi(a'|s') Q^\pi(s',a') $$</p>
    </dd>

    <dt><b>çŠ¶æ€ä»·å€¼å‡½æ•°ä¸çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°çš„å…³ç³»</b></dt>
    <dd>
      çŠ¶æ€ä»·å€¼å‡½æ•°å¯ä»¥ç”±çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°å¾—åˆ°ï¼š<br>
      <p>$$ V^\pi(s) = \sum_{a \in A} \pi(a|s) Q^\pi(s,a) $$</p>
      è§£é‡Šï¼šåœ¨çŠ¶æ€ <code>s</code> ä¸‹ï¼Œä»·å€¼å‡½æ•° <code>V^Ï€(s)</code> æ˜¯ç­–ç•¥é€‰æ‹©åŠ¨ä½œåçš„æœŸæœ› <code>Q^Ï€(s,a)</code>ã€‚
    </dd>
  </dl>

  <p><strong>ç”¨é€”ï¼š</strong>ç­–ç•¥è¯„ä¼°ã€ç­–ç•¥è¿­ä»£ã€ä»·å€¼è¿­ä»£çš„ç†è®ºåŸºç¡€ã€‚</p>
</section>


<!-- ç¬¬ä¸‰ç«  -->
<section id="chapter3" class="chapter">
  <h2>ç¬¬ä¸‰ç« ï¼šè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼ˆBellman Optimality Equationï¼‰</h2>

  <p>åœ¨å¯»æ‰¾æœ€ä¼˜ç­–ç•¥ <code>Ï€*</code> æ—¶ï¼ŒçŠ¶æ€å’ŒçŠ¶æ€-åŠ¨ä½œçš„ä»·å€¼å‡½æ•°æ»¡è¶³<strong>è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹</strong>ï¼Œä½“ç°æœ€ä¼˜æ€§åŸåˆ™ã€‚è¿™æ˜¯ç¬¬4-5ç« ï¼ˆåŠ¨æ€è§„åˆ’æ–¹æ³•ï¼‰å’Œç¬¬8-10ç« ï¼ˆæ·±åº¦Qå­¦ä¹ ï¼‰çš„ç†è®ºåŸºçŸ³ï¼Œå°†"æœ€ä¼˜ç­–ç•¥æœç´¢"è½¬åŒ–ä¸º"æœ€ä¼˜ä»·å€¼å‡½æ•°æ±‚è§£"é—®é¢˜ã€‚</p>

  <dl>
    <dt><b>æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•°</b></dt>
    <dd>
      <p>$$
      V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s,a) \big[ R(s,a) + \gamma V^*(s') \big]
      $$</p>
    </dd>

    <dt><b>æœ€ä¼˜çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°</b></dt>
    <dd>
      <p>$$
      Q^*(s,a) = R(s,a) + \gamma \sum_{s' \in S} \max_{a' \in A} Q^*(s',a')
      $$</p>
    </dd>
  </dl>

  <p><strong>å…³ç³»æ€»ç»“ï¼š</strong></p>
  <ul>
    <li>ç¬¬äºŒç« è´å°”æ›¼æ–¹ç¨‹ï¼šç»™å®šç­–ç•¥ Ï€ â†’ è®¡ç®— V^Ï€ æˆ– Q^Ï€</li>
    <li>ç¬¬ä¸‰ç« è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼šæ±‚æœ€ä¼˜ç­–ç•¥ Ï€* â†’ V* æˆ– Q*</li>
  </ul>
</section>
<!-- ç¬¬å››ç«  -->
<section id="chapter4" class="chapter">
  <h2>ç¬¬å››ç« ï¼šç­–ç•¥è¿­ä»£ï¼ˆPolicy Iterationï¼‰</h2>

  <p>ç­–ç•¥è¿­ä»£é€šè¿‡äº¤æ›¿è¿›è¡Œç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›æ¥æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚</p>

  <ol>
    <li><strong>åˆå§‹åŒ–ç­–ç•¥ï¼š</strong>é€‰æ‹©åˆå§‹ç­–ç•¥ <code>Ï€_0</code></li>
    <li><strong>ç­–ç•¥è¯„ä¼°ï¼š</strong>è®¡ç®—çŠ¶æ€ä»·å€¼å‡½æ•°ï¼š
      <p>$$
      v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}
      $$</p>
    </li>
    <li><strong>ç­–ç•¥æ”¹è¿›ï¼š</strong>æ›´æ–°ç­–ç•¥ï¼š
      <p>$$
      \pi_{k+1} = \arg\max_\pi (r_\pi + \gamma P_\pi v_{\pi_k})
      $$</p>
    </li>
    <li>é‡å¤è¯„ä¼°å’Œæ”¹è¿›ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›ã€‚</li>
  </ol>
</section>


<!-- ç¬¬äº”ç«  -->
<section id="chapter5" class="chapter">
  <h2>ç¬¬äº”ç« ï¼šå€¼è¿­ä»£ï¼ˆValue Iterationï¼‰</h2>

  <p>å€¼è¿­ä»£ç›´æ¥è¿­ä»£çŠ¶æ€ä»·å€¼å‡½æ•°ï¼Œé€šè¿‡è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ”¶æ•›åˆ°æœ€ä¼˜å€¼å‡½æ•°ï¼Œç„¶åå¯¼å‡ºæœ€ä¼˜ç­–ç•¥ã€‚</p>

  <ol>
    <li><strong>åˆå§‹åŒ–ä»·å€¼å‡½æ•°ï¼š</strong>é€‰æ‹©åˆå§‹å€¼ <code>v_0</code></li>
    <li><strong>è¿­ä»£æ›´æ–°ï¼š</strong>ä½¿ç”¨è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼š
      <p>$$
      v_{k+1} = \max_\pi (r_\pi + \gamma P_\pi v_k)
      $$</p>
    </li>
    <li><strong>ç­–ç•¥å¯¼å‡ºï¼š</strong>æ”¶æ•›åé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼š
      <p>$$
      \pi^*(s) = \arg\max_a \sum_{s'} P(s'|s,a) \big[R(s,a) + \gamma v^*(s') \big]
      $$</p>
    </li>
  </ol>
</section>


<!-- å¯¹æ¯”-->
<section id="chapterX" class="chapter">
  <h2>ç­–ç•¥è¿­ä»£ä¸å€¼è¿­ä»£çš„å¯¹æ¯”</h2>

  <table border="1" cellpadding="8" cellspacing="0">
    <thead>
      <tr>
        <th></th>
        <th>Policy Iteration</th>
        <th>Value Iteration</th>
        <th>Comments</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>(1)Policy</td>
        <td>\(\pi_0\)</td>
        <td>N/A</td>
        <td></td>
      </tr>
      <tr>
        <td>(2)Value</td>
        <td>\(v_{\pi_0} = r_{\pi_0} + \gamma P_{\pi_0} v_{\pi_0}\)</td>
        <td>\(v_0 := v_{\pi_0}\)</td>
        <td></td>
      </tr>
      <tr>
        <td>(3)Policy</td>
        <td>\(\pi_1 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_{\pi_0} )\)</td>
        <td>\(\pi_1 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_0 )\)</td>
        <td>The two policies are the same</td>
      </tr>
      <tr>
        <td>(4)Value</td>
        <td>\(v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}\)</td>
        <td>\(v_1 = r_{\pi_1} + \gamma P_{\pi_1} v_0\)</td>
        <td>\(v_{\pi_1} \ge v_1 \text{ since } v_{\pi_1} \ge v_{\pi_0}\)</td>
      </tr>
      <tr>
        <td>5) Policy</td>
        <td>\(\pi_2 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_{\pi_1} )\)</td>
        <td>\(\pi'_2 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_1 )\)</td>
        <td></td>
      </tr>
    </tbody>
  </table>

  <h3>é‡è¦è¿‡æ¸¡è¯´æ˜ï¼šä»åŠ¨æ€è§„åˆ’åˆ°é‡‡æ ·æ–¹æ³•</h3>
  <p><strong>åŠ¨æ€è§„åˆ’ï¼ˆç¬¬4-5ç« ï¼‰çš„å±€é™æ€§ï¼š</strong></p>
  <ul>
    <li>âœ“ æ”¶æ•›æ€§ç†è®ºå®Œå¤‡ï¼Œæœ€ä¼˜æ€§æœ‰ä¿è¯</li>
    <li>âœ— <strong>éœ€è¦çŸ¥é“ç¯å¢ƒæ¨¡å‹</strong>ï¼ˆçŠ¶æ€è½¬ç§»æ¦‚ç‡ Pã€å¥–åŠ±å‡½æ•° Rï¼‰</li>
    <li>âœ— è®¡ç®—å¤æ‚åº¦é«˜ï¼ˆè¿­ä»£æ¬¡æ•°å¤šï¼ŒçŠ¶æ€ç©ºé—´å¤§æ—¶æ•ˆç‡ä½ï¼‰</li>
  </ul>

  <p><strong>é‡‡æ ·æ–¹æ³•ï¼ˆç¬¬6-7ç« ï¼‰çš„ä¼˜åŠ¿ï¼š</strong></p>
  <ul>
    <li>âœ“ <strong>æ— æ¨¡å‹ï¼ˆModel-Freeï¼‰</strong>ï¼šåªéœ€ä¸ç¯å¢ƒäº¤äº’é‡‡æ ·ï¼Œæ— éœ€çŸ¥é“è½¬ç§»æ¦‚ç‡</li>
    <li>âœ“ æ›´åˆ‡åˆå®é™…ï¼šçœŸå®ä¸–ç•Œé€šå¸¸æ— æ³•è·å¾—å®Œæ•´çš„ç¯å¢ƒæ¨¡å‹</li>
    <li>âœ“ é€šè¿‡è’™ç‰¹å¡æ´›ä¼°è®¡å’Œæ—¶é—´å·®åˆ†å­¦ä¹ é€æ­¥ä¼˜åŒ–ç­–ç•¥</li>
  </ul>

  <p><strong>ç†è®ºæ¡¥æ¢ï¼š</strong>å°½ç®¡é‡‡æ ·æ–¹æ³•ä¸éœ€è¦æ˜¾å¼çš„ç¯å¢ƒæ¨¡å‹ï¼Œä½†å®ƒä»¬çš„ç†è®ºåŸºç¡€ä»æºäºè´å°”æ›¼æ–¹ç¨‹å’Œè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ã€‚é€šè¿‡é‡‡æ ·æ•°æ®éšå¼åœ°å­¦ä¹ è¿™äº›å…³ç³»ï¼Œä»è€Œä¼˜åŒ–ç­–ç•¥ã€‚</p>
</section>

 <!-- ç¬¬å…­ç«  -->
<section id="chapter6" class="chapter">
  <h2>ç¬¬å…­ç« ï¼šè’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆMonte Carlo Methodsï¼‰</h2>

  <p>è’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆMonte Carloï¼Œç®€ç§° <b>MC</b>ï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ä¸€ç§åŸºäº<strong>é‡‡æ ·</strong>çš„ç­–ç•¥è¯„ä¼°ä¸ä¼˜åŒ–æ–¹æ³•ã€‚
  ä¸åŠ¨æ€è§„åˆ’ä¸åŒï¼ŒMC ä¸ä¾èµ–ç¯å¢ƒçš„çŠ¶æ€è½¬ç§»æ¦‚ç‡æ¨¡å‹ <code>P(s'|s,a)</code>ï¼Œè€Œæ˜¯é€šè¿‡åå¤ä»ç­–ç•¥ <code>Ï€</code> ç”Ÿæˆå®Œæ•´å›åˆï¼ˆEpisodeï¼‰ï¼Œ
  æ ¹æ®å®é™…è·å¾—çš„å›æŠ¥ä¼°è®¡çŠ¶æ€æˆ–çŠ¶æ€-åŠ¨ä½œä»·å€¼ã€‚</p>

  <h3>ä¸€ã€åŸºæœ¬æ€æƒ³</h3>
  <p>ç»™å®šç­–ç•¥ \( \pi \)ï¼Œæ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’å¤šæ¬¡ï¼Œå¾—åˆ°è‹¥å¹²å®Œæ•´å›åˆï¼š</p>
  <p>$$
  (S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_T)
  $$</p>
  <p>å¯¹äºæ¯ä¸ªå›åˆï¼Œå®šä¹‰ä»æ—¶é—´æ­¥ \(t\) å¼€å§‹çš„ç´¯è®¡å›æŠ¥ï¼ˆReturnï¼‰ï¼š</p>
  <p>$$
  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}
  $$</p>
  <p>MC æ–¹æ³•é€šè¿‡å¤šæ¬¡é‡‡æ ·ï¼Œåˆ©ç”¨è¿™äº› \(G_t\) çš„å¹³å‡å€¼æ¥è¿‘ä¼¼ä»·å€¼å‡½æ•°ã€‚</p>

  <dl>
    <dt><b>çŠ¶æ€ä»·å€¼ä¼°è®¡ï¼š</b></dt>
    <dd>$$
    V(s) = \mathbb{E}_\pi[G_t \mid S_t = s] \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G^{(i)}(s)
    $$</dd>

    <dt><b>åŠ¨ä½œä»·å€¼ä¼°è®¡ï¼š</b></dt>
    <dd>$$
    Q(s,a) = \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a] \approx \frac{1}{N(s,a)} \sum_{i=1}^{N(s,a)} G^{(i)}(s,a)
    $$</dd>
  </dl>

  <h3>äºŒã€é¦–æ¬¡è®¿é—®ä¸æ¯æ¬¡è®¿é—®</h3>
  <ul>
    <li><b>First-Visit MCï¼š</b> ä»…åœ¨ä¸€ä¸ªå›åˆä¸­ç¬¬ä¸€æ¬¡è®¿é—®çŠ¶æ€ï¼ˆæˆ–çŠ¶æ€-åŠ¨ä½œå¯¹ï¼‰æ—¶è¿›è¡Œæ›´æ–°ã€‚</li>
    <li><b>Every-Visit MCï¼š</b> å¯¹å›åˆä¸­æ¯æ¬¡è®¿é—®çŠ¶æ€ï¼ˆæˆ–çŠ¶æ€-åŠ¨ä½œå¯¹ï¼‰éƒ½è¿›è¡Œæ›´æ–°ã€‚</li>
  </ul>
  <p>äºŒè€…æœ€ç»ˆéƒ½ä¼šæ”¶æ•›åˆ°çœŸå®çš„ä»·å€¼å‡½æ•°ï¼Œä½†é¦–æ¬¡è®¿é—® MC æ–¹å·®ç•¥å°ã€‚</p>

  <h3>ä¸‰ã€è’™ç‰¹å¡æ´›ç­–ç•¥è¯„ä¼°ç®—æ³•</h3>
  <p><strong>åŸºæœ¬æ€æƒ³ï¼š</strong>ç»™å®šç­–ç•¥ Ï€ï¼Œè¯„ä¼°å…¶çŠ¶æ€ä»·å€¼å‡½æ•° V(s)</p>
  <ol>
    <li><strong>åˆå§‹åŒ–ï¼š</strong>å¯¹æ‰€æœ‰çŠ¶æ€ sï¼Œè®¾ V(s) = 0ï¼ŒåŒæ—¶è®°å½•æ¯ä¸ªçŠ¶æ€çš„å›æŠ¥åˆ—è¡¨ Returns(s)</li>
    <li><strong>é‡‡é›†æ•°æ®ï¼š</strong>ä¸ç¯å¢ƒäº¤äº’å¤šæ¬¡ï¼Œæ¯æ¬¡äº§ç”Ÿä¸€ä¸ªå®Œæ•´å›åˆ (Sâ‚€, Aâ‚€, Râ‚, Sâ‚, ..., S_T)</li>
    <li><strong>è®¡ç®—å›æŠ¥ï¼š</strong>å¯¹æ¯ä¸ªå›åˆï¼Œä»æœ«å°¾å‘å‰è®¡ç®—ç´¯è®¡å›æŠ¥ G_t = R_{t+1} + Î³R_{t+2} + ...</li>
    <li><strong>ç­–ç•¥è¯„ä¼°ï¼ˆé¦–æ¬¡è®¿é—®ï¼‰ï¼š</strong>å¯¹æ¯ä¸ªçŠ¶æ€ï¼Œä»…åœ¨è¯¥å›åˆä¸­ç¬¬ä¸€æ¬¡å‡ºç°æ—¶ï¼š
      <ul>
        <li>å°†è¯¥æ—¶åˆ»çš„å›æŠ¥ G_t åŠ å…¥ Returns(S_t)</li>
        <li>æ›´æ–°ä»·å€¼å‡½æ•°ï¼šV(S_t) = Returns(S_t) çš„å¹³å‡å€¼</li>
      </ul>
    </li>
    <li><strong>é‡å¤ï¼š</strong>é‡å¤é‡‡é›†å’Œæ›´æ–°è¿‡ç¨‹ï¼ŒV(s) é€æ¸é€¼è¿‘çœŸå®æœŸæœ›å›æŠ¥</li>
  </ol>
  <p style="margin-top: 15px;">è¿™ä¸ªç®—æ³•æ˜¯<strong>æ— æ¨¡å‹ã€åŸºäºé‡‡æ ·</strong>çš„æ–¹æ³•â€”â€”åªéœ€è¦å®é™…å›åˆæ•°æ®ï¼Œä¸éœ€è¦çŸ¥é“ç¯å¢ƒçš„çŠ¶æ€è½¬ç§»æ¨¡å‹ã€‚</p>

  <h3>å››ã€è’™ç‰¹å¡æ´›æ§åˆ¶ï¼ˆMC Controlï¼‰</h3>
  <p>åœ¨ç­–ç•¥è¯„ä¼°çš„åŸºç¡€ä¸Šï¼ŒMC è¿˜å¯ä»¥å®ç°<strong>ç­–ç•¥æ”¹è¿›</strong>ï¼Œå½¢æˆä¸€ä¸ªä¸ç­–ç•¥è¿­ä»£ç±»ä¼¼çš„è¿‡ç¨‹ï¼š</p>
  <ol>
    <li>è¯„ä¼°å½“å‰ç­–ç•¥ \(\pi\)ï¼šä½¿ç”¨ MC æ–¹æ³•ä¼°è®¡ \(Q^\pi(s,a)\)</li>
    <li>æ”¹è¿›ç­–ç•¥ï¼šé‡‡ç”¨è´ªå©ªæˆ– Îµ-è´ªå©ªæ–¹å¼æ›´æ–°ç­–ç•¥
      <p>$$
      \pi'(s) = \arg\max_a Q(s,a)
      $$</p>
    </li>
    <li>é‡å¤ä»¥ä¸Šè¿‡ç¨‹ï¼Œç›´åˆ°æ”¶æ•›</li>
  </ol>

  <h3>äº”ã€Îµ-è´ªå©ªï¼ˆÎµ-Greedyï¼‰æ¢ç´¢ç­–ç•¥</h3>
  <p>ä¸ºäº†é¿å…è¿‡æ—©é™·å…¥æ¬¡ä¼˜ç­–ç•¥ï¼ŒMC æ§åˆ¶ä¸­å¸¸ç”¨ <b>Îµ-è´ªå©ªç­–ç•¥</b> è¿›è¡Œæ¢ç´¢ï¼š</p>
  <p>$$
  \pi(a|s) = 
  \begin{cases}
  1 - \varepsilon + \frac{\varepsilon}{|A(s)|}, & a = \arg\max_{a'} Q(s,a') \\
  \frac{\varepsilon}{|A(s)|}, & \text{å¦åˆ™}
  \end{cases}
  $$</p>
  <p>å…¶ä¸­ \(\varepsilon \in [0,1]\) è¡¨ç¤ºéšæœºæ¢ç´¢æ¦‚ç‡ã€‚</p>

  <h3>å…­ã€MC æ§åˆ¶ç¤ºä¾‹ä»£ç </h3>
<pre><code class="language-python">
import numpy as np
from collections import defaultdict

class MCControl:
    def __init__(self, num_states, num_actions, gamma=0.99, epsilon=0.1, alpha=0.1):
        self.num_states = num_states
        self.num_actions = num_actions
        self.gamma = gamma
        self.epsilon = epsilon
        self.alpha = alpha
        self.Q = defaultdict(lambda: np.zeros(num_actions))
        self.returns = defaultdict(list)  # å­˜å‚¨æ¯ä¸ª(s,a)çš„å›æŠ¥åˆ—è¡¨
        
    def epsilon_greedy(self, state):
        """Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.num_actions)
        else:
            return np.argmax(self.Q[state])
    
    def train_episode(self, env):
        """è¿è¡Œä¸€ä¸ªå®Œæ•´å›åˆå¹¶æ›´æ–°Qå€¼"""
        trajectory = []
        state = env.reset()
        done = False
        
        # æ”¶é›†å›åˆè½¨è¿¹
        while not done:
            action = self.epsilon_greedy(state)
            next_state, reward, done, _ = env.step(action)
            trajectory.append((state, action, reward))
            state = next_state
        
        # ä»åå‘å‰è®¡ç®—å›æŠ¥å¹¶æ›´æ–°Qå€¼
        G = 0
        visited_pairs = set()
        for t in range(len(trajectory) - 1, -1, -1):
            state, action, reward = trajectory[t]
            G = reward + self.gamma * G
            
            # åªåœ¨çŠ¶æ€-åŠ¨ä½œå¯¹ç¬¬ä¸€æ¬¡å‡ºç°æ—¶æ›´æ–°ï¼ˆFirst-Visit MCï¼‰
            if (state, action) not in visited_pairs:
                visited_pairs.add((state, action))
                self.returns[(state, action)].append(G)
                self.Q[state][action] = np.mean(self.returns[(state, action)])
    
    def train(self, env, num_episodes=1000):
        """è®­ç»ƒè¿‡ç¨‹"""
        rewards = []
        for episode in range(num_episodes):
            self.train_episode(env)
            
            if (episode + 1) % 100 == 0:
                # æµ‹è¯•å½“å‰ç­–ç•¥
                test_reward = 0
                for _ in range(10):
                    test_reward += self.test_episode(env)
                avg_reward = test_reward / 10
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
                rewards.append(avg_reward)
        return rewards
    
    def test_episode(self, env):
        """æµ‹è¯•å½“å‰ç­–ç•¥"""
        state = env.reset()
        done = False
        episode_reward = 0
        while not done:
            action = np.argmax(self.Q[state])
            state, reward, done, _ = env.step(action)
            episode_reward += reward
        return episode_reward
    
    def get_policy(self):
        """è·å–è´ªå©ªç­–ç•¥"""
        policy = {}
        for state in self.Q:
            policy[state] = np.argmax(self.Q[state])
        return policy
  </code></pre>


  <h3>ä¸ƒã€MC æ–¹æ³•çš„ç‰¹ç‚¹ä¸å±€é™</h3>
  <table border="1" cellpadding="8" cellspacing="0">
    <thead>
      <tr><th>ä¼˜ç‚¹</th><th>ç¼ºç‚¹</th></tr>
    </thead>
    <tbody>
      <tr>
        <td>æ— éœ€çŸ¥é“ç¯å¢ƒæ¨¡å‹ï¼ˆPã€Rï¼‰</td>
        <td>å¿…é¡»ç­‰å¾…å®Œæ•´å›åˆç»“æŸï¼Œä¸èƒ½ç”¨äºæŒç»­ä»»åŠ¡</td>
      </tr>
      <tr>
        <td>å®ç°ç®€å•ï¼Œæ¦‚å¿µç›´è§‚</td>
        <td>æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ï¼Œæ–¹å·®å¤§</td>
      </tr>
      <tr>
        <td>é€‚åˆç¦»çº¿æ¨¡æ‹Ÿç¯å¢ƒï¼ˆå¦‚æ¸¸æˆï¼‰</td>
        <td>å¯¹é•¿æœŸä»»åŠ¡æˆ–é«˜ç»´çŠ¶æ€ç©ºé—´ä¸é€‚ç”¨</td>
      </tr>
    </tbody>
  </table>

  <h3>å…«ã€å°ç»“</h3>
  <ul>
    <li>MC æ˜¯åŸºäºé‡‡æ ·çš„<strong>ç­–ç•¥è¯„ä¼°ä¸æ§åˆ¶</strong>æ–¹æ³•ã€‚</li>
    <li>ä¾èµ–å›åˆç»“æŸçš„å®é™…å›æŠ¥ï¼Œè€Œéä¼°è®¡çš„ä¸‹ä¸€æ­¥ä»·å€¼ã€‚</li>
    <li>ä¸åŠ¨æ€è§„åˆ’ç›¸æ¯”ï¼Œä¸éœ€çŸ¥é“æ¨¡å‹ï¼Œä½†æ”¶æ•›æ…¢ã€‚</li>
    <li>æ˜¯ç†è§£æ—¶åºå·®åˆ†ï¼ˆTDï¼‰å­¦ä¹ çš„é‡è¦è¿‡æ¸¡ã€‚</li>
  </ul>

</section>
<!-- ç¬¬ä¸ƒç«  -->
<section id="chapter7" class="chapter">
  <h2>ç¬¬ä¸ƒç« ï¼šæ—¶é—´å·®åˆ†å­¦ä¹ ï¼ˆTD Learningï¼‰</h2>

  <p>æ—¶é—´å·®åˆ†å­¦ä¹ ï¼ˆTemporal Differenceï¼Œç®€ç§° <b>TD</b>ï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ç»“åˆäº†
  <strong>è’™ç‰¹å¡æ´›æ–¹æ³•</strong>ä¸<strong>åŠ¨æ€è§„åˆ’</strong>çš„ç­–ç•¥è¯„ä¼°ä¸æ§åˆ¶æ–¹æ³•ã€‚
  ä¸è’™ç‰¹å¡æ´›ä¸åŒï¼ŒTD å¯ä»¥åœ¨å›åˆæœªç»“æŸæ—¶å°±æ›´æ–°çŠ¶æ€ä»·å€¼ï¼Œå› æ­¤å±äº<strong>åœ¨çº¿æ›´æ–°</strong>æ–¹æ³•ã€‚</p>

  <h3>ä¸€ã€TD(0)æ ¸å¿ƒå…¬å¼</h3>
  <p>å•æ­¥TDæ›´æ–°çŠ¶æ€ä»·å€¼å‡½æ•°å…¬å¼ï¼š</p>
  <p>$$
  V(s_t) \leftarrow V(s_t) + \alpha \big[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \big]
  $$</p>
  <p>å…¶ä¸­ï¼š</p>
  <ul>
    <li><code>V(s_t)</code>ï¼šå½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡</li>
    <li><code>r_{t+1}</code>ï¼šå½“å‰åŠ¨ä½œçš„å³æ—¶å¥–åŠ±</li>
    <li><code>Î³</code>ï¼šæŠ˜æ‰£å› å­</li>
    <li><code>Î±</code>ï¼šå­¦ä¹ ç‡</li>
    <li><code>Î´_t = r_{t+1} + Î³ V(s_{t+1}) - V(s_t)</code>ï¼šTDè¯¯å·®</li>
  </ul>

  <h3>äºŒã€TD(0)ç®—æ³•æµç¨‹</h3>
  <p><strong>TD ä¸ MC æœ€å¤§çš„åŒºåˆ«ï¼šä¸ç­‰å¾…å›åˆç»“æŸï¼Œåœ¨æ¯ä¸€æ­¥å°±è¿›è¡Œæ›´æ–°ï¼</strong></p>
  <ol>
    <li><strong>åˆå§‹åŒ–ï¼š</strong>å¯¹æ‰€æœ‰çŠ¶æ€ sï¼Œè®¾ V(s) = 0</li>
    <li><strong>äº¤äº’ä¸€æ­¥ï¼š</strong>åœ¨çŠ¶æ€ s_t æ‰§è¡ŒåŠ¨ä½œ a_tï¼Œè§‚å¯Ÿå³æ—¶å¥–åŠ± r_{t+1} å’Œä¸‹ä¸€çŠ¶æ€ s_{t+1}</li>
    <li><strong>è®¡ç®— TD è¯¯å·®ï¼š</strong>Î´_t = r_{t+1} + Î³Â·V(s_{t+1}) - V(s_t)</li>
    <li><strong>æ›´æ–°ä»·å€¼å‡½æ•°ï¼š</strong>V(s_t) â† V(s_t) + Î±Â·Î´_tï¼ˆä½¿ç”¨å­¦ä¹ ç‡ Î±ï¼‰</li>
    <li><strong>ç»§ç»­äº¤äº’ï¼š</strong>s_t â† s_{t+1}ï¼Œé‡å¤æ­¥éª¤ 2-4ï¼Œä¸éœ€è¦ç­‰å¾…å›åˆç»“æŸ</li>
  </ol>
  <p style="margin-top: 15px;"><strong>å…³é”®ä¼˜åŠ¿ï¼š</strong></p>
  <ul>
    <li>âœ“ å¯ä»¥è¿›è¡Œ <strong>åœ¨çº¿å­¦ä¹ </strong>ï¼Œå›åˆä¸­å®æ—¶æ›´æ–°</li>
    <li>âœ“ å¯ä»¥ç”¨äº <strong>æŒç»­ä»»åŠ¡</strong>ï¼ˆæ— ç»ˆæ­¢çŠ¶æ€çš„ä»»åŠ¡ï¼‰</li>
    <li>âœ“ æ”¶æ•›é€Ÿåº¦æ¯” MC æ›´å¿«</li>
    <li>âœ“ åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­æ›´ç¨³å®š</li>
  </ul>

  <h3>ä¸‰ã€TDå­¦ä¹ ä¸MCæ–¹æ³•å¯¹æ¯”</h3>
  <table>
    <tr><th>æ–¹æ³•</th><th>æ›´æ–°æ—¶æœº</th><th>æ˜¯å¦ä¾èµ–ç¯å¢ƒæ¨¡å‹</th><th>åå·®/æ–¹å·®</th></tr>
    <tr><td>è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰</td><td>å›åˆç»“æŸ</td><td>å¦</td><td>æ— åï¼Œæ–¹å·®å¤§</td></tr>
    <tr><td>TD(0)</td><td>æ¯ä¸€æ­¥</td><td>å¦</td><td>æœ‰åï¼Œæ–¹å·®å°</td></tr>
  </table>
  <p>MCä½¿ç”¨å®Œæ•´å›æŠ¥ <code>G_t</code> æ›´æ–°ä»·å€¼ï¼Œè€ŒTDä½¿ç”¨ä¸‹ä¸€çŠ¶æ€ä¼°è®¡ <code>V(s_{t+1})</code> å¼•å¯¼å½“å‰æ›´æ–°ã€‚</p>

  <h3>å››ã€TD(Î»)ä¸èµ„æ ¼è¿¹</h3>
  <p>TD(Î»)å¼•å…¥äº† <b>èµ„æ ¼è¿¹ï¼ˆeligibility traceï¼‰</b>ï¼Œå¯ä»¥ç»“åˆå¤šæ­¥ä¿¡æ¯æ›´æ–°ä»·å€¼ï¼š</p>
  <p>$$
  V(s) \leftarrow V(s) + \alpha \delta_t e(s)
  $$</p>
  <ul>
    <li><code>e(s)</code>ï¼šçŠ¶æ€çš„èµ„æ ¼è¿¹ï¼Œè®°å½•è¿‡å»æ—¶é—´æ­¥çš„é‡è¦æ€§</li>
    <li>Î» âˆˆ [0,1] æ§åˆ¶å¤šæ­¥ä¿¡æ¯è¡°å‡ï¼šÎ»=0 â†’ TD(0)ï¼ŒÎ»=1 â†’ æ¥è¿‘MC</li>
  </ul>

  <h3>äº”ã€TDæ§åˆ¶ä¸Îµ-è´ªå©ªç­–ç•¥</h3>
  <p>åœ¨ç­–ç•¥è¯„ä¼°åŸºç¡€ä¸Šï¼ŒTDä¹Ÿå¯è¿›è¡Œç­–ç•¥æ”¹è¿›ï¼Œå®ç°TDæ§åˆ¶ï¼š</p>
  <ol>
    <li>è¯„ä¼°å½“å‰ç­–ç•¥ï¼šä½¿ç”¨TDä¼°è®¡ Q(s,a)</li>
    <li>æ”¹è¿›ç­–ç•¥ï¼šé‡‡ç”¨Îµ-è´ªå©ªé€‰æ‹©åŠ¨ä½œ
      <p>$$
      \pi(a|s) = 
      \begin{cases}
      1 - \varepsilon + \frac{\varepsilon}{|A(s)|}, & a = \arg\max_{a'} Q(s,a') \\
      \frac{\varepsilon}{|A(s)|}, & \text{å¦åˆ™}
      \end{cases}
      $$</p>
    </li>
    <li>é‡å¤ä»¥ä¸Šè¿‡ç¨‹ç›´åˆ°æ”¶æ•›</li>
  </ol>

  <h3>å…­ã€TD æ§åˆ¶ç¤ºä¾‹ä»£ç ï¼ˆSARSAï¼‰</h3>
  <pre><code class="language-python">
import numpy as np
from collections import defaultdict

class SARSA:
    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.num_states = num_states
        self.num_actions = num_actions
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = defaultdict(lambda: np.zeros(num_actions))
    
    def epsilon_greedy(self, state):
        """Îµ-è´ªå©ªç­–ç•¥"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.num_actions)
        else:
            return np.argmax(self.Q[state])
    
    def train_step(self, state, action, reward, next_state, next_action, done):
        """å•æ­¥SARSAæ›´æ–°"""
        if done:
            target = reward
        else:
            target = reward + self.gamma * self.Q[next_state][next_action]
        
        # TDè¯¯å·®
        td_error = target - self.Q[state][action]
        
        # Qå€¼æ›´æ–°
        self.Q[state][action] += self.alpha * td_error
    
    def train_episode(self, env):
        """è¿è¡Œä¸€ä¸ªå®Œæ•´å›åˆ"""
        state = env.reset()
        action = self.epsilon_greedy(state)
        done = False
        episode_reward = 0
        
        while not done:
            next_state, reward, done, _ = env.step(action)
            next_action = self.epsilon_greedy(next_state)
            
            # æ›´æ–°Qå€¼
            self.train_step(state, action, reward, next_state, next_action, done)
            
            episode_reward += reward
            state = next_state
            action = next_action
        
        return episode_reward
    
    def train(self, env, num_episodes=1000):
        """è®­ç»ƒè¿‡ç¨‹"""
        rewards = []
        for episode in range(num_episodes):
            reward = self.train_episode(env)
            rewards.append(reward)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        return rewards
  </code></pre>

  <h3>ä¸ƒã€Bootstrappingï¼ˆè‡ªä¸¾/å¼•å¯¼ï¼‰è§£é‡Š</h3>
  <p><b>Bootstrapping</b> æ˜¯TDæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³ï¼š</p>
  <p>TDä¸æ˜¯ç­‰åˆ°å®Œæ•´å›åˆè·å¾—çœŸå®å›æŠ¥ <code>G_t</code> æ‰æ›´æ–°ä»·å€¼ï¼Œè€Œæ˜¯ä½¿ç”¨å¯¹ä¸‹ä¸€çŠ¶æ€ä»·å€¼çš„ä¼°è®¡ <code>V(s_{t+1})</code> æ¥å¼•å¯¼å½“å‰çŠ¶æ€ä»·å€¼æ›´æ–°ï¼š</p>
  <p>$$
  V(s_t) \leftarrow V(s_t) + \alpha \big[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \big]
  $$</p>
  <p>è¿™ç§æ–¹å¼ç§°ä¸ºâ€œè‡ªä¸¾â€ï¼Œç›´è§‚ç†è§£ä¸ºï¼š</p>
  <ul>
    <li>ç”¨å½“å‰å·²æœ‰çš„ä¼°è®¡å»æ”¹è¿›è‡ªèº«</li>
    <li>å®ç°åœ¨çº¿ã€é€æ­¥æ›´æ–°ï¼Œä¸ä¾èµ–æ•´å›åˆå®Œæ•´ä¿¡æ¯</li>
    <li>ä¼˜ç‚¹ï¼šæ”¶æ•›å¿«ã€å¯åœ¨çº¿å­¦ä¹ ï¼›ç¼ºç‚¹ï¼šåˆå§‹ä¼°è®¡ä¸å‡†å¯èƒ½å¼•å…¥åå·®</li>
  </ul>

  <h3>å…«ã€MC vs TD vs DP ä¸‰è€…å¯¹æ¯”ï¼ˆæ–¹å·®-åå·®æƒè¡¡ï¼‰</h3>
  <table>
    <tr><th>æ–¹æ³•</th><th>æ›´æ–°ä¾æ®</th><th>æ˜¯å¦éœ€è¦æ¨¡å‹</th><th>æ–¹å·®</th><th>åå·®</th><th>æ”¶æ•›æ€§</th></tr>
    <tr><td>åŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰</td><td>è´å°”æ›¼æ–¹ç¨‹ï¼ˆç†è®ºï¼‰</td><td>æ˜¯ï¼ˆéœ€è¦Pã€Rï¼‰</td><td>0</td><td>0</td><td>å·²çŸ¥æœ€ä¼˜ï¼Œä½†è®¡ç®—å¤æ‚</td></tr>
    <tr><td>è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰</td><td>å®Œæ•´å›åˆå›æŠ¥</td><td>å¦ï¼ˆæ— æ¨¡å‹ï¼‰</td><td>é«˜</td><td>0ï¼ˆæ— åï¼‰</td><td>å¿…é¡»ç­‰å›åˆç»“æŸ</td></tr>
    <tr><td>æ—¶é—´å·®åˆ†ï¼ˆTDï¼‰</td><td>TDè¯¯å·®ï¼ˆä¸€æ­¥ï¼‰</td><td>å¦ï¼ˆæ— æ¨¡å‹ï¼‰</td><td>ä½</td><td>æœ‰å</td><td>åœ¨çº¿æ›´æ–°ï¼Œå¿«é€Ÿæ”¶æ•›</td></tr>
  </table>

  <p><strong>å…³é”®æ´å¯Ÿï¼š</strong>TD é€‰æ‹©äº†æœ€å¥½çš„æŠ˜è¡·â€”â€”è™½ç„¶å¼•å…¥äº†åå·®ï¼ˆå› ä¸º V çš„åˆå§‹ä¼°è®¡ä¸å®Œç¾ï¼‰ï¼Œä½†æå¤§åœ°é™ä½äº†æ–¹å·®ï¼Œä½¿å¾—åœ¨çº¿å­¦ä¹ æˆä¸ºå¯èƒ½ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ TD æˆä¸ºäº†åç»­æ‰€æœ‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆQ-learningã€Actor-Criticã€PPOç­‰ï¼‰çš„æ ¸å¿ƒã€‚</p>

</section>
<!-- ç¬¬å…«ç«  -->
<section id="chapter8" class="chapter">
  <h2>ç¬¬å…«ç« ï¼šQ-learning</h2>

  <p>Q-learning æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­æœ€ç»å…¸çš„ <strong>å€¼è¿­ä»£ç®—æ³•</strong>ï¼Œç”¨äºå­¦ä¹ æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•° <code>Q*(s,a)</code>ï¼Œä»è€Œé—´æ¥å¾—åˆ°æœ€ä¼˜ç­–ç•¥ï¼š
  </p>
  <p>
  $$
  \pi^*(s) = \arg\max_a Q^*(s,a)
  $$
  </p>
  <p>ç‰¹ç‚¹ï¼š</p>
  <ul>
    <li>å±äº <strong>off-policy</strong> ç®—æ³•ï¼šæ›´æ–° Q å€¼æ—¶ä½¿ç”¨æœ€å¤§åŒ–æœªæ¥ Q å€¼ï¼Œè€Œè¡Œä¸ºç­–ç•¥å¯ä»¥å¸¦æ¢ç´¢</li>
    <li>ç›®æ ‡æ˜¯æ‰¾åˆ°èƒ½è·å¾—æœ€å¤§ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±çš„åŠ¨ä½œ</li>
  </ul>

  <h3>ä¸€ã€æ ¸å¿ƒå…¬å¼</h3>
  <p>Q-learning æ ¸å¿ƒæ›´æ–°å…¬å¼ï¼š</p>
  <p>
  $$
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \big[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \big]
  $$
  </p>
  <table>
    <tr><th>ç¬¦å·</th><th>å«ä¹‰</th></tr>
    <tr><td>s<sub>t</sub>, a<sub>t</sub></td><td>å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œ</td></tr>
    <tr><td>r<sub>t+1</sub></td><td>å³æ—¶å¥–åŠ±</td></tr>
    <tr><td>s<sub>t+1</sub></td><td>ä¸‹ä¸€çŠ¶æ€</td></tr>
    <tr><td>Î±</td><td>å­¦ä¹ ç‡ï¼ˆ0.1~0.5ï¼Œä¸€èˆ¬è®¾ç½®ï¼‰</td></tr>
    <tr><td>Î³</td><td>æŠ˜æ‰£å› å­ï¼ˆ0.9~0.99ï¼Œä¸€èˆ¬è®¾ç½®ï¼‰</td></tr>
    <tr><td>max Q(s<sub>t+1</sub>,a')</td><td>æœªæ¥çŠ¶æ€æœ€ä¼˜åŠ¨ä½œä»·å€¼</td></tr>
  </table>

  <h3>äºŒã€ç®—æ³•æµç¨‹</h3>
  <p><strong>Q-learning æ˜¯ Off-policy æ–¹æ³•ï¼Œè¡Œä¸ºç­–ç•¥å¯ä»¥æ¢ç´¢ï¼Œä½†å­¦ä¹ çš„æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚</strong></p>
  <ol>
    <li><strong>åˆå§‹åŒ–ï¼š</strong>å¯¹æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹ (s,a)ï¼Œè®¾ Q(s,a) = 0</li>
    <li><strong>ç­–ç•¥é€‰æ‹©ï¼š</strong>ä½¿ç”¨ Îµ-greedy ç­–ç•¥ä»çŠ¶æ€ s é€‰æ‹©åŠ¨ä½œ a
      <ul>
        <li>ä»¥æ¦‚ç‡ Îµ éšæœºé€‰æ‹©ï¼ˆæ¢ç´¢ï¼‰</li>
        <li>ä»¥æ¦‚ç‡ 1-Îµ é€‰æ‹© Q å€¼æœ€å¤§çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰</li>
      </ul>
    </li>
    <li><strong>æ‰§è¡Œå¹¶è§‚å¯Ÿï¼š</strong>æ‰§è¡ŒåŠ¨ä½œ aï¼Œå¾—åˆ°å¥–åŠ± r å’Œä¸‹ä¸€çŠ¶æ€ s'</li>
    <li><strong>è®¡ç®—ç›®æ ‡ Q å€¼ï¼š</strong>y = r + Î³Â·max_{a'} Q(s',a')
      <ul>
        <li>è™½ç„¶æˆ‘ä»¬ç”¨ Îµ-greedy æ¢ç´¢ï¼Œä½†è¿™é‡Œå– max è¯´æ˜æˆ‘ä»¬æœç€æœ€ä¼˜ç­–ç•¥å­¦ä¹ </li>
      </ul>
    </li>
    <li><strong>æ›´æ–° Q å€¼ï¼š</strong>Q(s,a) â† Q(s,a) + Î±Â·(y - Q(s,a))</li>
    <li><strong>é‡å¤ï¼š</strong>s â† s'ï¼Œç»§ç»­äº¤äº’å’Œæ›´æ–°</li>
  </ol>
  <p style="margin-top: 15px;"><strong>å­¦ä¹ ç‡ Î±ã€æŠ˜æ‰£å› å­ Î³ã€æ¢ç´¢ç‡ Îµ çš„ä½œç”¨ï¼š</strong></p>
  <ul>
    <li>Î±ï¼ˆå­¦ä¹ ç‡ï¼‰ï¼š0.1~0.5ï¼Œæ§åˆ¶å¯¹æ–°ä¿¡æ¯çš„å¸æ”¶ç¨‹åº¦</li>
    <li>Î³ï¼ˆæŠ˜æ‰£å› å­ï¼‰ï¼š0.9~0.99ï¼Œå†³å®šå¯¹æœªæ¥å¥–åŠ±çš„é‡è§†ç¨‹åº¦</li>
    <li>Îµï¼ˆæ¢ç´¢ç‡ï¼‰ï¼šé€šå¸¸ä» 1.0 è¡°å‡åˆ° 0.01~0.1ï¼Œé€æ­¥ä»æ¢ç´¢è½¬å‘åˆ©ç”¨</li>
  </ul>

  <h3>ä¸‰ã€ç‰¹ç‚¹ä¸æ³¨æ„äº‹é¡¹</h3>
  <ul>
    <li>é€‚ç”¨åœºæ™¯ï¼šç¦»æ•£çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å°çš„ä»»åŠ¡</li>
    <li>æ¢ç´¢ç­–ç•¥ï¼šÎµ-greedy æˆ– Boltzmannï¼Œå¯éšè®­ç»ƒè¡°å‡ Îµ</li>
    <li>å­¦ä¹ ç‡ Î±ï¼šä¸€èˆ¬ 0.1~0.5ï¼Œå¤ªå¤§ä¸ç¨³å®šï¼Œå¤ªå°æ”¶æ•›æ…¢</li>
    <li>æŠ˜æ‰£å› å­ Î³ï¼šä¸€èˆ¬ 0.9~0.99</li>
    <li>æ”¶æ•›æ€§ï¼šæ¯ä¸ªçŠ¶æ€åŠ¨ä½œå¯¹è¢«å……åˆ†è®¿é—®ï¼Œä¸” Î± è¡°å‡æ—¶å¯æ”¶æ•›</li>
  </ul>

  <h3>å››ã€Python ç¤ºä¾‹</h3>
  <pre><code class="language-python">
import numpy as np
import gym

class QLearning:
    """Q-learning ç®—æ³•å®ç°"""
    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.n_states = n_states
        self.n_actions = n_actions
        self.alpha = alpha  # å­¦ä¹ ç‡
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # æ¢ç´¢ç‡
        self.Q = np.zeros((n_states, n_actions))
    
    def select_action(self, state):
        """Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)
        else:
            return np.argmax(self.Q[state])
    
    def update(self, state, action, reward, next_state, done):
        """Q å€¼æ›´æ–°"""
        target = reward + self.gamma * np.max(self.Q[next_state]) * (1 - done)
        td_error = target - self.Q[state, action]
        self.Q[state, action] += self.alpha * td_error
    
    def train(self, env, n_episodes=2000, max_steps=100):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        for episode in range(n_episodes):
            state = env.reset()
            if isinstance(state, tuple):
                state = state[0]  # å…¼å®¹æ–°ç‰ˆ gym
            done = False
            
            for _ in range(max_steps):
                action = self.select_action(state)
                result = env.step(action)
                next_state = result[0]
                reward = result[1]
                done = result[2]
                
                self.update(state, action, reward, next_state, done)
                state = next_state
                if done:
                    break
        
        return self.Q


# ä½¿ç”¨ç¤ºä¾‹
env = gym.make('FrozenLake-v1', is_slippery=False)
n_states = env.observation_space.n
n_actions = env.action_space.n

agent = QLearning(n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1)
Q_table = agent.train(env, n_episodes=2000, max_steps=100)

print("è®­ç»ƒå®Œæˆï¼")
print("Q è¡¨å½¢çŠ¶ï¼š", Q_table.shape)
print("Q è¡¨ï¼š\n", Q_table)
  </code></pre>

  <h3>äº”ã€å‚æ•°è¯´æ˜</h3>
  <table>
    <tr><th>å‚æ•°</th><th>å»ºè®®èŒƒå›´</th><th>è¯´æ˜</th></tr>
    <tr><td>Î±ï¼ˆå­¦ä¹ ç‡ï¼‰</td><td>0.1~0.5</td><td>æ§åˆ¶ Q æ›´æ–°å¹…åº¦</td></tr>
    <tr><td>Î³ï¼ˆæŠ˜æ‰£å› å­ï¼‰</td><td>0.9~0.99</td><td>é•¿æœŸå¥–åŠ±é‡è¦æ€§</td></tr>
    <tr><td>Îµï¼ˆæ¢ç´¢ç‡ï¼‰</td><td>0.05~0.2</td><td>Îµ-greedy æ¢ç´¢æ¦‚ç‡ï¼Œå¯éšè®­ç»ƒè¡°å‡</td></tr>
    <tr><td>n_episodes</td><td>500~5000</td><td>æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´</td></tr>
    <tr><td>max_steps</td><td>100~1000</td><td>æ¯å›åˆæœ€å¤§æ­¥æ•°ï¼Œé¿å…æ— é™å¾ªç¯</td></tr>
  </table>

</section>
<!-- ç¬¬ä¹ç«  -->
<section id="chapter9" class="chapter">
  <h2>ç¬¬ä¹ç« ï¼šDQNï¼ˆDeep Q-Networkï¼‰</h2>

  <p>DQNï¼ˆDeep Q-Networkï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ç”¨äºé«˜ç»´çŠ¶æ€ç©ºé—´çš„ç»å…¸ç®—æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ç”¨<strong>æ·±åº¦ç¥ç»ç½‘ç»œ</strong>é€¼è¿‘åŠ¨ä½œä»·å€¼å‡½æ•° Q(s,a;Î¸)ï¼Œä»è€Œé—´æ¥å¾—åˆ°æœ€ä¼˜ç­–ç•¥ã€‚</p>
  
  <h3>ä¸€ã€èƒŒæ™¯ä¸åŸç†</h3>
  <ul>
    <li>ä¼ ç»Ÿ Q-learning åœ¨å¤§æˆ–è¿ç»­çŠ¶æ€ç©ºé—´ä¸­æ— æ³•ä½¿ç”¨ Q è¡¨</li>
    <li>DQN ä½¿ç”¨ç¥ç»ç½‘ç»œå°†çŠ¶æ€æ˜ å°„åˆ°åŠ¨ä½œä»·å€¼</li>
    <li>é€‚ç”¨äºé«˜ç»´è¾“å…¥ï¼Œå¦‚å›¾åƒç¯å¢ƒï¼ˆAtari æ¸¸æˆï¼‰</li>
    <li>æ ¸å¿ƒæŒ‘æˆ˜ï¼šè®­ç»ƒç¨³å®šæ€§ï¼Œéœ€è¦ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œ</li>
  </ul>

 <h3>æ·±åº¦å­¦ä¹ åŸºç¡€ï¼šç¥ç»ç½‘ç»œå±‚è¯¦è§£</h3>
<p>åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç¥ç»ç½‘ç»œæ˜¯æ™ºèƒ½ä½“çš„â€œå¤§è„‘â€ï¼Œä¸åŒå±‚ï¼ˆLayerï¼‰æ‰¿æ‹…ä¸åŒçš„åŠŸèƒ½è§’è‰²ï¼š</p>

<h4>1. è¾“å…¥å±‚ï¼ˆInput Layerï¼‰</h4>
<ul>
  <li><strong>ä½œç”¨</strong>ï¼šæ¥æ”¶æ¥è‡ªç¯å¢ƒçš„çŠ¶æ€è¾“å…¥ã€‚</li>
  <li><strong>ç¤ºä¾‹</strong>ï¼šCartPole çš„ 4 ç»´å‘é‡ [ä½ç½®, é€Ÿåº¦, æ†è§’, è§’é€Ÿåº¦]ï¼›Atari çš„å›¾åƒå¸§ [84Ã—84Ã—4]ã€‚</li>
</ul>

<h4>2. çº¿æ€§å±‚ï¼ˆLinear / å…¨è¿æ¥å±‚ï¼‰</h4>
<ul>
  <li><strong>PyTorch</strong>ï¼š<code>nn.Linear(in_features, out_features)</code></li>
  <li><strong>å…¬å¼</strong>ï¼šy = Wx + b</li>
  <li><strong>ä½œç”¨</strong>ï¼šå¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œæ˜¯ MLPã€Q ç½‘ç»œã€Actor-Critic ç­‰æ¨¡å‹çš„æ ¸å¿ƒç»„æˆã€‚</li>
</ul>

<h4>3. æ¿€æ´»å±‚ï¼ˆActivation Layerï¼‰</h4>
<p>ä¸ºç½‘ç»œå¼•å…¥éçº¿æ€§ï¼Œä½¿æ¨¡å‹èƒ½æ‹Ÿåˆå¤æ‚å‡½æ•°æ˜ å°„ï¼š</p>
<table>
  <tr><th>åç§°</th><th>å…¬å¼</th><th>è¾“å‡ºèŒƒå›´</th><th>å¸¸è§ç”¨é€”</th></tr>
  <tr><td>ReLU</td><td>max(0, x)</td><td>[0, âˆ)</td><td>å¸¸è§„éšè—å±‚æ¿€æ´»</td></tr>
  <tr><td>Tanh</td><td>(e^x - e^(-x))/(e^x + e^(-x))</td><td>[-1, 1]</td><td>è¿ç»­åŠ¨ä½œè¾“å‡ºï¼ˆDDPGã€SACï¼‰</td></tr>
  <tr><td>Softmax</td><td>e^(x_i)/Î£e^(x_j)</td><td>[0,1]</td><td>ç¦»æ•£åŠ¨ä½œç­–ç•¥ Ï€(a|s)</td></tr>
</table>

<h4>4. å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰</h4>
<ul>
  <li><strong>PyTorch</strong>ï¼š<code>nn.Conv2d(in_ch, out_ch, kernel_size, stride)</code></li>
  <li><strong>ä½œç”¨</strong>ï¼šæå–å›¾åƒçš„å±€éƒ¨ç©ºé—´ç‰¹å¾ï¼Œæ˜¯è§†è§‰è¾“å…¥ä»»åŠ¡ï¼ˆAtari ç­‰ï¼‰çš„åŸºç¡€ã€‚</li>
</ul>
<pre><code class="language-python">
nn.Conv2d(4, 32, 8, stride=4)
nn.ReLU()
nn.Conv2d(32, 64, 4, stride=2)
nn.ReLU()
nn.Conv2d(64, 64, 3, stride=1)
nn.ReLU()
nn.Flatten()
nn.Linear(3136, 512)
</code></pre>

<h4>5. è¾“å‡ºå±‚ï¼ˆOutput Layerï¼‰</h4>
<p>ç”Ÿæˆæœ€ç»ˆè¾“å‡ºï¼Œå½¢å¼å› ä»»åŠ¡è€Œå¼‚ï¼š</p>
<table>
  <tr><th>ä»»åŠ¡ç±»å‹</th><th>è¾“å‡ºç»´åº¦</th><th>æ¿€æ´»å‡½æ•°</th><th>è¾“å‡ºå«ä¹‰</th></tr>
  <tr><td>ç¦»æ•£åŠ¨ä½œç­–ç•¥ Ï€(a|s)</td><td>åŠ¨ä½œæ•°</td><td>Softmax</td><td>åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ</td></tr>
  <tr><td>è¿ç»­åŠ¨ä½œç­–ç•¥ Î¼(s)</td><td>åŠ¨ä½œç»´åº¦</td><td>Tanh</td><td>åŠ¨ä½œå‡å€¼</td></tr>
  <tr><td>ä»·å€¼å‡½æ•° V(s)</td><td>1</td><td>æ— </td><td>çŠ¶æ€ä»·å€¼</td></tr>
  <tr><td>Q å‡½æ•° Q(s,a)</td><td>åŠ¨ä½œæ•°</td><td>æ— </td><td>å„åŠ¨ä½œQå€¼</td></tr>
</table>

<h4>6. å…¶ä»–å¸¸è§å±‚</h4>
<ul>
  <li><strong>å¾ªç¯å±‚ï¼ˆRNN / LSTM / GRUï¼‰</strong>ï¼šå¤„ç†æ—¶é—´åºåˆ—æˆ–éƒ¨åˆ†å¯è§‚æµ‹ä»»åŠ¡ï¼ˆPOMDPï¼‰ã€‚</li>
  <li><strong>å½’ä¸€åŒ–å±‚ï¼ˆBatchNorm / LayerNormï¼‰</strong>ï¼šç¨³å®šè®­ç»ƒã€åŠ å¿«æ”¶æ•›ã€‚</li>
  <li><strong>Dropout å±‚</strong>ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆå¼ºåŒ–å­¦ä¹ ä¸­è¾ƒå°‘ä½¿ç”¨ï¼‰ã€‚</li>
  <li><strong>Flatten å±‚</strong>ï¼šå°†å·ç§¯è¾“å‡ºå±•å¹³ä¸ºå‘é‡ï¼Œè¿æ¥ CNN ä¸å…¨è¿æ¥å±‚ã€‚</li>
</ul>

<h4>æ€»ç»“</h4>
<p>
å‘é‡ä»»åŠ¡ä»¥å…¨è¿æ¥å±‚ä¸ºä¸»ï¼Œå›¾åƒä»»åŠ¡ç”±å·ç§¯+Flatten+çº¿æ€§å±‚æ„æˆï¼Œåºåˆ—ä»»åŠ¡å¼•å…¥å¾ªç¯ç»“æ„ã€‚<br>
éšè—å±‚æå–ç‰¹å¾ï¼Œå·ç§¯å±‚æ•è·å±€éƒ¨æ¨¡å¼ï¼Œå¾ªç¯å±‚å»ºæ¨¡æ—¶é—´ä¾èµ–ã€‚æ‰€æœ‰å±‚åä½œï¼Œå…±åŒæ„æˆæ™ºèƒ½ä½“çš„â€œæ€ç»´ä½“ç³»â€ã€‚
</p>

  <h3>äºŒã€æ ¸å¿ƒå…¬å¼</h3>
  <p>DQN æœ€å°åŒ– TD è¯¯å·®æŸå¤±ï¼š</p>
  <p>$$
  L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \Big[ \big( y - Q(s,a;\theta) \big)^2 \Big]
  $$</p>
  <p>å…¶ä¸­ç›®æ ‡ Q å€¼ä¸ºï¼š</p>
  <p>$$
  y = r + \gamma \max_{a'} Q(s',a'; \theta^-)
  $$</p>
  <ul>
    <li>Q(s,a;Î¸)ï¼šå½“å‰ Q ç½‘ç»œé¢„æµ‹</li>
    <li>yï¼šç›®æ ‡ Q å€¼ï¼Œç”±ç›®æ ‡ç½‘ç»œ Î¸â» æä¾›</li>
    <li>ğ’Ÿï¼šç»éªŒå›æ”¾æ± </li>
    <li>Î³ï¼šæŠ˜æ‰£å› å­</li>
  </ul>

  <h3>ä¸‰ã€è®­ç»ƒæµç¨‹</h3>
  <ol>
    <li>åˆå§‹åŒ– Q ç½‘ç»œ Q(s,a;Î¸) å’Œç›®æ ‡ç½‘ç»œ Q(s,a;Î¸â»)</li>
    <li>ä¸ç¯å¢ƒäº¤äº’ï¼ŒæŒ‰ Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œï¼Œå­˜å…¥ç»éªŒå›æ”¾æ± </li>
    <li>ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·å°æ‰¹é‡æ•°æ®è®­ç»ƒ Q ç½‘ç»œï¼Œæœ€å°åŒ– TD loss</li>
    <li>å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ Î¸â» â† Î¸</li>
    <li>é‡å¤ä»¥ä¸Šæ­¥éª¤ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›</li>
  </ol>

  <h3>å››ã€æ ¸å¿ƒæŠ€æœ¯ç‚¹</h3>
  <table>
    <tr><th>æŠ€æœ¯</th><th>ä½œç”¨</th></tr>
    <tr><td>ç»éªŒå›æ”¾ Replay Buffer</td><td>æ‰“ç ´æ•°æ®ç›¸å…³æ€§ï¼Œæé«˜æ ·æœ¬åˆ©ç”¨ç‡ï¼Œç¨³å®šè®­ç»ƒ</td></tr>
    <tr><td>ç›®æ ‡ç½‘ç»œ Target Network</td><td>å‡ç¼“è®­ç»ƒéœ‡è¡ï¼Œä¿è¯ç›®æ ‡ç¨³å®š</td></tr>
    <tr><td>Îµ-greedy ç­–ç•¥</td><td>å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨</td></tr>
    <tr><td>å½’ä¸€åŒ– / å›¾åƒé¢„å¤„ç†</td><td>é™ä½é«˜ç»´è¾“å…¥ç»´åº¦ï¼Œæå‡è®­ç»ƒæ•ˆç‡</td></tr>
  </table>

  <h3>äº”ã€DQN ç¤ºä¾‹ä»£ç </h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple

Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))

class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.memory = deque(maxlen=capacity)
    
    def add(self, state, action, reward, next_state, done):
        self.memory.append(Transition(state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        return zip(*batch)
    
    def __len__(self):
        return len(self.memory)

class DQN:
    def __init__(self, state_dim, action_dim, gamma=0.99, epsilon=1.0, alpha=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.min_epsilon = 0.01
        self.epsilon_decay = 0.995
        
        # Qç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.Q = QNetwork(state_dim, action_dim)
        self.Q_target = QNetwork(state_dim, action_dim)
        self.Q_target.load_state_dict(self.Q.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.Q.parameters(), lr=alpha)
        self.replay_buffer = ReplayBuffer(capacity=10000)
        self.loss_fn = nn.MSELoss()
    
    def select_action(self, state):
        """Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            with torch.no_grad():
                q_values = self.Q(state_tensor)
            return q_values.argmax(dim=1).item()
    
    def train_step(self, batch_size):
        """ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·å¹¶è®­ç»ƒ"""
        if len(self.replay_buffer) < batch_size:
            return
        
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
        
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(np.array(actions))
        rewards = torch.FloatTensor(np.array(rewards))
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(np.array(dones))
        
        # è®¡ç®—å½“å‰Qå€¼
        q_values = self.Q(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            max_next_q = self.Q_target(next_states).max(dim=1)[0]
            target_q = rewards + self.gamma * max_next_q * (1 - dones)
        
        # è®¡ç®—æŸå¤±
        loss = self.loss_fn(q_values, target_q)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, env, num_episodes=1000, batch_size=32, target_update_freq=500):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        rewards = []
        step_count = 0
        
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            episode_reward = 0
            
            while not done:
                # é€‰æ‹©åŠ¨ä½œ
                action = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                # å­˜å‚¨ç»éªŒ
                self.replay_buffer.add(state, action, reward, next_state, done)
                
                # è®­ç»ƒ
                self.train_step(batch_size)
                
                episode_reward += reward
                state = next_state
                step_count += 1
                
                # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
                if step_count % target_update_freq == 0:
                    self.Q_target.load_state_dict(self.Q.state_dict())
            
            rewards.append(episode_reward)
            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Epsilon: {self.epsilon:.3f}")
        
        return rewards
  </code></pre>

  <h3>å…­ã€å‚æ•°è¯´æ˜</h3>
  <table>
    <tr><th>å‚æ•°</th><th>å»ºè®®èŒƒå›´</th><th>è¯´æ˜</th></tr>
    <tr><td>Î³ï¼ˆæŠ˜æ‰£å› å­ï¼‰</td><td>0.95~0.99</td><td>é•¿æœŸå¥–åŠ±æƒé‡</td></tr>
    <tr><td>å­¦ä¹ ç‡ lr</td><td>1e-4~1e-3</td><td>Adam æˆ– RMSProp ä¼˜åŒ–å™¨</td></tr>
    <tr><td>Îµ åˆå€¼</td><td>1.0</td><td>åˆæœŸé«˜æ¢ç´¢</td></tr>
    <tr><td>Îµ æœ€å°å€¼</td><td>0.01~0.1</td><td>ä¿æŒä¸€å®šæ¢ç´¢</td></tr>
    <tr><td>Îµ è¡°å‡ç‡</td><td>0.995~0.999</td><td>é€æ­¥é™ä½ Îµ</td></tr>
    <tr><td>ç»éªŒå›æ”¾å¤§å°</td><td>1e4~1e6</td><td>æ ¹æ®ç¯å¢ƒå¤§å°é€‰æ‹©</td></tr>
    <tr><td>æ‰¹é‡å¤§å° batch_size</td><td>32~64</td><td>æ¯æ¬¡è®­ç»ƒé‡‡æ ·é‡</td></tr>
    <tr><td>ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡</td><td>500~10000 steps</td><td>å®šæœŸåŒæ­¥ Q ç½‘ç»œåˆ°ç›®æ ‡ç½‘ç»œ</td></tr>
  </table>

  <h3>ä¸ƒã€è®­ç»ƒå®Œæˆåˆ¤æ–­</h3>
  <ul>
    <li><strong>ç´¯è®¡å¥–åŠ±ç¨³å®šï¼š</strong>è¿ç»­è‹¥å¹²å›åˆå¹³å‡å¥–åŠ±å˜åŒ–å¾ˆå°</li>
    <li><strong>æµ‹è¯•ç¯å¢ƒè¡¨ç°ï¼š</strong>ä½¿ç”¨çº¯åˆ©ç”¨ç­–ç•¥æµ‹è¯•å¹³å‡æˆåŠŸç‡æˆ–å®Œæˆç‡è¾¾åˆ°ç›®æ ‡</li>
    <li><strong>æŸå¤±æ”¶æ•›ï¼š</strong>è§‚å¯Ÿ TD loss åŸºæœ¬ç¨³å®š</li>
    <li><strong>Îµ-greedy æ¢ç´¢ç‡è¾ƒä½ï¼š</strong>ç­–ç•¥ä¸»è¦ä¾èµ–ç½‘ç»œä¼°è®¡åŠ¨ä½œï¼Œæµ‹è¯•è¡¨ç°ç¨³å®š</li>
  </ul>
  <pre><code class="language-python">
# æ¯éš” N å›åˆæµ‹è¯•ç­–ç•¥
if episode % test_interval == 0:
    total_reward = 0
    for _ in range(test_episodes):
        state = env.reset()
        done = False
        while not done:
            action = np.argmax(Q.predict(state))
            state, reward, done, _ = env.step(action)
            total_reward += reward
    avg_reward = total_reward / test_episodes
    if avg_reward >= target_reward:
        print("è®­ç»ƒå®Œæˆï¼Œç­–ç•¥æ”¶æ•›")
        break
  </code></pre>

</section>

<!-- ç¬¬åç«  -->
<section id="chapter10" class="chapter">
  <h2>ç¬¬åç« ï¼šDDQNï¼ˆDouble Deep Q-Networkï¼‰</h2>

  <p>DDQN æ˜¯ DQN çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç”¨äºè§£å†³ DQN åœ¨è®­ç»ƒä¸­å­˜åœ¨çš„<strong>Q å€¼è¿‡ä¼°è®¡ï¼ˆOverestimationï¼‰</strong>é—®é¢˜ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å°†åŠ¨ä½œé€‰æ‹©ä¸åŠ¨ä½œè¯„ä»·åˆ†å¼€ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚</p>

  <h3>ä¸€ã€èƒŒæ™¯ä¸åŸç†</h3>
  <ul>
    <li>DQN ä½¿ç”¨ max(Q) åŒæ—¶é€‰æ‹©æœ€ä¼˜åŠ¨ä½œå’Œä¼°è®¡åŠ¨ä½œä»·å€¼ï¼Œå®¹æ˜“é«˜ä¼° Q å€¼ã€‚</li>
    <li>DDQN å°†åŠ¨ä½œé€‰æ‹©å’ŒåŠ¨ä½œè¯„ä»·åˆ†å¼€ï¼š<strong>ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ</strong>ï¼Œ<strong>ç›®æ ‡ç½‘ç»œè¯„ä»·åŠ¨ä½œä»·å€¼</strong>ã€‚</li>
    <li>è¿™æ ·å¯ä»¥é™ä½ max æ“ä½œå¯¹å¶ç„¶é«˜ä¼°çš„æ”¾å¤§æ•ˆåº”ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚</li>
  </ul>

  <h3>äºŒã€æ ¸å¿ƒå…¬å¼</h3>
  <p>DDQN ç›®æ ‡ Q å€¼è®¡ç®—ï¼š</p>
  <p>$$
  y^{DDQN} = r + \gamma Q(s', \arg\max_{a'} Q(s', a'; \theta), \theta^-)
  $$</p>
  <ul>
    <li>åŠ¨ä½œé€‰æ‹©ä½¿ç”¨ä¸»ç½‘ç»œå‚æ•° <code>Î¸</code></li>
    <li>åŠ¨ä½œè¯„ä»·ä½¿ç”¨ç›®æ ‡ç½‘ç»œå‚æ•° <code>Î¸^-</code></li>
    <li>æŸå¤±å‡½æ•°ä¸ DQN ä¸€è‡´ï¼š
      <p>$$
      L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \Big[ (y^{DDQN} - Q(s,a;\theta))^2 \Big]
      $$</p>
    </li>
  </ul>

  <h3>ä¸‰ã€è®­ç»ƒæµç¨‹</h3>
  <ol>
    <li>åˆå§‹åŒ–ä¸»ç½‘ç»œ Q(s,a;Î¸) å’Œç›®æ ‡ç½‘ç»œ Q(s,a;Î¸â»)</li>
    <li>ä¸ç¯å¢ƒäº¤äº’ï¼Œä½¿ç”¨ Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œï¼Œå­˜å…¥ç»éªŒå›æ”¾æ± </li>
    <li>ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·å°æ‰¹é‡è®­ç»ƒï¼Œä½¿ç”¨ DDQN ç›®æ ‡æ›´æ–°ä¸»ç½‘ç»œå‚æ•° Î¸</li>
    <li>å®šæœŸåŒæ­¥ç›®æ ‡ç½‘ç»œ Î¸â» â† Î¸</li>
    <li>é‡å¤ä»¥ä¸Šæ­¥éª¤ï¼Œç›´åˆ°è®­ç»ƒå®Œæˆ</li>
  </ol>

  <h3>å››ã€DDQN ä¸ DQN åŒºåˆ«</h3>
  <table>
    <tr><th>ç‰¹æ€§</th><th>DQN</th><th>DDQN</th></tr>
    <tr><td>ç›®æ ‡è®¡ç®—</td><td>y = r + Î³ max_a Q(s',a;Î¸â»)</td><td>y = r + Î³ Q(s', argmax_a Q(s',a;Î¸), Î¸â»)</td></tr>
    <tr><td>é€‰æ‹©åŠ¨ä½œ</td><td>ç›®æ ‡ç½‘ç»œ</td><td>ä¸»ç½‘ç»œ</td></tr>
    <tr><td>è¯„ä»·åŠ¨ä½œ</td><td>ç›®æ ‡ç½‘ç»œ</td><td>ç›®æ ‡ç½‘ç»œ</td></tr>
    <tr><td>è¿‡ä¼°è®¡é—®é¢˜</td><td>å­˜åœ¨</td><td>å¤§å¹…å‡è½»</td></tr>
    <tr><td>è®­ç»ƒç¨³å®šæ€§</td><td>ä¸€èˆ¬</td><td>æ›´ç¨³</td></tr>
  </table>

  <h3>äº”ã€DDQN ç¤ºä¾‹ä»£ç </h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

class DDQN:
    def __init__(self, state_dim, action_dim, gamma=0.99, epsilon=1.0, alpha=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.min_epsilon = 0.01
        self.epsilon_decay = 0.995
        
        # Qç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.Q = self._build_network(state_dim, action_dim)
        self.Q_target = self._build_network(state_dim, action_dim)
        self.Q_target.load_state_dict(self.Q.state_dict())
        
        self.optimizer = optim.Adam(self.Q.parameters(), lr=alpha)
        self.replay_buffer = deque(maxlen=10000)
        self.loss_fn = nn.MSELoss()
    
    def _build_network(self, state_dim, action_dim):
        return nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def select_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            q_values = self.Q(state_tensor)
        return q_values.argmax(dim=1).item()
    
    def train_step(self, batch_size):
        if len(self.replay_buffer) < batch_size:
            return
        
        indices = np.random.choice(len(self.replay_buffer), batch_size, replace=False)
        batch = [self.replay_buffer[i] for i in indices]
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(np.array(actions))
        rewards = torch.FloatTensor(np.array(rewards))
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(np.array(dones))
        
        # å½“å‰Qå€¼
        q_values = self.Q(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # DDQNæ ¸å¿ƒï¼šç”¨Qç½‘ç»œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼Œç”¨Q_targetç½‘ç»œè¯„ä¼°ä»·å€¼
        with torch.no_grad():
            # ç”¨Qç½‘ç»œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            next_actions = self.Q(next_states).argmax(dim=1)
            # ç”¨Q_targetç½‘ç»œè¯„ä¼°è¿™ä¸ªåŠ¨ä½œçš„ä»·å€¼
            max_next_q = self.Q_target(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
            target_q = rewards + self.gamma * max_next_q * (1 - dones)
        
        # è®¡ç®—æŸå¤±
        loss = self.loss_fn(q_values, target_q)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, env, num_episodes=1000, batch_size=32, target_update_freq=500):
        rewards = []
        step_count = 0
        
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            episode_reward = 0
            
            while not done:
                action = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                self.replay_buffer.append((state, action, reward, next_state, done))
                self.train_step(batch_size)
                
                episode_reward += reward
                state = next_state
                step_count += 1
                
                if step_count % target_update_freq == 0:
                    self.Q_target.load_state_dict(self.Q.state_dict())
            
            rewards.append(episode_reward)
            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        
        return rewards
  </code></pre>

  <h3>å…­ã€å‚æ•°è¯´æ˜ï¼ˆä¸€èˆ¬è®¾ç½®ï¼‰</h3>
  <table>
    <tr><th>å‚æ•°</th><th>å»ºè®®èŒƒå›´</th><th>è¯´æ˜</th></tr>
    <tr><td>Î³ï¼ˆæŠ˜æ‰£å› å­ï¼‰</td><td>0.95~0.99</td><td>é•¿æœŸå¥–åŠ±æƒé‡</td></tr>
    <tr><td>å­¦ä¹ ç‡ lr</td><td>1e-4~1e-3</td><td>Adam æˆ– RMSProp ä¼˜åŒ–å™¨</td></tr>
    <tr><td>Îµ åˆå€¼</td><td>1.0</td><td>åˆæœŸé«˜æ¢ç´¢</td></tr>
    <tr><td>Îµ æœ€å°å€¼</td><td>0.01~0.1</td><td>ä¿æŒä¸€å®šæ¢ç´¢</td></tr>
    <tr><td>Îµ è¡°å‡ç‡</td><td>0.995~0.999</td><td>é€æ­¥é™ä½ Îµ</td></tr>
    <tr><td>ç»éªŒå›æ”¾å¤§å°</td><td>1e4~1e6</td><td>æ ¹æ®ç¯å¢ƒå¤§å°é€‰æ‹©</td></tr>
    <tr><td>æ‰¹é‡å¤§å° batch_size</td><td>32~64</td><td>æ¯æ¬¡è®­ç»ƒé‡‡æ ·é‡</td></tr>
    <tr><td>ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡</td><td>500~10000 steps</td><td>å®šæœŸåŒæ­¥ Q ç½‘ç»œåˆ°ç›®æ ‡ç½‘ç»œ</td></tr>
  </table>

  <h3>ä¸ƒã€è®­ç»ƒå®Œæˆåˆ¤æ–­</h3>
  åŒä¸Š
  <h3>å…«ã€Q ç½‘ç»œå‚æ•°æ›´æ–°ä¸è¿‡ä¼°è®¡åˆ†æ</h3>
  <ul>
    <li><strong>Q ç½‘ç»œå‚æ•° Î¸ å®æ—¶æ›´æ–°ï¼š</strong>ç”¨äºé€‰æ‹©åŠ¨ä½œï¼Œéšç€æ¢¯åº¦ä¸‹é™ä¸æ–­è®­ç»ƒ</li>
    <li><strong>ç›®æ ‡ç½‘ç»œå‚æ•° Î¸â» éš”ä¸€æ®µæ—¶é—´æ›´æ–°ï¼š</strong>ç”¨äºè¯„ä»·åŠ¨ä½œä»·å€¼ï¼Œä¿æŒç¨³å®šé¿å… max æ“ä½œæŠŠå¶ç„¶é«˜ä¼°æ”¾å¤§</li>
    <li><strong>è®­ç»ƒé€»è¾‘ï¼š</strong>ä¸»ç½‘ç»œå®æ—¶å­¦ â†’ é€‰æ‹©åŠ¨ä½œï¼›ç›®æ ‡ç½‘ç»œç¨³å®š â†’ è¯„ä»·åŠ¨ä½œï¼›å®šæœŸåŒæ­¥ â†’ Î¸â» â† Î¸</li>
    <li>è¿™æ ·åšçš„ç›´è§‰ï¼šä¸»ç½‘ç»œåƒâ€œå†²åŠ¨çš„ç©å®¶â€ï¼Œç›®æ ‡ç½‘ç»œåƒâ€œç¨³é‡çš„è£åˆ¤â€ï¼Œé™ä½ Q å€¼è¿‡ä¼°è®¡ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§</li>
  </ul>

  <h3>ä¹ã€ä»ä»·å€¼å‡½æ•°æ–¹æ³•å‘ç­–ç•¥æ¢¯åº¦è½¬å˜</h3>
  <p><strong>Q-learning ä¸ DDQN çš„å±€é™æ€§ï¼š</strong></p>
  <ul>
    <li>âœ“ ä¼˜åŠ¿ï¼šé€‚åˆç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œç†è®ºå®Œå¤‡</li>
    <li>âœ— åŠ£åŠ¿ï¼šæ— æ³•ç›´æ¥å¤„ç†<strong>è¿ç»­åŠ¨ä½œç©ºé—´</strong>ï¼ˆå¦‚æœºå™¨äººæ§åˆ¶ï¼‰</li>
    <li>âœ— åŠ£åŠ¿ï¼šå¯¹é«˜ç»´çŠ¶æ€ç©ºé—´å®¹æ˜“è¿‡ä¼°è®¡ Q å€¼</li>
    <li>âœ— åŠ£åŠ¿ï¼šé‡‡æ ·æ•ˆç‡ä½ï¼Œéœ€è¦å¤§é‡ç»éªŒå›æ”¾</li>
  </ul>

  <p><strong>ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆç¬¬11-13ç« ï¼‰çš„ä¼˜åŠ¿ï¼š</strong></p>
  <ul>
    <li>âœ“ ç›´æ¥å¯¹ç­–ç•¥ Ï€(a|s;Î¸) å‚æ•°åŒ–ï¼Œè‡ªç„¶æ”¯æŒ<strong>è¿ç»­åŠ¨ä½œç©ºé—´</strong></li>
    <li>âœ“ å¯ä»¥å­¦ä¹ <strong>éšæœºç­–ç•¥</strong>è€Œä¸ä»…æ˜¯ç¡®å®šæ€§ç­–ç•¥</li>
    <li>âœ“ ä½¿ç”¨ Advantage å‡½æ•°ä½œä¸ºåŸºçº¿ï¼Œæ–¹å·®æ›´ä½</li>
    <li>âœ“ æ ·æœ¬æ•ˆç‡é«˜ï¼Œèƒ½å……åˆ†åˆ©ç”¨äº¤äº’æ•°æ®</li>
  </ul>

  <p><strong>ä»"ä¼°è®¡Qå€¼é€‰åŠ¨ä½œ"â†’"ç›´æ¥å­¦ç­–ç•¥"çš„å“²å­¦è½¬å˜ï¼š</strong></p>
  <ul>
    <li><strong>Q-learningï¼š</strong>é€šè¿‡ä¼°è®¡æ¯ä¸ªåŠ¨ä½œçš„ä»·å€¼ï¼Œæ¥é—´æ¥ä¼˜åŒ–ç­–ç•¥</li>
    <li><strong>ç­–ç•¥æ¢¯åº¦ï¼š</strong>ç»•è¿‡ä»·å€¼å‡½æ•°ï¼Œç›´æ¥åœ¨ç­–ç•¥å‚æ•°ç©ºé—´ä¸­æ¢¯åº¦ä¸Šå‡</li>
    <li><strong>Actor-Criticï¼š</strong>ç»“åˆä¸¤è€…ï¼šç”¨ V(s) ä½œä¸ºåŸºçº¿ï¼ˆCriticï¼‰ï¼Œç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼ˆActorï¼‰</li>
  </ul>

</section>
<!-- ç¬¬åä¸€ç«  -->
<section id="chapter11" class="chapter">
  <h2>ç¬¬åä¸€ç« ï¼šç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆPolicy Gradient, PGï¼‰</h2>

  <p>ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç›´æ¥å¯¹ç­–ç•¥ <code>&pi;(a|s;Î¸)</code> å‚æ•°åŒ–ä¼˜åŒ–ï¼Œè€Œä¸æ˜¯å…ˆä¼°è®¡ Q å€¼å†é€‰åŠ¨ä½œã€‚é€‚ç”¨äºåŠ¨ä½œç©ºé—´è¿ç»­æˆ–å¤æ‚çš„ç¯å¢ƒã€‚</p>

  <h3>ä¸€ã€ç­–ç•¥æ¢¯åº¦ç›®æ ‡</h3>
  <p>æœ€å¤§åŒ–æœŸæœ›ç´¯è®¡å¥–åŠ±ï¼š</p>
  <p>$$
  J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]
  $$</p>
  <ul>
    <li><code>Î¸</code>ï¼šç­–ç•¥å‚æ•°</li>
    <li><code>Ï„</code>ï¼šä»ç­–ç•¥ç”Ÿæˆçš„è½¨è¿¹ï¼ˆstate-actionåºåˆ—ï¼‰</li>
    <li><code>R(Ï„)</code>ï¼šè½¨è¿¹æ€»å¥–åŠ±</li>
  </ul>

  <h3>äºŒã€ç­–ç•¥æ¢¯åº¦å®šç†</h3>
  <p>ç­–ç•¥æ¢¯åº¦å…¬å¼ï¼š</p>
  <p>$$
  \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [ \nabla_\theta \log \pi_\theta(a|s) \, A^\pi(s,a) ]
  $$</p>
  <ul>
    <li><code>âˆ‡Î¸ log Ï€Î¸(a|s)</code>ï¼šè°ƒæ•´ç­–ç•¥å‚æ•° Î¸ æ”¹å˜åŠ¨ä½œæ¦‚ç‡çš„æ–¹å‘</li>
    <li><code>AÏ€(s,a)</code>ï¼š<strong>ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantageï¼‰</strong>ï¼Œè¡¡é‡åŠ¨ä½œç›¸å¯¹äºå¹³å‡æ°´å¹³çš„å¥½åç¨‹åº¦</li>
  </ul>

  <h3>ç­–ç•¥æ¢¯åº¦æ¨å¯¼è¯¦è§£</h3>
  <p><strong>ç›´è§‰ï¼šæˆ‘ä»¬åˆ°åº•æƒ³ä¼˜åŒ–ä»€ä¹ˆï¼Ÿ</strong></p>
  <p>åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›è°ƒæ•´ç­–ç•¥å‚æ•° Î¸ï¼Œè®©æœŸæœ›ç´¯è®¡å¥–åŠ±æœ€å¤§ï¼š</p>
  <p>$$J(\theta) = \mathbb{E}_{\pi_\theta}[R]$$</p>
  <p>è¿™å°±åƒç›‘ç£å­¦ä¹ é‡Œæœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œåªä¸è¿‡è¿™é‡Œæ˜¯æœ€å¤§åŒ–å¥–åŠ±æœŸæœ›ã€‚è¦è®© J(Î¸) å˜å¤§ï¼Œæˆ‘ä»¬è¦çŸ¥é“å®ƒå¯¹å‚æ•° Î¸ çš„æ¢¯åº¦æ–¹å‘â€”â€”ä¹Ÿå°±æ˜¯ç­–ç•¥æ¢¯åº¦ã€‚</p>

  <p><strong>æ¨å¯¼è¿‡ç¨‹ï¼šä¸ºä»€ä¹ˆä¼šå‡ºç° log Ï€(a|s) å’Œä¼˜åŠ¿å‡½æ•° A(s,a)ï¼Ÿ</strong></p>
  
  <p><strong>ç¬¬ 1 æ­¥ï¼šä»"æœŸæœ›çš„å¯¼æ•°"å¼€å§‹</strong></p>
  <p>æƒ³æ±‚ âˆ‡Î¸J(Î¸)ï¼Œå¯ä»¥å†™æˆè½¨è¿¹ç§¯åˆ†çš„å½¢å¼ï¼š</p>
  <p>$$\nabla_\theta J(\theta) = \nabla_\theta \int p_\theta(\tau) R(\tau) d\tau$$</p>
  <p>å…¶ä¸­ p<sub>Î¸</sub>(Ï„) æ˜¯ç­–ç•¥ç”Ÿæˆæ•´æ¡è½¨è¿¹çš„æ¦‚ç‡ã€‚</p>
  
  <p><strong>ç¬¬ 2 æ­¥ï¼šç”¨ log trickï¼ˆå¯¹æ•°æŠ€å·§ï¼‰è®©æ¢¯åº¦è¿›åˆ° log é‡Œé¢</strong></p>
  <p>åˆ©ç”¨æ’ç­‰å¼ï¼š</p>
  <p>$$\nabla_\theta p_\theta(\tau) = p_\theta(\tau) \nabla_\theta \log p_\theta(\tau)$$</p>
  <p>äºæ˜¯ï¼š</p>
  <p>$$\nabla_\theta J(\theta) = \int p_\theta(\tau) \nabla_\theta \log p_\theta(\tau) R(\tau) d\tau = \mathbb{E}_{\tau \sim p_\theta}[\nabla_\theta \log p_\theta(\tau) R(\tau)]$$</p>
  
  <p><strong>ç¬¬ 3 æ­¥ï¼šæ‹†è½¨è¿¹æ¦‚ç‡ï¼Œåªå‰©ä¸‹ç­–ç•¥çš„éƒ¨åˆ†</strong></p>
  <p>è½¨è¿¹æ¦‚ç‡ç”±ç¯å¢ƒå’Œç­–ç•¥å…±åŒå†³å®šï¼š</p>
  <p>$$p_\theta(\tau) = p(s_0) \prod_t \pi_\theta(a_t|s_t) p(s_{t+1}|s_t,a_t)$$</p>
  <p>ç¯å¢ƒçš„çŠ¶æ€è½¬ç§» p(s'|s,a) ä¸ä¾èµ– Î¸ï¼Œæ‰€ä»¥åªæœ‰ç­–ç•¥éƒ¨åˆ†éœ€è¦å¯¼æ•°ï¼š</p>
  <p>$$\nabla_\theta \log p_\theta(\tau) = \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t)$$</p>
  <p>äºæ˜¯ï¼š</p>
  <p>$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau)\right]$$</p>
  
  <p><strong>ä¸ºä»€ä¹ˆç”¨ log è€Œä¸æ˜¯ç›´æ¥å¯¹ Ï€(a|s) æ±‚å¯¼ï¼Ÿ</strong></p>
  <ul>
    <li><strong>æ•°å­¦ç¨³å®šæ€§</strong>ï¼šlog æŠŠä¹˜æ³•å˜åŠ æ³•ï¼Œç¨³å®šæ¢¯åº¦ä¼ æ’­</li>
    <li><strong>è®¡ç®—æ•ˆç‡</strong>ï¼šlog åçš„æ¢¯åº¦å½¢å¼æ›´é€‚åˆç¥ç»ç½‘ç»œåå‘ä¼ æ’­</li>
    <li><strong>é‡‡æ ·ä¸€è‡´æ€§</strong>ï¼šæˆ‘ä»¬åªèƒ½é‡‡æ ·åˆ°å…·ä½“åŠ¨ä½œ aï¼Œlog Ï€ è®©æˆ‘ä»¬ç”¨å•ä¸ªæ ·æœ¬ä¼°è®¡æ•´ä¸ªåˆ†å¸ƒçš„æ¢¯åº¦</li>
  </ul>
  
  <p>è¿™å°±æ˜¯ç»å…¸çš„ REINFORCE ç®—æ³•çš„æ•°å­¦æ¥æºï¼</p>

  <h3>ä¸‰ã€Advantageï¼ˆä¼˜åŠ¿å‡½æ•°ï¼‰çš„å®šä¹‰</h3>
  <p>Advantage æ˜¯<strong>åŠ¨ä½œä»·å€¼ä¸çŠ¶æ€ä»·å€¼çš„å·®</strong>ï¼Œè¡¨ç¤ºç›¸å¯¹ä¼˜åŠ¿ï¼š</p>
  <p>$$
  A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
  $$</p>
  <ul>
    <li><code>QÏ€(s,a)</code>ï¼šåœ¨çŠ¶æ€ s æ‰§è¡ŒåŠ¨ä½œ a çš„é¢„æœŸå›æŠ¥</li>
    <li><code>VÏ€(s)</code>ï¼šåœ¨çŠ¶æ€ s éµå¾ªç­–ç•¥çš„é¢„æœŸå›æŠ¥ï¼ˆæ‰€æœ‰åŠ¨ä½œçš„å¹³å‡ï¼‰</li>
    <li><strong>A > 0</strong>ï¼šè¯¥åŠ¨ä½œæ¯”å¹³å‡åŠ¨ä½œæ›´ä¼˜</li>
    <li><strong>A < 0</strong>ï¼šè¯¥åŠ¨ä½œæ¯”å¹³å‡åŠ¨ä½œæ›´å·®</li>
  </ul>
  <p><strong>ç›´è§‚æ„ä¹‰</strong>ï¼šä¸åªçœ‹åŠ¨ä½œæœ¬èº«çš„å¥½åï¼ˆQ å€¼ï¼‰ï¼Œè€Œæ˜¯çœ‹å®ƒç›¸å¯¹äºè¯¥çŠ¶æ€å…¶ä»–åŠ¨ä½œçš„<strong>ç›¸å¯¹ä¼˜åŠ£</strong>ã€‚è¿™æ ·å¯ä»¥å‡å°‘è®­ç»ƒæ–¹å·®ï¼Œæé«˜æ”¶æ•›ç¨³å®šæ€§ã€‚</p>

  <h3>å››ã€æ ¸å¿ƒç›´è§‰</h3>
  <ul>
    <li>å¦‚æœ Advantage é«˜ â†’ å¢åŠ  Ï€(a|s) çš„æ¦‚ç‡</li>
    <li>å¦‚æœ Advantage ä½ â†’ å‡å°‘ Ï€(a|s) çš„æ¦‚ç‡</li>
    <li>å¦‚æœ Advantage æ¥è¿‘ 0 â†’ å‡ ä¹ä¸è°ƒæ•´è¯¥åŠ¨ä½œçš„æ¦‚ç‡</li>
  </ul>
  <p>æ•°å­¦æ¥æºï¼šæ›´æ–°å…¬å¼ <code>Î”Î¸ âˆ âˆ‡Î¸ log Ï€(a|s) * A(s,a)</code>ï¼ŒAdvantage æœ‰æ­£æœ‰è´Ÿï¼Œå¯ä»¥åŒæ—¶æ¨é«˜ä¼˜ç§€åŠ¨ä½œã€å‹ä½åŠ£è´¨åŠ¨ä½œã€‚</p>
  <p>æ¦‚ç‡è§’åº¦ï¼šlog Ï€ æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•è°ƒèŠ‚ Î¸ï¼Œä½¿é€‰æ‹©åŠ¨ä½œæ¦‚ç‡å¢å¤§/å‡å°ï¼›ä¹˜ä¸Š A(s,a) è®©è°ƒæ•´å’ŒåŠ¨ä½œç›¸å¯¹å¥½åæŒ‚é’©ã€‚</p>

  <h3>äº”ã€REINFORCE ç®—æ³•ï¼ˆMonte Carlo Policy Gradientï¼‰</h3>
  <p>REINFORCE ä½¿ç”¨ç´¯è®¡å›æŠ¥ G_t ä½œä¸º Q å€¼çš„æ— åä¼°è®¡ï¼Œä½†æ–¹å·®è¾ƒå¤§ï¼š</p>
  <pre><code class="language-python">
for episode in range(num_episodes):
    states, actions, rewards = run_episode(env, policy)
    G = 0
    returns = []
    # è®¡ç®—å›åˆç´¯è®¡å›æŠ¥ï¼ˆæ— åä¼°è®¡ï¼‰
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    
    # ä½¿ç”¨å›æŠ¥ G ä½œä¸ºåŸºçº¿å‡æ³•çš„å‚è€ƒï¼Œè®¡ç®— Advantage
    baseline = np.mean(returns)  # ç®€å•çš„åŸºçº¿ä¼°è®¡
    for s, a, Gt in zip(states, actions, returns):
        advantage = Gt - baseline  # Advantage = Q(s,a) - V(s) çš„è¿‘ä¼¼
        theta += alpha * grad_log_pi(s, a) * advantage
  </code></pre>

  <h3>å…­ã€ç­–ç•¥æ¢¯åº¦è®­ç»ƒå®Œæˆåˆ¤æ–­</h3>
  <ul>
    <li>ç­–ç•¥ç¨³å®šæ€§ï¼šè¿ç»­è‹¥å¹²å›åˆç­–ç•¥é€‰æ‹©åŠ¨ä½œæ¦‚ç‡å˜åŒ–ä¸å¤§</li>
    <li>å¹³å‡ç´¯è®¡å¥–åŠ±æ”¶æ•›ï¼šæµ‹è¯•ç¯å¢ƒä¸‹å¹³å‡å¥–åŠ±è¾¾åˆ°ç›®æ ‡</li>
  </ul>

  <h3>ä¸ƒã€Advantage çš„å…³é”®æ„ä¹‰</h3>
  <p><strong>ä¸ºä»€ä¹ˆä½¿ç”¨ Advantage è€Œä¸æ˜¯ç›´æ¥ç”¨ Q å€¼ï¼Ÿ</strong></p>
  <ul>
    <li><strong>é™ä½æ–¹å·®</strong>ï¼šAdvantage = Q(s,a) - V(s) æ˜¯ç›¸å¯¹å€¼ï¼Œå‡å°‘äº†ç»å¯¹å€¼çš„æ³¢åŠ¨ã€‚å¦‚æœæ‰€æœ‰åŠ¨ä½œçš„ Q å€¼éƒ½å¾ˆå¤§ï¼ˆæˆ–å¾ˆå°ï¼‰ï¼Œç›´æ¥ç”¨ Q ä¼šå¯¼è‡´é«˜æ–¹å·®ï¼Œç”¨ Advantage åˆ™ç›¸å¯¹ç¨³å®šã€‚</li>
    <li><strong>åŸºçº¿æ•ˆåº”</strong>ï¼šAdvantage æœ¬è´¨ä¸Šæ˜¯ä»¥ V(s) ä½œä¸ºåŸºçº¿ï¼Œè¡¡é‡"ç›¸å¯¹äºå¹³å‡åŠ¨ä½œ"çš„å¥½åï¼Œè€Œä¸æ˜¯"ç»å¯¹"å¥½åã€‚</li>
    <li><strong>åç»­ç®—æ³•çš„åŸºç¡€</strong>ï¼šActor-Criticã€A2Cã€A3Cã€PPO ç­‰æ‰€æœ‰ç°ä»£ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œéƒ½ä»¥ Advantage ä½œä¸ºæ ¸å¿ƒæ¦‚å¿µã€‚</li>
  </ul>

  <p><strong>ç›´è§‚å¯¹æ¯”ï¼š</strong></p>
  <ul>
    <li>ç”¨ Q(s,a)ï¼šåœ¨çŠ¶æ€ sï¼Œæœ‰ä¸¤ä¸ªåŠ¨ä½œ a1ã€a2ï¼ŒQ(a1)=100ï¼ŒQ(a2)=99ï¼Œéƒ½å¾ˆä¼˜ç§€ï¼Œä½†æ›´æ–°æ—¶ä¸¤è€…å¯¹æ¢¯åº¦çš„å½±å“åŠ›é‡å·®å¼‚å°ã€‚</li>
    <li>ç”¨ A(s,a)ï¼šå‡è®¾ V(s)=98ï¼Œåˆ™ A(a1)=+2ï¼ŒA(a2)=+1ï¼ŒAdvantage æ¸…æ™°åœ°åŒºåˆ†äº†"è°æ›´ä¼˜"ï¼Œè¿™ä¹Ÿé™ä½äº†ä¼°è®¡çš„æ–¹å·®ã€‚</li>
  </ul>

</section>
<!-- ç¬¬åäºŒç«  -->
<section id="chapter12" class="chapter">
  <h2>ç¬¬åäºŒç« ï¼šActor-Critic (AC) æ–¹æ³•</h2>

  <p>Actor-Critic æ–¹æ³•æ˜¯ç­–ç•¥æ¢¯åº¦å®¶æ—çš„ä¸€ç§æ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆä»·å€¼å‡½æ•°ï¼ˆCriticï¼‰æ¥é™ä½ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚</p>

  <h3>ä¸€ã€REINFORCE ä¸ Actor-Critic å¯¹æ¯”</h3>
  <table>
    <tr>
      <th>æ–¹æ³•</th>
      <th>Advantage ä¼°è®¡æ–¹å¼</th>
      <th>ç­–ç•¥æ›´æ–°ä¾æ®</th>
      <th>æ›´æ–°é¢‘ç‡</th>
      <th>æ–¹å·®</th>
    </tr>
    <tr>
      <td>REINFORCE</td>
      <td>å›åˆç´¯è®¡å›æŠ¥ G_tï¼ˆé«˜æ–¹å·®ï¼Œæ— åï¼‰</td>
      <td>G_t * âˆ‡ log Ï€(a|s)</td>
      <td>å›åˆç»“æŸ</td>
      <td>é«˜</td>
    </tr>
    <tr>
      <td>Actor-Critic (AC)</td>
      <td>TD è¯¯å·® Î´_t = r + Î³ V(s') - V(s)ï¼ˆä½æ–¹å·®ï¼Œæœ‰åï¼‰</td>
      <td>Î´_t * âˆ‡ log Ï€(a|s)</td>
      <td>æ¯ä¸€æ­¥ï¼ˆåœ¨çº¿ï¼‰</td>
      <td>ä½</td>
    </tr>
  </table>
  <p><strong>æ ¸å¿ƒåŒºåˆ«</strong>ï¼šä¸¤è€…éƒ½åœ¨è®¡ç®— Advantageï¼Œä½†ä¼°è®¡æ–¹å¼ä¸åŒã€‚REINFORCE ç”¨å®Œæ•´å›åˆçš„å›æŠ¥ï¼ˆé«˜æ–¹å·®ï¼‰ï¼ŒAC ç”¨ TD è¯¯å·®ï¼ˆä½æ–¹å·®ï¼Œä½†å¼•å…¥åå·®ï¼‰ã€‚</p>

  <h3>äºŒã€æ ¸å¿ƒæ€æƒ³</h3>
  <ul>
    <li><b>Actor</b>ï¼šç”ŸæˆåŠ¨ä½œç­–ç•¥ Ï€(a|s;Î¸_actor)ï¼Œç›´æ¥è¾“å‡ºåŠ¨ä½œæ¦‚ç‡</li>
    <li><b>Critic</b>ï¼šè¯„ä¼°çŠ¶æ€ä»·å€¼ V(s;Î¸_critic)ï¼Œæä¾› TD è¯¯å·® Î´_t æ¥æŒ‡å¯¼ Actor æ›´æ–°</li>
  </ul>
  <p>ç›´è§‚ç†è§£ï¼šActor æ˜¯â€œå†³ç­–è€…â€ï¼ŒCritic æ˜¯â€œè£åˆ¤â€ï¼Œè£åˆ¤å‘Šè¯‰å†³ç­–è€…åŠ¨ä½œå¥½åï¼ŒActor æ ¹æ®åé¦ˆè°ƒæ•´ç­–ç•¥ã€‚</p>

  <h3>ä¸‰ã€Advantage å‡½æ•°ä¸ TD è¯¯å·®çš„æ•°å­¦æ¨å¯¼</h3>
  
  <h4>ä¸€ã€Advantage å‡½æ•°çš„ç†è®ºå®šä¹‰</h4>
  <p>Advantage å‡½æ•°å®šä¹‰ä¸ºï¼š</p>
  <p>$$
  A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)
  $$</p>
  <p>å®ƒè¡¨ç¤ºåœ¨çŠ¶æ€ $s_t$ ä¸‹é‡‡å–åŠ¨ä½œ $a_t$ï¼Œç›¸å¯¹äºè¯¥çŠ¶æ€å¹³å‡æ°´å¹³ $V^\pi(s_t)$ çš„"ç›¸å¯¹ä¼˜åŠ¿"ã€‚</p>
  <p>å¦‚æœæ‰¾åˆ° $Q^\pi(s_t, a_t)$ çš„è¿‘ä¼¼å¼ï¼Œå°±èƒ½å¾—åˆ° $A^\pi(s_t, a_t)$ çš„è¿‘ä¼¼ã€‚</p>

  <h4>äºŒã€å±•å¼€ Q å‡½æ•°çš„å®šä¹‰</h4>
  <p>Q å‡½æ•°å®šä¹‰ä¸ºï¼š</p>
  <p>$$
  Q^\pi(s_t, a_t) = \mathbb{E}_\pi[r_t + \gamma V^\pi(s_{t+1})]
  $$</p>
  <p>è¿™æ˜¯å› ä¸ºçŠ¶æ€ä»·å€¼å‡½æ•°æ»¡è¶³ Bellman æ–¹ç¨‹ï¼š$V^\pi(s_t) = \mathbb{E}_{a_t \sim \pi}[Q^\pi(s_t, a_t)]$</p>

  <h4>ä¸‰ã€ä»£å…¥ Advantage å®šä¹‰</h4>
  <p>å°† $Q^\pi(s_t, a_t)$ çš„å±•å¼€å¼ä»£å…¥ Advantage å®šä¹‰ï¼š</p>
  <p>$$
  A^\pi(s_t, a_t) = \mathbb{E}_\pi[r_t + \gamma V^\pi(s_{t+1})] - V^\pi(s_t)
  $$</p>
  <p>åœ¨å®é™…è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€æ¬¡é‡‡æ ·æ¥è¿‘ä¼¼æœŸæœ›ï¼ˆå³ä½¿ç”¨å½“å‰é‡‡æ ·çš„å®é™…å¥–åŠ± $r_t$ ä»£æ›¿æœŸæœ›ï¼‰ï¼Œå¾—åˆ°ï¼š</p>
  <p>$$
  A^\pi(s_t, a_t) \approx r_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)
  $$</p>

  <h4>å››ã€è¿™æ­£æ˜¯ TD è¯¯å·®çš„å®šä¹‰</h4>
  <p>TD è¯¯å·®ï¼ˆTemporal Difference Errorï¼‰å®šä¹‰ä¸ºï¼š</p>
  <p>$$
  \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
  $$</p>
  <p><strong>å…³é”®ç»“è®º</strong>ï¼š</p>
  <p>$$
  A^\pi(s_t, a_t) \approx \delta_t
  $$</p>
  <p><strong>å³ TD è¯¯å·®å°±æ˜¯ Advantage çš„ä¸€æ¬¡é‡‡æ ·ä¼°è®¡ã€‚</strong></p>

  <h4>äº”ã€ä¸ºä»€ä¹ˆè¿™æ ·åšåˆç†ï¼Ÿ</h4>
  <p><b>â‘  TD è¯¯å·®æ˜¯ Advantage çš„æ— åä¼°è®¡</b></p>
  <p>åªè¦ $V(s)$ è¿‘ä¼¼ $V^\pi(s)$ï¼Œé‚£ä¹ˆ TD è¯¯å·®çš„æœŸæœ›å°±æ˜¯ Advantageï¼š</p>
  <p>$$
  \mathbb{E}[\delta_t] = \mathbb{E}[r_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)] = Q^\pi(s_t, a_t) - V^\pi(s_t) = A^\pi(s_t, a_t)
  $$</p>
  <p>å› æ­¤ <strong>TD è¯¯å·®æ˜¯ Advantage çš„æ— åä¼°è®¡</strong>ã€‚</p>

  <p><b>â‘¡ TD è¯¯å·®å…·æœ‰ä½æ–¹å·®çš„ä¼˜ç‚¹</b></p>
  <ul>
    <li><strong>ç›´æ¥ç”¨å›æŠ¥ä¼°è®¡</strong>ï¼ˆREINFORCEï¼‰ï¼š$G_t = \sum_k \gamma^k r_{t+k}$ éœ€è¦ç´¯è®¡å®Œæ•´è½¨è¿¹çš„å¥–åŠ±ï¼Œæ–¹å·®å¾ˆé«˜</li>
    <li><strong>ç”¨ TD è¯¯å·®ä¼°è®¡</strong>ï¼ˆACï¼‰ï¼š$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ åªä¾èµ–å½“å‰ä¸€æ­¥å’Œä¸‹ä¸€çŠ¶æ€ï¼Œæ–¹å·®æä½ï¼Œè®­ç»ƒæ›´ç¨³å®š</li>
    <li><strong>ç»“æœ</strong>ï¼šè™½ç„¶ V å‡½æ•°çš„å­¦ä¹ ä¼šå¼•å…¥ä¸€äº›åå·®ï¼Œä½†æ€»ä½“ä¸Šæ–¹å·®çš„ä¸‹é™è¿œè¶…åå·®çš„å¼•å…¥ï¼Œä½¿å¾— AC ç®—æ³•åœ¨å®è·µä¸­æ¯” REINFORCE é«˜æ•ˆå¾—å¤š</li>
  </ul>

  <p><strong>è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨ ACã€A2Cã€PPO ç­‰æ‰€æœ‰ç°ä»£ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­ï¼Œéƒ½ç”¨ TD è¯¯å·®æˆ–å…¶å¤šæ­¥ç‰ˆæœ¬æ¥è¿‘ä¼¼ Advantageã€‚</strong></p>

  <h3>å››ã€æ¢¯åº¦æ›´æ–°çš„æ•°å­¦åŸç†ï¼šä¸ºä»€ä¹ˆ Actor æ˜¯"åŠ "ã€Critic æ˜¯"å‡"ï¼Ÿ</h3>

  <h4>4.1 æ¢¯åº¦çš„å‡ ä½•å«ä¹‰</h4>
  <p>è®¾æœ‰ä¸€ä¸ªå‡½æ•° $f(\theta)$ï¼Œå…¶ä¸­ $\theta$ æ˜¯æ¨¡å‹å‚æ•°ã€‚</p>
  <p><strong>æ¢¯åº¦ï¼ˆGradientï¼‰$\nabla_\theta f(\theta)$</strong> è¡¨ç¤ºåœ¨å½“å‰ä½ç½®ï¼Œå‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘ã€‚</p>
  <ul>
    <li>æ²¿ç€<strong>æ¢¯åº¦æ–¹å‘</strong>èµ°ä¸€æ­¥ï¼Œ$f(\theta)$ ä¼šå˜å¤§å¾—æœ€å¿«ï¼›</li>
    <li>æ²¿ç€<strong>åæ¢¯åº¦æ–¹å‘</strong>èµ°ä¸€æ­¥ï¼Œ$f(\theta)$ ä¼šå˜å°å¾—æœ€å¿«ã€‚</li>
  </ul>
  <p><strong>ç›´è§‰ä¸Šï¼š</strong></p>
  <ul>
    <li>æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬"å¾€å“ªè¾¹çˆ¬å±±æœ€å¿«"ï¼›</li>
    <li>åæ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬"å¾€å“ªè¾¹ä¸‹å±±æœ€å¿«"ã€‚</li>
  </ul>

  <h4>4.2 æ¢¯åº¦ä¸‹é™ï¼ˆæœ€å°åŒ–æŸå¤±ï¼‰</h4>
  <p>å‡è®¾æˆ‘ä»¬è¦<strong>æœ€å°åŒ–</strong>æŸå¤±å‡½æ•° $L(\theta)$ã€‚</p>
  <p>æˆ‘ä»¬å¸Œæœ›æ¯ä¸€æ­¥è®© $L$ å˜å°ï¼Œå› æ­¤è¦å¾€ä¸‹èµ°ï¼š</p>
  <p>$$
  \theta \gets \theta - \alpha \nabla_\theta L(\theta)
  $$</p>
  <p>å…¶ä¸­ï¼š</p>
  <ul>
    <li>$\alpha > 0$ æ˜¯å­¦ä¹ ç‡ï¼›</li>
    <li><strong>å‡å·ä»£è¡¨"åæ¢¯åº¦æ–¹å‘"</strong>ï¼›</li>
    <li>å› ä¸ºæ¢¯åº¦æ˜¯"ä¸Šå‡æ–¹å‘"ï¼Œè¦æœ€å°åŒ–å°±è¦å¾€åæ–¹å‘èµ°ã€‚</li>
  </ul>
  <pre><code>    â†‘  æ¢¯åº¦æ–¹å‘ï¼šLå¢åŠ æœ€å¿«
Î¸ â†â”€â”€ åæ¢¯åº¦æ–¹å‘ï¼šLå‡å°æœ€å¿«</code></pre>
  <p><strong>è¿™å°±æ˜¯"æœ€å°åŒ– â‡’ å‡æ¢¯åº¦"çš„åŸå› ã€‚</strong></p>

  <h4>4.3 æ¢¯åº¦ä¸Šå‡ï¼ˆæœ€å¤§åŒ–å›æŠ¥ï¼‰</h4>
  <p>ç›¸åï¼Œå¦‚æœç›®æ ‡æ˜¯<strong>æœ€å¤§åŒ–</strong>æŸä¸ªå‡½æ•° $J(\theta)$ï¼ˆæ¯”å¦‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ï¼‰ï¼š</p>
  <p>$$
  \theta \gets \theta + \alpha \nabla_\theta J(\theta)
  $$</p>
  <p>è¿™é‡Œæˆ‘ä»¬<strong>æ²¿æ¢¯åº¦æ–¹å‘èµ°</strong>ï¼Œå› ä¸ºæ¢¯åº¦æŒ‡å‘"ä¸Šå‡æœ€å¿«"çš„æ–¹å‘ã€‚</p>
  <pre><code>Î¸ â”€â”€â†’ æ¢¯åº¦æ–¹å‘ï¼šJå¢åŠ æœ€å¿«</code></pre>

  <h4>4.4 è”ç³» Actor-Critic çš„æ›´æ–°å…¬å¼</h4>
  
  <p><strong>â‘  Critic è¦æœ€å°åŒ– TD è¯¯å·®å¹³æ–¹ï¼š</strong></p>
  <p>$$
  L = \frac{1}{2} \delta_t^2
  $$</p>
  <p>æ‰€ä»¥ï¼š</p>
  <p>$$
  \theta_\text{Critic} \gets \theta_\text{Critic} - \alpha \nabla_\theta L = \theta_\text{Critic} + \alpha \delta_t \nabla_\theta V_\theta(s)
  $$</p>
  <p>ï¼ˆå› ä¸º $-\nabla_\theta L = \delta_t \nabla_\theta V_\theta(s)$ï¼‰</p>
  <p><em>è¿™å®é™…ä¸Šæ˜¯åå‘ä¼ æ’­æ¢¯åº¦å½¢å¼çš„æ•°å­¦è¡¨è¾¾å¼ã€‚</em></p>

  <p><strong>â‘¡ Actor è¦æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ï¼š</strong></p>
  <p>$$
  J = \mathbb{E}[\log \pi_\theta(a|s) A_t]
  $$</p>
  <p>æ‰€ä»¥ï¼š</p>
  <p>$$
  \theta_\text{Actor} \gets \theta_\text{Actor} + \alpha \nabla_\theta J = \theta_\text{Actor} + \alpha \nabla_\theta \log \pi_\theta(a|s) \delta_t
  $$</p>
  <p><em>å…¶ä¸­ $\nabla_\theta \log \pi_\theta(a_t|s_t)$ æ˜¯ä» Actor ç½‘ç»œåå‘ä¼ æ’­å¾—åˆ°çš„æ¢¯åº¦ï¼›$\delta_t$ æ˜¯æƒé‡ä¿¡å·ï¼ˆå‘Šè¯‰ Actor è¿™æ¬¡åŠ¨ä½œæ˜¯å¥½è¿˜æ˜¯åï¼‰ã€‚</em></p>

  <p><strong>æ€»ç»“ï¼šä¸ºä»€ä¹ˆä¸¤ä¸ªæ›´æ–°å¼ä¸€ä¸ªæ˜¯"åŠ "ï¼Œä¸€ä¸ªæ˜¯"å‡"ï¼Ÿ</strong></p>
  <ul>
    <li><strong>Critic æ˜¯æœ€å°åŒ–è¯¯å·®</strong> â†’ åæ¢¯åº¦ä¸‹é™ï¼ˆå®é™…å†™æˆåŠ å·å½¢å¼æ˜¯å› ä¸ºå·²ç»å±•å¼€äº†è´Ÿå·ï¼‰ï¼›</li>
    <li><strong>Actor æ˜¯æœ€å¤§åŒ–å›æŠ¥</strong> â†’ é¡ºæ¢¯åº¦ä¸Šå‡ã€‚</li>
  </ul>

  <h3>äº”ã€åŠ¨ä½œé‡‡æ ·æœºåˆ¶ï¼š$a_t \sim \pi(a|s_t)$ çš„å…·ä½“å®ç°</h3>

  <p>åœ¨ Actor-Critic ç®—æ³•ä¸­ï¼ŒActor æ˜¯ä¸€ä¸ªç­–ç•¥ç½‘ç»œï¼Œå®ƒçš„è¾“å‡ºä¸æ˜¯ä¸€ä¸ªå•ä¸€åŠ¨ä½œï¼Œè€Œæ˜¯ä¸€ä¸ª<strong>åŠ¨ä½œåˆ†å¸ƒ</strong>ï¼ˆä¾‹å¦‚"åœ¨çŠ¶æ€ $s_t$ ä¸‹å„ä¸ªåŠ¨ä½œçš„æ¦‚ç‡"ï¼‰ã€‚</p>
  <p>ç„¶åï¼š</p>
  <ul>
    <li>AC ç®—æ³•ä¼š<strong>ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ï¼ˆsampleï¼‰</strong>ä¸€ä¸ªåŠ¨ä½œï¼Œæ‰§è¡Œå®ƒä¸ç¯å¢ƒäº¤äº’ã€‚</li>
  </ul>
  <p>æ‰€ä»¥ï¼š</p>
  <p>$$
  a_t \sim \pi_\theta(a|s_t)
  $$</p>
  <p>æ„æ€æ˜¯"ä»ç”± Actor ç½‘ç»œå®šä¹‰çš„åˆ†å¸ƒ Ï€ ä¸­æŠ½å–ä¸€ä¸ªåŠ¨ä½œ $a_t$"ã€‚</p>

  <h4>5.1 ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆä¾‹å¦‚ CartPole, Atariï¼‰</h4>
  <p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒActor è¾“å‡ºä¸€ä¸ª<strong>ç¦»æ•£åˆ†å¸ƒ</strong>ï¼š</p>
  <p>$$
  \pi_\theta(a|s) = \text{softmax}(f_\theta(s))
  $$</p>
  <p>ä¹Ÿå°±æ˜¯è¯´ï¼Œç½‘ç»œè¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ $p_i$ï¼Œç„¶åä»ä¸­é‡‡æ ·ã€‚</p>

  <p><strong>ç¤ºä¾‹ï¼ˆPyTorchï¼‰ï¼š</strong></p>
  <pre><code class="language-python">import torch
import torch.nn.functional as F

# å‡è®¾ Actor è¾“å‡º logits
logits = actor_net(state)              # shape [n_actions]
probs = F.softmax(logits, dim=-1)      # è½¬æˆæ¦‚ç‡åˆ†å¸ƒ
dist = torch.distributions.Categorical(probs)
action = dist.sample()                 # ä»åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œ</code></pre>

  <ul>
    <li><code>Categorical</code> è¡¨ç¤ºå¤šé¡¹å¼åˆ†å¸ƒï¼›</li>
    <li><code>dist.sample()</code> å°±æ˜¯å®ç° $a_t \sim \pi_\theta(a|s_t)$ï¼›</li>
    <li><code>dist.log_prob(action)</code> ä¹‹åä¼šç”¨äºç­–ç•¥æ¢¯åº¦æ›´æ–°ã€‚</li>
  </ul>

  <p><strong>è¿™å°±å¯¹åº”ä¼ªä»£ç é‡Œçš„é‚£ä¸€è¡Œï¼š"é€‰æ‹©åŠ¨ä½œ $a_t \sim \pi(a|s_t)$"</strong></p>
  <p>å®é™…ä¸Šå°±æ˜¯ä» softmax æ¦‚ç‡ä¸­æŠ½æ ·åŠ¨ä½œã€‚</p>

  <h4>5.2 è¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆä¾‹å¦‚ Pendulum, Mujocoï¼‰</h4>
  <p>åœ¨è¿ç»­åŠ¨ä½œç©ºé—´ä¸‹ï¼Œæˆ‘ä»¬æ— æ³•ç”¨ softmaxï¼Œå› ä¸ºåŠ¨ä½œæ˜¯å®æ•°ã€‚</p>
  <p>äºæ˜¯ Actor è¾“å‡ºä¸€ä¸ª<strong>é«˜æ–¯åˆ†å¸ƒå‚æ•°</strong>ï¼š</p>
  <p>$$
  \pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)^2)
  $$</p>
  <p>ç„¶åä»è¯¥åˆ†å¸ƒé‡‡æ ·ï¼š</p>
  <p>$$
  a_t = \mu_\theta(s_t) + \sigma_\theta(s_t) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
  $$</p>

  <p><strong>ç¤ºä¾‹ï¼ˆPyTorchï¼‰ï¼š</strong></p>
  <pre><code class="language-python">mu, log_std = actor_net(state)
std = log_std.exp()
dist = torch.distributions.Normal(mu, std)
action = dist.sample()                 # è¿ç»­é‡‡æ ·
action = torch.tanh(action)            # é™åˆ¶åœ¨[-1,1]èŒƒå›´</code></pre>

  <ul>
    <li>æœ‰äº›å®ç°è¿˜ä¼šç”¨ <code>dist.rsample()</code>ï¼ˆé‡å‚æ•°åŒ–é‡‡æ ·ï¼‰æ¥ä¿æŒæ¢¯åº¦å¯ä¼ æ’­ã€‚</li>
  </ul>

  <h3>å…­ã€æ ¸å¿ƒå…¬å¼æ€»ç»“</h3>
  <ul>
    <li><b>TD è¯¯å·®ï¼ˆç”¨äº Advantage ä¼°è®¡ï¼‰</b>ï¼š
      <p>$$
      \delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
      $$</p>
    </li>
    <li><b>Actor æ›´æ–°ï¼ˆæ¢¯åº¦ä¸Šå‡ï¼Œæœ€å¤§åŒ–å›æŠ¥ï¼‰</b>ï¼š
      <p>$$
      \theta_\text{Actor} \gets \theta_\text{Actor} + \alpha_\text{Actor} \nabla_\theta \log \pi_\theta(a|s) \, \delta_t
      $$</p>
    </li>
    <li><b>Critic æ›´æ–°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼Œæœ€å°åŒ–è¯¯å·®ï¼‰</b>ï¼š
      <p>$$
      \phi_\text{Critic} \gets \phi_\text{Critic} + \alpha_\text{Critic} \delta_t \nabla_\phi V_\phi(s)
      $$</p>
      <p><em>ï¼ˆç­‰ä»·äº $\phi \gets \phi - \alpha \nabla_\phi \frac{1}{2}\delta_t^2$ï¼‰</em></p>
    </li>
  </ul>

  <h3>ä¸ƒã€è®­ç»ƒæµç¨‹</h3>
  <ol>
    <li>åˆå§‹åŒ– Actor å’Œ Critic ç½‘ç»œ</li>
    <li>ä¸ç¯å¢ƒäº¤äº’ï¼Œ<strong>é€‰æ‹©åŠ¨ä½œ $a_t \sim \pi(a|s_t)$</strong>ï¼ˆä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ï¼‰</li>
    <li>æ‰§è¡ŒåŠ¨ä½œï¼Œè·å¾— $r_{t+1}, s_{t+1}$</li>
    <li>Critic è®¡ç®— TD è¯¯å·® $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$</li>
    <li>Actor ä½¿ç”¨ $\delta_t$ è¿›è¡Œ<strong>æ¢¯åº¦ä¸Šå‡</strong>æ›´æ–°ç­–ç•¥</li>
    <li>Critic ä½¿ç”¨ $\delta_t$ è¿›è¡Œ<strong>æ¢¯åº¦ä¸‹é™</strong>æ›´æ–°ä»·å€¼å‡½æ•°</li>
    <li>é‡å¤ä»¥ä¸Šæ­¥éª¤ç›´åˆ°è®­ç»ƒå®Œæˆ</li>
  </ol>

  <h3>å…«ã€Actor-Critic ç¤ºä¾‹ä»£ç ï¼ˆç¦»æ•£åŠ¨ä½œç©ºé—´ï¼‰</h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

class ActorNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        logits = self.fc3(x)
        return torch.softmax(logits, dim=-1), logits  # åŒæ—¶è¿”å› logitsï¼Œä¾¿äºç®—ç†µ

class CriticNetwork(nn.Module):
    def __init__(self, state_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 1)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)  # [B,1]

class ActorCritic:
    def __init__(self, state_dim, action_dim, gamma=0.99, actor_lr=1e-3, critic_lr=1e-3,
                 entropy_coef=0.01, grad_clip=0.5, device=None):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.entropy_coef = entropy_coef
        self.grad_clip = grad_clip
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        self.actor = ActorNetwork(state_dim, action_dim).to(self.device)
        self.critic = CriticNetwork(state_dim).to(self.device)
        
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)
    
    def _to_tensor(self, s):
        return torch.as_tensor(s, dtype=torch.float32, device=self.device).unsqueeze(0)

    def select_action(self, state):
        """é‡‡æ ·åŠ¨ä½œï¼ˆä¿ç•™æ¢¯åº¦å›¾ï¼‰ï¼Œä¹Ÿå¯ä»¥åªè¿”å›åŠ¨ä½œï¼ŒæŠŠlog_probæ”¾åˆ°train_stepé‡Œå†å‰å‘ä¸€æ¬¡ã€‚"""
        state_tensor = self._to_tensor(state)
        action_probs, logits = self.actor(state_tensor)  # ä¸è¦ç”¨ no_grad
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)  # [1] -> æ ‡é‡
        entropy = dist.entropy()          # [1]
        return int(action.item()), log_prob, entropy
    
    def train_step(self, state, action, reward, next_state, done):
        """å•æ­¥è®­ç»ƒï¼ˆTD(0)ï¼‰"""
        state_tensor = self._to_tensor(state)
        next_state_tensor = self._to_tensor(next_state)

        # ---- Critic å‰å‘ï¼ˆè¦æœ‰æ¢¯åº¦ï¼‰----
        value = self.critic(state_tensor).squeeze(-1)        # [1]
        with torch.no_grad():
            next_value = self.critic(next_state_tensor).squeeze(-1)  # [1]
            target_value = torch.as_tensor([reward], device=self.device, dtype=torch.float32)
            if not done:
                target_value = target_value + self.gamma * next_value
        
        advantage = (target_value - value)  # [1]

        # ---- Actor å‰å‘ï¼ˆé‡æ–°è®¡ç®—log_probä¸ç†µï¼Œç¡®ä¿æœ‰æ¢¯åº¦å›¾ï¼‰----
        action_probs, _ = self.actor(state_tensor)
        dist = Categorical(action_probs)
        log_prob = dist.log_prob(torch.as_tensor([action], device=self.device))
        entropy = dist.entropy()

        # ---- æŸå¤± ----
        actor_loss = -(log_prob * advantage.detach()) - self.entropy_coef * entropy
        critic_loss = torch.mean((value - target_value.detach()) ** 2)

        # ---- åä¼ ä¸ä¼˜åŒ–ï¼ˆå¸¦æ¢¯åº¦è£å‰ªï¼‰----
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        if self.grad_clip is not None:
            nn.utils.clip_grad_norm_(self.actor.parameters(), self.grad_clip)
        self.actor_optimizer.step()

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        if self.grad_clip is not None:
            nn.utils.clip_grad_norm_(self.critic.parameters(), self.grad_clip)
        self.critic_optimizer.step()

        return actor_loss.item(), critic_loss.item(), float(entropy.item())
    
    def _reset_env(self, env):
        """å…¼å®¹ Gym / Gymnasium ä¸åŒ reset è¿”å›æ ¼å¼"""
        out = env.reset()
        if isinstance(out, tuple) and len(out) == 2:
            obs, _ = out
            return obs
        return out
    
    def _step_env(self, env, action):
        """å…¼å®¹ Gym / Gymnasium ä¸åŒ step è¿”å›æ ¼å¼"""
        out = env.step(action)
        if len(out) == 4:
            next_state, reward, done, info = out
        else:
            next_state, reward, terminated, truncated, info = out
            done = terminated or truncated
        return next_state, reward, done, info

    def train_episode(self, env):
        state = self._reset_env(env)
        done = False
        episode_reward = 0.0
        
        while not done:
            action, _, _ = self.select_action(state)  # è¿™é‡Œä¸å¿…è¿”å›log_probï¼Œtrain_stepé‡Œä¼šé‡æ–°è®¡ç®—
            next_state, reward, done, _ = self._step_env(env, action)
            
            self.train_step(state, action, reward, next_state, done)
            episode_reward += reward
            state = next_state
        
        return episode_reward
    
    def train(self, env, num_episodes=1000, print_every=100):
        rewards = []
        for episode in range(1, num_episodes + 1):
            R = self.train_episode(env)
            rewards.append(R)
            if episode % print_every == 0:
                avg_R = np.mean(rewards[-print_every:])
                print(f"Episode {episode}, Avg Reward: {avg_R:.2f}")
        return rewards

  </code></pre>

  <h3>å…­ã€è®­ç»ƒå®Œæˆåˆ¤æ–­</h3>
  <ul>
    <li>å¹³å‡ç´¯è®¡å¥–åŠ±ç¨³å®šå¹¶æ”¶æ•›</li>
    <li>Actor ç­–ç•¥æ¦‚ç‡å˜åŒ–ä¸å¤§</li>
    <li>Critic TD loss æ”¶æ•›</li>
  </ul>

  <h3>ä¸ƒã€ç›´è§‚ç†è§£æ€»ç»“</h3>
  <ul>
    <li>Actor å†³ç­–ï¼ŒCritic è¯„ä»·</li>
    <li>TD è¯¯å·®æŒ‡å¯¼ Actor æ›´æ–°ï¼Œæ¯” REINFORCE é«˜æ•ˆã€ä½æ–¹å·®</li>
    <li>å¯åœ¨çº¿é€æ­¥æ›´æ–°ç­–ç•¥ï¼Œä¸å¿…ç­‰æ•´å›åˆç»“æŸ</li>
  </ul>

</section>
<!-- ç¬¬åä¸‰ç«  -->
<section id="chapter13" class="chapter">
  <h2>ç¬¬åä¸‰ç« ï¼šA3C ä¸ A2C</h2>

  <p>A3C (2016) å¼€åˆ›äº†Næ­¥TD + å¹¶è¡Œè®­ç»ƒï¼ŒA2C (2017) å°†å¼‚æ­¥æ”¹ä¸ºåŒæ­¥ä»¥æ›´å¥½åˆ©ç”¨GPUã€‚</p>

  <h3>ä¸€ã€A3C (Asynchronous Advantage Actor-Critic)</h3>
  <p><b>æ ¸å¿ƒï¼š</b>Næ­¥TD + å¼‚æ­¥å¤šçº¿ç¨‹</p>
  <p><b>Næ­¥å›æŠ¥ï¼š</b>$R_t = \sum_{i=0}^{T-1-t} \gamma^i r_{t+1+i} + \gamma^{T-t} V(s_T)$ï¼Œ$A_t = R_t - V(s_t)$</p>
  <p><b>å¼‚æ­¥æ›´æ–°ï¼š</b>æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹é‡‡æ ·Tæ­¥ï¼Œç«‹å³å¼‚æ­¥æ›´æ–°å…¨å±€ç½‘ç»œã€‚</p>

  <h3>A3C ç®€åŒ–ä»£ç </h3>
  <pre><code class="language-python">
import torch
import torch.nn as nn
import threading

class A3CNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.shared = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU())
        self.actor = nn.Linear(128, action_dim)
        self.critic = nn.Linear(128, 1)
    
    def forward(self, x):
        shared = self.shared(x)
        return F.softmax(self.actor(shared), dim=-1), self.critic(shared)

class A3CWorker(threading.Thread):
    def __init__(self, global_net, env, T=20):
        super().__init__()
        self.global_net = global_net
        self.local_net = A3CNetwork(state_dim, action_dim)
        self.env = env
        self.T = T
    
    def run(self):
        while True:
            # æ”¶é›†Tæ­¥ç»éªŒ
            states, actions, rewards = [], [], []
            state = self.env.reset()
            
            for _ in range(self.T):
                probs, _ = self.local_net(torch.FloatTensor(state))
                action = torch.multinomial(probs, 1).item()
                next_state, reward, done, _ = self.env.step(action)
                
                states.append(state)
                actions.append(action)
                rewards.append(reward)
                
                if done: break
                state = next_state
            
            # è®¡ç®—Næ­¥å›æŠ¥ï¼Œæ›´æ–°æœ¬åœ°ç½‘ç»œï¼Œå¼‚æ­¥æ›´æ–°å…¨å±€ç½‘ç»œ
            # ... çœç•¥å…·ä½“è®¡ç®— ...
  </code></pre>

  <h3>äºŒã€A2C (Advantage Actor-Critic)</h3>
  <p><b>æ ¸å¿ƒï¼š</b>Næ­¥TD + åŒæ­¥å¤šç¯å¢ƒ</p>
  <p><b>åŒæ­¥æ›´æ–°ï¼š</b>å¤šä¸ªç¯å¢ƒåŒæ­¥é‡‡æ ·Tæ­¥ï¼Œç´¯ç§¯æ¢¯åº¦åæ‰¹é‡æ›´æ–°ï¼ŒGPUå‹å¥½ã€‚</p>

  <h3>A2C ç®€åŒ–ä»£ç </h3>
  <pre><code class="language-python">
class A2C:
    def __init__(self, state_dim, action_dim, n_envs=8, T=5):
        self.net = A3CNetwork(state_dim, action_dim)  # å¤ç”¨ç½‘ç»œç»“æ„
        self.envs = [gym.make('CartPole-v1') for _ in range(n_envs)]
        self.T = T
        
    def train_step(self):
        batch_data = []
        
        # æ‰€æœ‰ç¯å¢ƒåŒæ­¥é‡‡æ ·Tæ­¥
        for env in self.envs:
            states, actions, rewards = [], [], []
            state = env.reset()
            
            for _ in range(self.T):
                probs, _ = self.net(torch.FloatTensor(state))
                action = torch.multinomial(probs, 1).item()
                next_state, reward, done, _ = env.step(action)
                
                states.append(state)
                actions.append(action) 
                rewards.append(reward)
                
                if done: break
                state = next_state
            
            batch_data.extend(list(zip(states, actions, rewards)))
        
        # è®¡ç®—Næ­¥å›æŠ¥å’ŒæŸå¤±ï¼ŒåŒæ­¥æ›´æ–°ç½‘ç»œ
        # ... çœç•¥å…·ä½“è®¡ç®— ...
  </code></pre>

  <h3>ä¸‰ã€æ ¸å¿ƒå¯¹æ¯”</h3>
  <table>
    <tr><th>ç‰¹æ€§</th><th>A3C</th><th>A2C</th></tr>
    <tr><td>å¹¶è¡Œæ–¹å¼</td><td>å¼‚æ­¥å¤šçº¿ç¨‹</td><td>åŒæ­¥å¤šç¯å¢ƒ</td></tr>
    <tr><td>æ›´æ–°æ—¶æœº</td><td>å„çº¿ç¨‹ç‹¬ç«‹æ›´æ–°</td><td>æ‰¹é‡ç´¯ç§¯æ›´æ–°</td></tr>
    <tr><td>æ¢ç´¢æ€§</td><td>æ›´å¼º</td><td>è¾ƒå¥½</td></tr>
    <tr><td>ç¨³å®šæ€§</td><td>ä¸€èˆ¬</td><td>æ›´é«˜</td></tr>
    <tr><td>GPUåˆ©ç”¨</td><td>ä¸€èˆ¬</td><td>æ›´å¥½</td></tr>
  </table>

</section>

<!-- å…¨ä½“ç®—æ³•æ€»ç»“ -->
<section id="summary" class="chapter">
  <h2>å…¨ä½“ç®—æ³•æ€»ç»“ä¸é€‰æ‹©æŒ‡å—</h2>
  
  <table border="1" cellpadding="8" cellspacing="0">
    <thead>
      <tr>
        <th>ç®—æ³•</th>
        <th>éœ€è¦æ¨¡å‹</th>
        <th>åŠ¨ä½œç©ºé—´</th>
        <th>æ ¸å¿ƒæ€æƒ³</th>
        <th>ä¼˜ç‚¹</th>
        <th>ç¼ºç‚¹</th>
        <th>é€‚ç”¨åœºæ™¯</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>ç­–ç•¥è¿­ä»£</td>
        <td>æ˜¯</td>
        <td>ç¦»æ•£</td>
        <td>è¯„ä¼°+æ”¹è¿›å¾ªç¯</td>
        <td>ç†è®ºå®Œå¤‡</td>
        <td>éœ€è¦æ¨¡å‹ï¼Œè®¡ç®—å¤æ‚</td>
        <td>æ¨¡å‹å·²çŸ¥å°è§„æ¨¡é—®é¢˜</td>
      </tr>
      <tr>
        <td>å€¼è¿­ä»£</td>
        <td>æ˜¯</td>
        <td>ç¦»æ•£</td>
        <td>ç›´æ¥ä¼˜åŒ–Vå‡½æ•°</td>
        <td>ç®€æ´é«˜æ•ˆ</td>
        <td>éœ€è¦æ¨¡å‹</td>
        <td>æ¨¡å‹å·²çŸ¥å°è§„æ¨¡é—®é¢˜</td>
      </tr>
      <tr>
        <td>è’™ç‰¹å¡æ´›</td>
        <td>å¦</td>
        <td>ç¦»æ•£</td>
        <td>é‡‡æ ·+å›åˆå›æŠ¥</td>
        <td>æ— éœ€æ¨¡å‹ï¼Œç†è®ºç®€å•</td>
        <td>æ–¹å·®é«˜ï¼Œéœ€ç­‰å›åˆç»“æŸ</td>
        <td>ç¦»æ•£å°ç¯å¢ƒã€ç¦»çº¿å­¦ä¹ </td>
      </tr>
      <tr>
        <td>æ—¶é—´å·®åˆ† (TD)</td>
        <td>å¦</td>
        <td>ç¦»æ•£</td>
        <td>è‡ªä¸¾+ä¸€æ­¥æ›´æ–°</td>
        <td>ä½æ–¹å·®ï¼Œåœ¨çº¿å­¦ä¹ </td>
        <td>å¼•å…¥åå·®</td>
        <td>ç¦»æ•£ä¸­ç­‰è§„æ¨¡é—®é¢˜</td>
      </tr>
      <tr>
        <td>Q-learning</td>
        <td>å¦</td>
        <td>ç¦»æ•£</td>
        <td>Off-policyå€¼è¿­ä»£</td>
        <td>æ”¶æ•›å¿«ï¼Œæ— æ¨¡å‹</td>
        <td>Qå€¼è¡¨è¿‡å¤§ä¸å¯è¡Œ</td>
        <td>ç¦»æ•£é—®é¢˜çš„æ ‡å‡†æ–¹æ³•</td>
      </tr>
      <tr>
        <td>DQN</td>
        <td>å¦</td>
        <td>ç¦»æ•£</td>
        <td>ç¥ç»ç½‘ç»œé€¼è¿‘Q</td>
        <td>æ”¯æŒé«˜ç»´çŠ¶æ€</td>
        <td>Qå€¼è¿‡ä¼°è®¡ï¼Œè®­ç»ƒä¸ç¨³å®š</td>
        <td>é«˜ç»´è§‚æµ‹ã€è§†è§‰ä»»åŠ¡</td>
      </tr>
      <tr>
        <td>DDQN</td>
        <td>å¦</td>
        <td>ç¦»æ•£</td>
        <td>åŒç½‘ç»œå‡ç¼“é«˜ä¼°</td>
        <td>è®­ç»ƒç¨³å®š</td>
        <td>è®¡ç®—é‡å¢åŠ </td>
        <td>å¤æ‚ç¯å¢ƒã€æŒç»­ä»»åŠ¡</td>
      </tr>
      <tr>
        <td>REINFORCE</td>
        <td>å¦</td>
        <td>è¿ç»­/ç¦»æ•£</td>
        <td>ç­–ç•¥å‚æ•°ç›´æ¥æ¢¯åº¦ä¸Šå‡</td>
        <td>æ”¯æŒè¿ç»­åŠ¨ä½œ</td>
        <td>æ–¹å·®å¤§ï¼Œæ”¶æ•›æ…¢</td>
        <td>è¿ç»­æ§åˆ¶é—®é¢˜</td>
      </tr>
      <tr>
        <td>Actor-Critic</td>
        <td>å¦</td>
        <td>è¿ç»­/ç¦»æ•£</td>
        <td>Actor+CriticåŸºçº¿</td>
        <td>ä½æ–¹å·®å¿«é€Ÿæ”¶æ•›</td>
        <td>éœ€è¦ç»´æŠ¤ä¸¤ä¸ªç½‘ç»œ</td>
        <td>è¿ç»­æ§åˆ¶ã€å®æ—¶åº”ç”¨</td>
      </tr>
      <tr>
        <td>A2C</td>
        <td>å¦</td>
        <td>è¿ç»­/ç¦»æ•£</td>
        <td>å¤šç¯å¢ƒåŒæ­¥é‡‡æ ·</td>
        <td>å¹¶è¡Œé«˜æ•ˆç¨³å®š</td>
        <td>éœ€å¤šGPUæ”¯æŒ</td>
        <td>å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒ</td>
      </tr>
      <tr>
        <td>A3C</td>
        <td>å¦</td>
        <td>è¿ç»­/ç¦»æ•£</td>
        <td>å¤šçº¿ç¨‹å¼‚æ­¥æ›´æ–°</td>
        <td>é«˜æ¢ç´¢æ€§ã€å¿«é€Ÿæ”¶æ•›</td>
        <td>å®ç°å¤æ‚</td>
        <td>èµ„æºå—é™ã€å®æ—¶ç³»ç»Ÿ</td>
      </tr>
    </tbody>
  </table>

  <h3>ç®—æ³•é€‰æ‹©æµç¨‹å›¾</h3>
  <ol>
    <li><strong>æ˜¯å¦çŸ¥é“ç¯å¢ƒçš„è½¬ç§»æ¦‚ç‡æ¨¡å‹ï¼Ÿ</strong>
      <ul>
        <li>æ˜¯ â†’ ä½¿ç”¨åŠ¨æ€è§„åˆ’ï¼ˆç­–ç•¥è¿­ä»£/å€¼è¿­ä»£ï¼‰</li>
        <li>å¦ â†’ ç»§ç»­ç¬¬2æ­¥</li>
      </ul>
    </li>
    <li><strong>åŠ¨ä½œç©ºé—´æ˜¯ç¦»æ•£è¿˜æ˜¯è¿ç»­ï¼Ÿ</strong>
      <ul>
        <li>ç¦»æ•£ â†’ ç»§ç»­ç¬¬3æ­¥</li>
        <li>è¿ç»­ â†’ ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆREINFORCE/AC/A2C/A3Cï¼‰</li>
      </ul>
    </li>
    <li><strong>çŠ¶æ€ç©ºé—´ç»´åº¦ï¼Ÿ</strong>
      <ul>
        <li>ä½ï¼ˆå‡ ç™¾ä¸ªçŠ¶æ€ä»¥ä¸‹ï¼‰ â†’ ä½¿ç”¨Q-learningè¡¨æ ¼å½¢å¼</li>
        <li>é«˜ï¼ˆå›¾åƒç­‰ï¼‰â†’ ä½¿ç”¨DQN/DDQN</li>
      </ul>
    </li>
    <li><strong>æ˜¯å¦éœ€è¦æ ·æœ¬æ•ˆç‡å’Œç¨³å®šæ€§ï¼Ÿ</strong>
      <ul>
        <li>å¦ â†’ DQNå¯ä»¥</li>
        <li>æ˜¯ â†’ ä½¿ç”¨DDQNæˆ–ç»„åˆA2C</li>
      </ul>
    </li>
  </ol>

</section>

  </main>
</div>

<footer>
  <p>Â© 2025 <span>Haoyun Tang</span> | Reinforcement Learning Notes</p>
</footer>

<!-- ===== åŠ¨æ€ç²’å­ç‰¹æ•ˆ ===== -->
<script>
const canvas = document.getElementById("trailCanvas");
const ctx = canvas.getContext("2d");
let particles = [];
function resize() {
  canvas.width = window.innerWidth;
  canvas.height = window.innerHeight;
}
window.addEventListener("resize", resize);
resize();
document.addEventListener("mousemove", e => {
  for (let i = 0; i < 2; i++) {
    particles.push({x: e.clientX, y: e.clientY, alpha: 1, r: Math.random()*3+1});
  }
});
function animate() {
  ctx.fillStyle = "rgba(0,0,0,0.2)";
  ctx.fillRect(0, 0, canvas.width, canvas.height);
  particles.forEach(p => {
    p.y -= 0.3;
    p.alpha -= 0.01;
    ctx.beginPath();
    ctx.arc(p.x, p.y, p.r, 0, Math.PI*2);
    ctx.fillStyle = `rgba(0,188,212,${p.alpha})`;
    ctx.fill();
  });
  particles = particles.filter(p => p.alpha > 0);
  requestAnimationFrame(animate);
}
animate();
</script>

<!-- ===== ä»£ç é«˜äº®åˆå§‹åŒ– ===== -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  // è‡ªåŠ¨æ·»åŠ è¡Œå·ç±»å
  document.querySelectorAll('pre[class*="language-"]').forEach(pre => {
    if (!pre.classList.contains('line-numbers')) {
      pre.classList.add('line-numbers');
    }
  });
  
  // è§¦å‘ Prism é«˜äº®
  if (window.Prism) {
    Prism.highlightAll();
  }
});

// é¡µé¢åŠ è½½åé‡æ–°é«˜äº®
window.addEventListener('load', function() {
  if (window.Prism) {
    Prism.highlightAll();
  }
});
</script>

<!-- ===== å¯¼èˆªè‡ªåŠ¨é«˜äº® ===== -->
<script>
const navLinks = document.querySelectorAll("nav a");
const asideLinks = document.querySelectorAll("aside a");
const allLinks = [...navLinks, ...asideLinks];

window.addEventListener("scroll", () => {
  let fromTop = window.scrollY + 150;
  
  allLinks.forEach(link => {
    const section = document.querySelector(link.getAttribute("href"));
    if (section && section.offsetTop <= fromTop && section.offsetTop + section.offsetHeight > fromTop) {
      link.classList.add("active");
    } else {
      link.classList.remove("active");
    }
  });
});
</script>
</body>
</html>
