<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>强化学习学习笔记</title>
  <link rel="stylesheet" href="style.css">
  <style>
    body { margin:0; font-family: 'Segoe UI', sans-serif; background-color:#121212; color:#e0e0e0; }
    header { background-color:#1e1e1e; padding:20px; text-align:center; }
    nav ul { display:flex; justify-content:center; list-style:none; padding:0; margin:0; }
    nav li { margin:0 15px; }
    nav a { color:#e0e0e0; text-decoration:none; font-weight:bold; }
    nav a:hover { color:#00bcd4; }
    main { padding:20px; max-width:900px; margin:auto; }
    .chapter { margin-bottom:40px; padding:20px; background-color:#1e1e1e; border-radius:10px; }
    .chapter h2 { color:#00bcd4; }
    pre { background:#2a2a2a; padding:10px; border-radius:5px; overflow-x:auto; }
    footer { text-align:center; padding:20px; background:#1e1e1e; margin-top:40px; }
  </style>
</head>
<body>
  <header>
    <h1>强化学习学习笔记</h1>
    <nav>
      <ul>
        <li><a href="#chapter0">环境 & GitHub</a></li>
        <li><a href="#chapter1">基本概念</a></li>
        <li><a href="#chapter2">MDP</a></li>
        <li><a href="#chapter3">算法基础</a></li>
        <li><a href="#chapter4">实践</a></li>
        <li><a href="#chapter5">深度强化学习</a></li>
        <li><a href="#chapter6">高级算法</a></li>
        <li><a href="#chapter7">实践项目</a></li>
        <li><a href="#chapter8">经典文献精读</a></li>
      </ul>
    </nav>
  </header>

  <main>
    <!-- 第零章 -->
    <section id="chapter0" class="chapter">
      <h2>第零章：环境配置 & GitHub 项目上传</h2>
      <p>首先创建项目文件夹，例如 G:\test，然后用管理员身份打开 Anaconda Prompt，配置清华镜像源：</p>
      <pre><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
conda config --set show_channel_urls yes</code></pre>
      <p>配置 pip 镜像：</p>
      <pre><code>pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></pre>
      <p>创建虚拟环境并安装核心包：</p>
      <pre><code>cd /d G:\test
conda create --prefix .\.venv python=3.11
conda activate G:\test\.venv
conda install numpy pandas matplotlib scikit-learn jupyterlab
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia</code></pre>
      <p>上传到 GitHub：</p>
      <pre><code>git init
git remote add origin https://github.com/HaoyunT/test.git
git checkout -b main
git add .
git commit -m "初始化项目"
git push -u origin main</code></pre>
    </section>

 <!-- 第一章 -->
<section id="chapter1" class="chapter">
  <h2>第一章：强化学习基本概念</h2>

  <p>强化学习（Reinforcement Learning, RL）中的核心概念如下，每一项都在智能体与环境的交互中起到关键作用：</p>

  <dl>
    <dt><b>智能体（Agent）</b></dt>
    <dd>
      强化学习中的决策执行主体。智能体在每个时间步 t 观察环境状态 <code>S_t</code>，选择动作 <code>A_t</code>，并通过奖励信号调整策略。  
      <em>示例：</em>在游戏中，玩家角色即为智能体。
    </dd>

    <dt><b>环境（Environment）</b></dt>
    <dd>
      智能体交互的对象，定义状态空间、动作空间和状态转移规则。在智能体执行动作后返回下一个状态 <code>S_{t+1}</code> 和即时奖励 <code>R_t</code>。  
      <em>示例：</em>游戏关卡、物理模拟器、金融市场等。
    </dd>

    <dt><b>状态（State）</b></dt>
    <dd>
      对环境在某一时刻的描述，包含智能体决策所需的全部信息。通常表示为向量或矩阵，满足马尔可夫性质。  
      <em>示例：</em>角色位置、敌人位置、道具信息等。
    </dd>

    <dt><b>动作（Action）</b></dt>
    <dd>
      智能体可在当前状态下执行的操作，作用于环境以改变状态。动作空间可为离散或连续。  
      <em>示例：</em>左右移动、跳跃、射击等操作。
    </dd>

    <dt><b>奖励（Reward）</b></dt>
    <dd>
      环境对智能体动作的即时反馈，用于衡量动作优劣。强化学习的目标是最大化未来折扣累计奖励 <code>G_t = Σ γ^k R_{t+k}</code>。  
      <em>示例：</em>吃金币 +10，击败敌人 +50，掉坑 -100。
    </dd>

    <dt><b>策略（Policy）</b></dt>
    <dd>
      智能体选择动作的规则或概率分布，表示为 <code>π(a|s)</code> 或确定性函数 <code>a = π(s)</code>。强化学习的目标是找到最优策略 <code>π*</code>，以最大化长期回报。  
      <em>示例：</em>给定状态选择最优动作的决策函数。
    </dd>

    <dt><b>价值函数（Value Function）</b></dt>
    <dd>
      衡量某状态或状态-动作对在长期交互中的预期回报。常用的有：
      <ul>
        <li>状态价值函数 <code>V^π(s) = E[G_t | S_t=s]</code></li>
        <li>状态-动作价值函数 <code>Q^π(s,a) = E[G_t | S_t=s, A_t=a]</code></li>
      </ul>
      价值函数用于策略评估与改进。
    </dd>
  </dl>

  <p><strong>概念关系概述：</strong>智能体感知环境状态 → 根据策略选择动作 → 动作作用于环境 → 环境返回奖励与下一个状态 → 智能体通过价值函数和奖励调整策略，以最大化累计回报。</p>
</section>

    </section>

    <!-- 第二章 -->
    <section id="chapter2" class="chapter">
      <h2>第二章：马尔可夫决策过程（MDP）</h2>
      <p>MDP 是强化学习的数学基础，包括：</p>
      <ul>
        <li>状态集合 S，动作集合 A</li>
        <li>状态转移概率 P(s'|s,a)</li>
        <li>奖励函数 R(s,a)</li>
        <li>折扣因子 γ</li>
        <li>回报（Return） G_t = Σ γ^k r_{t+k}</li>
      </ul>
    </section>

    <!-- 第三章 -->
    <section id="chapter3" class="chapter">
      <h2>第三章：算法基础</h2>
      <p>主要方法：</p>
      <ul>
        <li>价值迭代与策略迭代</li>
        <li>蒙特卡洛方法（MC）</li>
        <li>时间差分学习（TD）：SARSA、Q-learning</li>
      </ul>
    </section>

    <!-- 第四章 -->
    <section id="chapter4" class="chapter">
      <h2>第四章：实践（Gym 实现）</h2>
      <p>使用 Python Gym 库实现上述算法，进行实验与可视化。</p>
    </section>

    <!-- 第五章 -->
    <section id="chapter5" class="chapter">
      <h2>第五章：深度强化学习</h2>
      <p>深度强化学习方法包括 DQN 及其变体，策略梯度方法（REINFORCE、Actor-Critic、A2C、A3C）。</p>
    </section>

    <!-- 第六章 -->
    <section id="chapter6" class="chapter">
      <h2>第六章：高级算法</h2>
      <p>高级 RL 算法：TRPO、PPO 等。</p>
    </section>

    <!-- 第七章 -->
    <section id="chapter7" class="chapter">
      <h2>第七章：实践项目 & 比赛</h2>
      <p>在模拟环境中实现算法，参与 OpenAI Gym 等在线排行榜比赛。</p>
    </section>

    <!-- 第八章 -->
    <section id="chapter8" class="chapter">
      <h2>第八章：经典文献精读</h2>
      <p>精读 Sutton & Barto 的书籍，复现经典论文（如 DQN、PPO），完成习题并进行项目实践。</p>
    </section>
  </main>

  <footer>
    <p>© 2025 唐浩云 | Reinforcement Learning Journey</p>
  </footer>
</body>
</html>
