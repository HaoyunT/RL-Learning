<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>强化学习学习笔记</title>
<!-- MathJax -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<style>
/* ===== 全局 ===== */
body {
  margin:0;
  font-family:'Segoe UI','Roboto',sans-serif;
  background-color:#121212;
  color:#e0e0e0;
  line-height:1.7;
}
header {
  background-color:#1e1e1e;
  padding:20px;
  text-align:center;
}
nav ul {
  display:flex;
  justify-content:center;
  list-style:none;
  padding:0;
  margin:0;
  flex-wrap: wrap;
}
nav li { margin:5px 15px; }
nav a { color:#e0e0e0; text-decoration:none; font-weight:bold; }
nav a:hover { color:#00bcd4; }
main { padding:20px; max-width:900px; margin:auto; }
.chapter {
  margin-bottom:40px;
  padding:25px;
  background-color:#1e1e1e;
  border-radius:10px;
}
.chapter h2 {
  color:#00bcd4;
  border-bottom:2px solid #00bcd4;
  padding-bottom:5px;
}
dl dt { font-weight:bold; margin-top:15px; font-size:1.1rem; }
dl dd { margin-left:20px; margin-bottom:10px; color:#cfcfcf; }
dl dd ul { margin-left:20px; }
pre {
  background:#2a2a2a;
  padding:15px;
  border-radius:8px;
  overflow-x:auto;
  font-family:'Fira Code', monospace;
  font-size:0.95rem;
}
em { color:#ff9800; font-style:normal; }
footer { text-align:center; padding:20px; background:#1e1e1e; margin-top:40px; }
</style>
</head>
<body>

<header>
  <h1>强化学习学习笔记</h1>
  <nav>
    <ul>
      <li><a href="#chapter0">环境 & GitHub</a></li>
      <li><a href="#chapter1">基本概念</a></li>
      <li><a href="#chapter2">MDP</a></li>
    </ul>
  </nav>
</header>

<main>
<!-- 第零章 -->
<section id="chapter0" class="chapter">
  <h2>第零章：环境配置 & GitHub 项目上传</h2>
  <p>创建项目文件夹，如 G:\test，然后配置 Anaconda 清华镜像：</p>
  <pre><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
conda config --set show_channel_urls yes</code></pre>
  <p>配置 pip 镜像：</p>
  <pre><code>pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></pre>
  <p>创建虚拟环境并安装核心包：</p>
  <pre><code>cd /d G:\test
conda create --prefix .\.venv python=3.11
conda activate G:\test\.venv
conda install numpy pandas matplotlib scikit-learn jupyterlab
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia</code></pre>
  <p>上传到 GitHub：</p>
  <pre><code>git init
git remote add origin https://github.com/HaoyunT/test.git
git checkout -b main
git add .
git commit -m "初始化项目"
git push -u origin main</code></pre>
</section>

<!-- 第一章 -->
<section id="chapter1" class="chapter">
  <h2>第一章：强化学习基本概念</h2>
  <p>强化学习（Reinforcement Learning, RL）核心概念：</p>
  <dl>
    <dt>智能体（Agent）</dt>
    <dd>决策执行主体。在每个时间步 \(t\) 观察状态 \(S_t\)，选择动作 \(A_t\)，并通过奖励信号调整策略。<br><em>示例：</em>游戏中的玩家角色。</dd>

    <dt>环境（Environment）</dt>
    <dd>智能体交互对象，定义状态空间、动作空间和状态转移规则。动作后返回状态 \(S_{t+1}\) 和奖励 \(R_t\)。<br><em>示例：</em>游戏关卡或物理模拟器。</dd>

    <dt>状态（State）</dt>
    <dd>描述环境在某一时刻的特征，满足马尔可夫性质。<br><em>示例：</em>角色位置、敌人位置。</dd>

    <dt>动作（Action）</dt>
    <dd>智能体可执行的操作，改变环境状态。<br><em>示例：</em>左右移动、跳跃。</dd>

    <dt>奖励（Reward）</dt>
    <dd>环境对动作的反馈，用于衡量优劣。强化学习目标最大化累计奖励：
      <br>\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k}\)
    </dd>

    <dt>策略（Policy）</dt>
    <dd>选择动作的规则，\(\pi(a|s)\) 或 \(a = \pi(s)\)。目标是找到最优策略 \(\pi^*\)。</dd>

    <dt>价值函数（Value Function）</dt>
    <dd>衡量状态或状态-动作对的长期回报：
      <ul>
        <li>状态价值：\(V^\pi(s) = \mathbb{E}[G_t | S_t=s]\)</li>
        <li>动作价值：\(Q^\pi(s,a) = \mathbb{E}[G_t | S_t=s, A_t=a]\)</li>
      </ul>
    </dd>
  </dl>
</section>

<!-- 第二章 -->
<section id="chapter2" class="chapter">
  <h2>第二章：马尔可夫决策过程（MDP）</h2>
  <p>MDP 数学基础：</p>
  <ul>
    <li>状态集合 \(S\)，动作集合 \(A\)</li>
    <li>状态转移概率 \(P(s'|s,a)\)</li>
    <li>奖励函数 \(R(s,a)\)</li>
    <li>折扣因子 \(\gamma\)</li>
    <li>回报 \(G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}\)</li>
  </ul>
</section>

</main>
<footer>
  <p>© 2025 唐浩云 | Reinforcement Learning Journey</p>
</footer>

</body>
</html>
