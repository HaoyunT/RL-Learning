<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>强化学习笔记 | Yun</title>

<!-- 数学公式渲染 -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
 src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Prism.js 核心 + 主题 + 语言 -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-dracula.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
<!-- 语言支持 -->
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-javascript.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-typescript.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-bash.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-json.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-css.min.js"></script>
<!-- 行号插件 -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>


<style>
/* ===== 全局样式 ===== */
body {
  margin: 0;
  font-family: "Segoe UI", "Roboto", -apple-system, sans-serif;
  background: linear-gradient(135deg, #0f1419 0%, #1a1f2e 100%);
  color: #c8cdd3;
  line-height: 1.8;
  scroll-behavior: smooth;
}

/* 背景粒子 */
#trailCanvas {
  position: fixed;
  top: 0; left: 0;
  width: 100%; height: 100%;
  z-index: 0;
  pointer-events: none;
}

/* Header */
header {
  position: sticky;
  top: 0;
  z-index: 100;
  text-align: center;
  background: rgba(15, 20, 25, 0.95);
  padding: 25px 20px;
  backdrop-filter: blur(8px);
  border-bottom: 1px solid rgba(86, 171, 145, 0.2);
  box-shadow: 0 4px 12px rgba(0,0,0,0.4);
}
header h1 {
  font-size: 2.2rem;
  color: #56ab91;
  margin: 0;
  font-weight: 700;
  letter-spacing: 1px;
}

/* 导航栏 */
nav ul {
  list-style: none;
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  padding: 0;
  margin: 15px 0 0;
  gap: 8px;
}
nav li { margin: 0; }
nav a {
  color: #b0b5bb;
  text-decoration: none;
  font-weight: 600;
  font-size: 0.95rem;
  padding: 6px 14px;
  border-radius: 6px;
  transition: all 0.3s ease;
  background: rgba(86, 171, 145, 0.08);
  border: 1px solid rgba(86, 171, 145, 0.15);
}
nav a:hover, nav a.active {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.15);
  border-color: rgba(86, 171, 145, 0.4);
  box-shadow: 0 0 12px rgba(86, 171, 145, 0.2);
}

/* 容器布局 */
.container {
  display: flex;
  position: relative;
  z-index: 5;
  max-width: 1400px;
  margin: 30px auto;
  gap: 30px;
  padding: 0 20px;
}

/* 侧边栏 */
aside {
  position: sticky;
  top: 120px;
  width: 250px;
  height: fit-content;
  background: rgba(25, 30, 40, 0.8);
  border: 1px solid rgba(86, 171, 145, 0.15);
  border-radius: 10px;
  padding: 20px;
  backdrop-filter: blur(10px);
  flex-shrink: 0;
}

aside h3 {
  color: #56ab91;
  font-size: 1.1rem;
  margin: 0 0 15px 0;
  padding-bottom: 10px;
  border-bottom: 2px solid rgba(86, 171, 145, 0.3);
}

aside ul {
  list-style: none;
  padding: 0;
  margin: 0;
}

aside li {
  margin-bottom: 8px;
}

aside a {
  color: #b0b5bb;
  text-decoration: none;
  font-size: 0.9rem;
  display: block;
  padding: 8px 12px;
  border-radius: 6px;
  transition: all 0.2s ease;
  border-left: 3px solid transparent;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

aside a:hover, aside a.active {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
  border-left-color: #56ab91;
  padding-left: 15px;
}

/* 内容区 */
main {
  flex: 1;
  min-width: 0;
}

.chapter {
  background: rgba(25, 30, 40, 0.7);
  border: 1px solid rgba(86, 171, 145, 0.1);
  border-radius: 12px;
  padding: 35px;
  margin-bottom: 35px;
  transition: all 0.3s ease;
}

.chapter:hover {
  border-color: rgba(86, 171, 145, 0.25);
  box-shadow: 0 8px 24px rgba(86, 171, 145, 0.08);
  transform: translateY(-2px);
}

.chapter h2 {
  color: #56ab91;
  font-size: 1.8rem;
  margin: 0 0 20px 0;
  padding-bottom: 12px;
  border-bottom: 2px solid rgba(86, 171, 145, 0.2);
  font-weight: 700;
}

.chapter p {
  margin: 15px 0;
  color: #c8cdd3;
}

.chapter ul, .chapter ol {
  margin: 15px 0;
  padding-left: 30px;
}

.chapter li {
  margin-bottom: 8px;
  color: #c8cdd3;
}

/* 代码块样式 */
pre[class*="language-"] {
  background: linear-gradient(135deg, #0a0e15 0%, #141a25 100%) !important;
  padding: 20px !important;
  border-radius: 10px !important;
  overflow-x: auto !important;
  font-size: 0.92rem !important;
  line-height: 1.6 !important;
  border: 1px solid rgba(86, 171, 145, 0.2) !important;
  position: relative;
  font-family: 'Fira Code', 'Courier New', monospace !important;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3) !important;
}

pre[class*="language-"]::before {
  content: attr(class);
  position: absolute;
  top: 8px;
  right: 12px;
  font-size: 0.7rem;
  color: rgba(86, 171, 145, 0.6);
  font-weight: bold;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

/* 代码行号 */
pre.line-numbers {
  padding-left: 60px !important;
}

.line-numbers-rows {
  background: rgba(86, 171, 145, 0.08) !important;
  border-right: 2px solid rgba(86, 171, 145, 0.2) !important;
}

.line-numbers-rows > span:before {
  color: rgba(86, 171, 145, 0.6) !important;
  font-weight: bold;
}

pre code {
  color: inherit !important;
  background: none !important;
  font-family: inherit !important;
  font-size: inherit !important;
}

/* 代码块内各元素高亮 */
.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
  color: #6272a4;
}

.token.punctuation {
  color: #f8f8f2;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
  color: #ff79c6;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
  color: #50fa7b;
}

.token.operator,
.token.entity,
.token.url {
  color: #8be9fd;
}

.token.atrule,
.token.attr-value,
.token.keyword {
  color: #ff79c6;
}

.token.function,
.token.class-name {
  color: #8be9fd;
}

.token.regex,
.token.important,
.token.variable {
  color: #f1fa8c;
}

/* 代码块鼠标悬停效果 */
pre[class*="language-"]:hover {
  box-shadow: 
    0 12px 48px rgba(0, 188, 212, 0.25),
    inset 0 1px 0 rgba(255,255,255,0.15) !important;
  transform: translateY(-2px);
  transition: all 0.3s ease;
}

/* 滚动条美化 */
pre[class*="language-"]::-webkit-scrollbar {
  height: 10px;
  background: rgba(0, 188, 212, 0.1);
}

pre[class*="language-"]::-webkit-scrollbar-thumb {
  background: rgba(0, 188, 212, 0.5);
  border-radius: 5px;
  transition: background 0.3s;
}

pre[class*="language-"]::-webkit-scrollbar-thumb:hover {
  background: rgba(0, 188, 212, 0.8);
}


/* 表格优化 */
table {
  width: 100%;
  border-collapse: collapse;
  margin-top: 15px;
  background: rgba(255,255,255,0.05);
  border-radius: 8px;
  overflow: hidden;
}
table th, table td {
  padding: 10px 15px;
  border-bottom: 1px solid rgba(255,255,255,0.1);
}
table th {
  background: rgba(0,188,212,0.2);
  color: #00e5ff;
}

/* Footer */
footer {
  text-align: center;
  padding: 15px;
  margin: 40px 0 10px;
  font-size: 0.9rem;
  color: #aaa;
  background: rgba(20,20,20,0.7);
  border-radius: 8px;
}
footer span {
  color: #00bcd4;
}

/* ===== 统一章节样式（简化版） ===== */
.chapter h3 {
  padding-bottom: 8px;
  margin-top: 20px;
  border-bottom: 1px solid rgba(255,255,255,0.1);
  font-size: 1.1rem;
}

.chapter strong {
  font-weight: 600;
}

.chapter code {
  background: rgba(255,255,255,0.05);
  padding: 2px 6px;
  border-radius: 3px;
  font-size: 0.9em;
}

.chapter li {
  margin-bottom: 10px;
  line-height: 1.8;
  transition: all 0.2s ease;
}

.chapter li:hover {
  transform: translateX(3px);
}

.chapter table {
  border-collapse: collapse;
  width: 100%;
  margin: 15px 0;
}

.chapter table td, .chapter table th {
  border: 1px solid rgba(255,255,255,0.1);
  padding: 10px;
  text-align: left;
}

.chapter table th {
  background: rgba(255,255,255,0.05);
  font-weight: 600;
}

/* ===== 第一章（基本概念）- 翠绿 ===== */
#chapter1 h3, #chapter1 dt {
  color: #56ab91;
}

#chapter1 strong {
  color: #56ab91;
}

#chapter1 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第二章（贝尔曼方程）- 翠绿 ===== */
#chapter2 h3, #chapter2 dt {
  color: #56ab91;
}

#chapter2 strong {
  color: #56ab91;
}

#chapter2 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第三章（贝尔曼最优）- 翠绿 ===== */
#chapter3 h3, #chapter3 dt {
  color: #56ab91;
}

#chapter3 strong {
  color: #56ab91;
}

#chapter3 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第四章（策略迭代）- 翠绿 ===== */
#chapter4 h3 {
  color: #56ab91;
}

#chapter4 strong {
  color: #56ab91;
}

#chapter4 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第五章（值迭代）- 翠绿 ===== */
#chapter5 h3 {
  color: #56ab91;
}

#chapter5 strong {
  color: #56ab91;
}

#chapter5 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第六章（蒙特卡洛）- 翠绿 ===== */
#chapter6 h3 {
  color: #56ab91;
}

#chapter6 strong {
  color: #56ab91;
}

#chapter6 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

#chapter6 dt {
  color: #56ab91;
  font-weight: bold;
  margin-top: 10px;
  margin-bottom: 8px;
}

#chapter6 dd {
  background: rgba(86, 171, 145, 0.05);
  border-left: 3px solid #56ab91;
  padding: 10px 12px;
  margin-left: 0;
  margin-bottom: 12px;
  border-radius: 4px;
  font-size: 0.95rem;
}

/* ===== 第七章（时间差分）- 翠绿 ===== */
#chapter7 h3 {
  color: #56ab91;
}

#chapter7 strong {
  color: #56ab91;
}

#chapter7 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第八章（Q-learning）- 翠绿 ===== */
#chapter8 h3 {
  color: #56ab91;
}

#chapter8 strong {
  color: #56ab91;
}

#chapter8 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第九章（DQN）- 翠绿 ===== */
#chapter9 h3 {
  color: #56ab91;
}

#chapter9 strong {
  color: #56ab91;
}

#chapter9 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第十章（DDQN）- 翠绿 ===== */
#chapter10 h3 {
  color: #56ab91;
}

#chapter10 strong {
  color: #56ab91;
}

#chapter10 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第十一章（策略梯度）- 翠绿 ===== */
#chapter11 h3 {
  color: #56ab91;
}

#chapter11 strong {
  color: #56ab91;
}

#chapter11 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第十二章（Actor-Critic）- 翠绿 ===== */
#chapter12 h3 {
  color: #56ab91;
}

#chapter12 strong {
  color: #56ab91;
}

#chapter12 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

/* ===== 第十三章（A2C&A3C）- 翠绿 ===== */
#chapter13 h3 {
  color: #56ab91;
}

#chapter13 strong {
  color: #56ab91;
}

#chapter13 code {
  color: #56ab91;
  background: rgba(86, 171, 145, 0.1);
}

#chapter13 strong {
  color: #5dde95;
  font-weight: 600;
}

#chapter13 table td, #chapter13 table th {
  border-color: rgba(93, 222, 149, 0.3);
}

/* ===== 对比表格样式增强 ===== */
table {
  width: 100%;
  border-collapse: collapse;
  margin-top: 15px;
  background: linear-gradient(135deg, rgba(255,255,255,0.02) 0%, rgba(255,255,255,0.05) 100%);
  border-radius: 12px;
  overflow: hidden;
  box-shadow: 0 4px 20px rgba(0,0,0,0.3);
  transition: all 0.3s ease;
}

table:hover {
  box-shadow: 0 8px 30px rgba(0,0,0,0.4);
}

table th, table td {
  padding: 15px;
  border-bottom: 1px solid rgba(255,255,255,0.1);
  text-align: left;
}

table th {
  background: rgba(255, 255, 255, 0.05);
  color: #e0e0e0;
  font-weight: bold;
  font-size: 1.05rem;
}

table tr:hover {
  background: rgba(255, 255, 255, 0.05);
  transition: all 0.2s ease;
}

table td {
  color: #e0e0e0;
}

table tr:last-child th,
table tr:last-child td {
  border-bottom: none;
}

/* Footer */
footer {
  text-align: center;
  padding: 30px 20px;
  background: rgba(15, 20, 25, 0.95);
  border-top: 1px solid rgba(86, 171, 145, 0.15);
  color: #b0b5bb;
  margin-top: 60px;
  font-size: 0.9rem;
}

footer p {
  margin: 10px 0;
}

footer span {
  color: #56ab91;
  font-weight: 600;
}

/* 响应式设计 */
@media (max-width: 1024px) {
  .container {
    flex-direction: column;
    gap: 20px;
  }
  
  aside {
    position: relative;
    top: auto;
    width: 100%;
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
  }
  
  aside ul {
    grid-column: 1 / -1;
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
  }
  
  aside li {
    margin-bottom: 0;
  }
}

@media (max-width: 768px) {
  header h1 {
    font-size: 1.8rem;
  }
  
  nav ul {
    gap: 4px;
  }
  
  nav a {
    padding: 4px 10px;
    font-size: 0.85rem;
  }
  
  .container {
    padding: 0 10px;
  }
  
  .chapter {
    padding: 20px;
    margin-bottom: 20px;
  }
  
  aside h3 {
    font-size: 1rem;
  }
  
  aside a {
    font-size: 0.85rem;
  }
}

</style>
</head>

<body>
<canvas id="trailCanvas"></canvas>

<header>
  <h1>强化学习笔记</h1>
  <nav>
    <ul>
      <li><a href="#chapter0">环境配置</a></li>
      <li><a href="#chapter1">基本概念</a></li>
      <li><a href="#chapter2">贝尔曼方程</a></li>
      <li><a href="#chapter3">贝尔曼最优方程</a></li>
      <li><a href="#chapter4">策略迭代</a></li>
      <li><a href="#chapter5">值迭代</a></li>
      <li><a href="#chapter6">蒙特卡洛方法</a></li>
      <li><a href="#chapter7">时间差分学习</a></li>
      <li><a href="#chapter8">Q-learning</a></li>
      <li><a href="#chapter9">DQN</a></li>
      <li><a href="#chapter10">DDQN</a></li>
       <li><a href="#chapter11">策略梯度方法</a></li>
       <li><a href="#chapter12">Actor-Critic</a></li>
       <li><a href="#chapter13">A2C&A3C</a></li>
    </ul>
  </nav>
</header>

<div class="container">
  <aside>
    <h3>📚 目录</h3>
    <ul>
      <li><a href="#chapter0">环境配置</a></li>
      <li><a href="#chapter1">基本概念</a></li>
      <li><a href="#chapter2">贝尔曼方程</a></li>
      <li><a href="#chapter3">贝尔曼最优方程</a></li>
      <li><a href="#chapter4">策略迭代</a></li>
      <li><a href="#chapter5">值迭代</a></li>
      <li><a href="#chapter6">蒙特卡洛方法</a></li>
      <li><a href="#chapter7">时间差分学习</a></li>
      <li><a href="#chapter8">Q-learning</a></li>
      <li><a href="#chapter9">DQN</a></li>
      <li><a href="#chapter10">DDQN</a></li>
      <li><a href="#chapter11">策略梯度方法</a></li>
      <li><a href="#chapter12">Actor-Critic</a></li>
      <li><a href="#chapter13">A2C&A3C</a></li>
    </ul>
  </aside>

  <main>
  <section id="chapter0" class="chapter">
    <h2>第零章：环境配置 & GitHub 项目上传</h2>
    <pre><code class="language-python"># 配置镜像源（管理员身份打开Anaconda Prompt）
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
conda config --set show_channel_urls yes

#配置Pip镜像源
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# 创建虚拟环境（不需要与你本地版本一致，按照自己需求就行了）
cd /d G:\test
conda create --prefix .\.venv python=3.11
conda activate G:\test\.venv

#打开powershell查询一下自己的型号（后续安装pytorch版本需要，我的是12.4，因此我选的12.1就可以兼容）
conda activate G:\test\.venv
nvidia-smi 

# 安装依赖
conda install numpy pandas matplotlib pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia

# 上传到 GitHub
git init
git remote add origin https://github.com/HaoyunT/test.git
git checkout -b main
git add .
git commit -m "初始化项目"
git push -u origin main</code></pre>
  </section>

<!-- 第一章 -->
<section id="chapter1" class="chapter">
  <h2>第一章：强化学习基本概念</h2>
  <p>强化学习（Reinforcement Learning, RL）核心概念：</p>
  <dl>
    <dt>智能体（Agent）</dt>
    <dd>决策执行主体。在每个时间步 \(t\) 观察状态 \(S_t\)，选择动作 \(A_t\)，并通过奖励信号调整策略。<br><em>示例：</em>游戏中的玩家角色。</dd>

    <dt>环境（Environment）</dt>
    <dd>智能体交互对象，定义状态空间、动作空间和状态转移规则。动作后返回状态 \(S_{t+1}\) 和奖励 \(R_t\)。<br><em>示例：</em>游戏关卡或物理模拟器。</dd>

    <dt>状态（State）</dt>
    <dd>描述环境在某一时刻的特征，满足马尔可夫性质。<br><em>示例：</em>角色位置、敌人位置。</dd>

    <dt>动作（Action）</dt>
    <dd>智能体可执行的操作，改变环境状态。<br><em>示例：</em>左右移动、跳跃。</dd>

    <dt>奖励（Reward）</dt>
    <dd>环境对动作的反馈，用于衡量优劣。强化学习目标最大化累计奖励：
      <br>\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k}\)
    </dd>

    <dt>策略（Policy）</dt>
    <dd>选择动作的规则，\(\pi(a|s)\) 或 \(a = \pi(s)\)。目标是找到最优策略 \(\pi^*\)。</dd>

    <dt>价值函数（Value Function）</dt>
    <dd>衡量状态或状态-动作对的长期回报：
      <ul>
        <li>状态价值：\(V^\pi(s) = \mathbb{E}[G_t | S_t=s]\)</li>
        <li>动作价值：\(Q^\pi(s,a) = \mathbb{E}[G_t | S_t=s, A_t=a]\)</li>
      </ul>
    </dd>
  </dl>
</section>

<!-- 第二章 -->
<section id="chapter2" class="chapter">
  <h2>第二章：贝尔曼方程（Bellman Equation）</h2>
  <p>贝尔曼方程用于描述策略下状态或状态-动作的价值递归关系，是强化学习中<strong>策略评估的核心工具</strong>。</p>

  <dl>
    <dt><b>状态价值函数（State Value Function）</b></dt>
    <dd>
      对于策略 <code>π</code> 下的状态价值函数 <code>V^π(s)</code>：<br>
      <p>$$ V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s,a) \big[ R(s,a) + \gamma V^\pi(s') \big] $$</p>
      说明：当前状态价值 = 即时奖励 + 折扣后的未来价值期望。
    </dd>

    <dt><b>状态-动作价值函数（Action-Value Function）</b></dt>
    <dd>
      对于状态-动作对的价值函数 <code>Q^π(s,a)</code>：<br>
      <p>$$ Q^\pi(s,a) = R(s,a) + \gamma \sum_{s' \in S} \sum_{a' \in A} \pi(a'|s') Q^\pi(s',a') $$</p>
    </dd>

    <dt><b>状态价值函数与状态-动作价值函数的关系</b></dt>
    <dd>
      状态价值函数可以由状态-动作价值函数得到：<br>
      <p>$$ V^\pi(s) = \sum_{a \in A} \pi(a|s) Q^\pi(s,a) $$</p>
      解释：在状态 <code>s</code> 下，价值函数 <code>V^π(s)</code> 是策略选择动作后的期望 <code>Q^π(s,a)</code>。
    </dd>
  </dl>

  <p><strong>用途：</strong>策略评估、策略迭代、价值迭代的理论基础。</p>
</section>


<!-- 第三章 -->
<section id="chapter3" class="chapter">
  <h2>第三章：贝尔曼最优方程（Bellman Optimality Equation）</h2>

  <p>在寻找最优策略 <code>π*</code> 时，状态和状态-动作的价值函数满足<strong>贝尔曼最优方程</strong>，体现最优性原则。</p>

  <dl>
    <dt><b>最优状态价值函数</b></dt>
    <dd>
      <p>$$
      V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s,a) \big[ R(s,a) + \gamma V^*(s') \big]
      $$</p>
    </dd>

    <dt><b>最优状态-动作价值函数</b></dt>
    <dd>
      <p>$$
      Q^*(s,a) = R(s,a) + \gamma \sum_{s' \in S} \max_{a' \in A} Q^*(s',a')
      $$</p>
    </dd>
  </dl>

  <p><strong>关系总结：</strong></p>
  <ul>
    <li>第二章贝尔曼方程：给定策略 π → 计算 V^π 或 Q^π</li>
    <li>第三章贝尔曼最优方程：求最优策略 π* → V* 或 Q*</li>
  </ul>
</section>
<!-- 第四章 -->
<section id="chapter4" class="chapter">
  <h2>第四章：策略迭代（Policy Iteration）</h2>

  <p>策略迭代通过交替进行策略评估和策略改进来收敛到最优策略。</p>

  <ol>
    <li><strong>初始化策略：</strong>选择初始策略 <code>π_0</code></li>
    <li><strong>策略评估：</strong>计算状态价值函数：
      <p>$$
      v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}
      $$</p>
    </li>
    <li><strong>策略改进：</strong>更新策略：
      <p>$$
      \pi_{k+1} = \arg\max_\pi (r_\pi + \gamma P_\pi v_{\pi_k})
      $$</p>
    </li>
    <li>重复评估和改进，直到策略收敛。</li>
  </ol>
</section>


<!-- 第五章 -->
<section id="chapter5" class="chapter">
  <h2>第五章：值迭代（Value Iteration）</h2>

  <p>值迭代直接迭代状态价值函数，通过贝尔曼最优方程收敛到最优值函数，然后导出最优策略。</p>

  <ol>
    <li><strong>初始化价值函数：</strong>选择初始值 <code>v_0</code></li>
    <li><strong>迭代更新：</strong>使用贝尔曼最优方程：
      <p>$$
      v_{k+1} = \max_\pi (r_\pi + \gamma P_\pi v_k)
      $$</p>
    </li>
    <li><strong>策略导出：</strong>收敛后选择最优动作：
      <p>$$
      \pi^*(s) = \arg\max_a \sum_{s'} P(s'|s,a) \big[R(s,a) + \gamma v^*(s') \big]
      $$</p>
    </li>
  </ol>
</section>


<!-- 对比-->
<section id="chapterX" class="chapter">
  <h2>策略迭代与值迭代的对比</h2>

  <table border="1" cellpadding="8" cellspacing="0">
    <thead>
      <tr>
        <th></th>
        <th>Policy Iteration</th>
        <th>Value Iteration</th>
        <th>Comments</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>(1)Policy</td>
        <td>\(\pi_0\)</td>
        <td>N/A</td>
        <td></td>
      </tr>
      <tr>
        <td>(2)Value</td>
        <td>\(v_{\pi_0} = r_{\pi_0} + \gamma P_{\pi_0} v_{\pi_0}\)</td>
        <td>\(v_0 := v_{\pi_0}\)</td>
        <td></td>
      </tr>
      <tr>
        <td>(3)Policy</td>
        <td>\(\pi_1 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_{\pi_0} )\)</td>
        <td>\(\pi_1 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_0 )\)</td>
        <td>The two policies are the same</td>
      </tr>
      <tr>
        <td>(4)Value</td>
        <td>\(v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}\)</td>
        <td>\(v_1 = r_{\pi_1} + \gamma P_{\pi_1} v_0\)</td>
        <td>\(v_{\pi_1} \ge v_1 \text{ since } v_{\pi_1} \ge v_{\pi_0}\)</td>
      </tr>
      <tr>
        <td>5) Policy</td>
        <td>\(\pi_2 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_{\pi_1} )\)</td>
        <td>\(\pi'_2 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_1 )\)</td>
        <td></td>
      </tr>
    </tbody>
  </table>
</section>

 <!-- 第六章 -->
<section id="chapter6" class="chapter">
  <h2>第六章：蒙特卡洛方法（Monte Carlo Methods）</h2>

  <p>蒙特卡洛方法（Monte Carlo，简称 <b>MC</b>）是强化学习中一种基于<strong>采样</strong>的策略评估与优化方法。
  与动态规划不同，MC 不依赖环境的状态转移概率模型 <code>P(s'|s,a)</code>，而是通过反复从策略 <code>π</code> 生成完整回合（Episode），
  根据实际获得的回报估计状态或状态-动作价值。</p>

  <h3>一、基本思想</h3>
  <p>给定策略 \( \pi \)，智能体与环境交互多次，得到若干完整回合：</p>
  <p>$$
  (S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_T)
  $$</p>
  <p>对于每个回合，定义从时间步 \(t\) 开始的累计回报（Return）：</p>
  <p>$$
  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}
  $$</p>
  <p>MC 方法通过多次采样，利用这些 \(G_t\) 的平均值来近似价值函数。</p>

  <dl>
    <dt><b>状态价值估计：</b></dt>
    <dd>$$
    V(s) = \mathbb{E}_\pi[G_t \mid S_t = s] \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G^{(i)}(s)
    $$</dd>

    <dt><b>动作价值估计：</b></dt>
    <dd>$$
    Q(s,a) = \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a] \approx \frac{1}{N(s,a)} \sum_{i=1}^{N(s,a)} G^{(i)}(s,a)
    $$</dd>
  </dl>

  <h3>二、首次访问与每次访问</h3>
  <ul>
    <li><b>First-Visit MC：</b> 仅在一个回合中第一次访问状态（或状态-动作对）时进行更新。</li>
    <li><b>Every-Visit MC：</b> 对回合中每次访问状态（或状态-动作对）都进行更新。</li>
  </ul>
  <p>二者最终都会收敛到真实的价值函数，但首次访问 MC 方差略小。</p>

  <h3>三、蒙特卡洛策略评估算法</h3>
  <p><strong>基本思想：</strong>给定策略 π，评估其状态价值函数 V(s)</p>
  <ol>
    <li><strong>初始化：</strong>对所有状态 s，设 V(s) = 0，同时记录每个状态的回报列表 Returns(s)</li>
    <li><strong>采集数据：</strong>与环境交互多次，每次产生一个完整回合 (S₀, A₀, R₁, S₁, ..., S_T)</li>
    <li><strong>计算回报：</strong>对每个回合，从末尾向前计算累计回报 G_t = R_{t+1} + γR_{t+2} + ...</li>
    <li><strong>策略评估（首次访问）：</strong>对每个状态，仅在该回合中第一次出现时：
      <ul>
        <li>将该时刻的回报 G_t 加入 Returns(S_t)</li>
        <li>更新价值函数：V(S_t) = Returns(S_t) 的平均值</li>
      </ul>
    </li>
    <li><strong>重复：</strong>重复采集和更新过程，V(s) 逐渐逼近真实期望回报</li>
  </ol>
  <p style="margin-top: 15px;">这个算法是<strong>无模型、基于采样</strong>的方法——只需要实际回合数据，不需要知道环境的状态转移模型。</p>

  <h3>四、蒙特卡洛控制（MC Control）</h3>
  <p>在策略评估的基础上，MC 还可以实现<strong>策略改进</strong>，形成一个与策略迭代类似的过程：</p>
  <ol>
    <li>评估当前策略 \(\pi\)：使用 MC 方法估计 \(Q^\pi(s,a)\)</li>
    <li>改进策略：采用贪婪或 ε-贪婪方式更新策略
      <p>$$
      \pi'(s) = \arg\max_a Q(s,a)
      $$</p>
    </li>
    <li>重复以上过程，直到收敛</li>
  </ol>

  <h3>五、ε-贪婪（ε-Greedy）探索策略</h3>
  <p>为了避免过早陷入次优策略，MC 控制中常用 <b>ε-贪婪策略</b> 进行探索：</p>
  <p>$$
  \pi(a|s) = 
  \begin{cases}
  1 - \varepsilon + \frac{\varepsilon}{|A(s)|}, & a = \arg\max_{a'} Q(s,a') \\
  \frac{\varepsilon}{|A(s)|}, & \text{否则}
  \end{cases}
  $$</p>
  <p>其中 \(\varepsilon \in [0,1]\) 表示随机探索概率。</p>

  <h3>六、MC 控制示例代码</h3>
<pre><code class="language-python">
import numpy as np
from collections import defaultdict

class MCControl:
    def __init__(self, num_states, num_actions, gamma=0.99, epsilon=0.1, alpha=0.1):
        self.num_states = num_states
        self.num_actions = num_actions
        self.gamma = gamma
        self.epsilon = epsilon
        self.alpha = alpha
        self.Q = defaultdict(lambda: np.zeros(num_actions))
        self.returns = defaultdict(list)  # 存储每个(s,a)的回报列表
        
    def epsilon_greedy(self, state):
        """ε-贪婪策略选择动作"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.num_actions)
        else:
            return np.argmax(self.Q[state])
    
    def train_episode(self, env):
        """运行一个完整回合并更新Q值"""
        trajectory = []
        state = env.reset()
        done = False
        
        # 收集回合轨迹
        while not done:
            action = self.epsilon_greedy(state)
            next_state, reward, done, _ = env.step(action)
            trajectory.append((state, action, reward))
            state = next_state
        
        # 从后向前计算回报并更新Q值
        G = 0
        visited_pairs = set()
        for t in range(len(trajectory) - 1, -1, -1):
            state, action, reward = trajectory[t]
            G = reward + self.gamma * G
            
            # 只在状态-动作对第一次出现时更新（First-Visit MC）
            if (state, action) not in visited_pairs:
                visited_pairs.add((state, action))
                self.returns[(state, action)].append(G)
                self.Q[state][action] = np.mean(self.returns[(state, action)])
    
    def train(self, env, num_episodes=1000):
        """训练过程"""
        rewards = []
        for episode in range(num_episodes):
            episode_reward = sum(r for _, _, r in self.collect_episode(env))
            rewards.append(episode_reward)
            self.train_episode(env)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        return rewards
    
    def get_policy(self):
        """获取贪婪策略"""
        policy = {}
        for state in self.Q:
            policy[state] = np.argmax(self.Q[state])
        return policy
  </code></pre>


  <h3>七、MC 方法的特点与局限</h3>
  <table border="1" cellpadding="8" cellspacing="0">
    <thead>
      <tr><th>优点</th><th>缺点</th></tr>
    </thead>
    <tbody>
      <tr>
        <td>无需知道环境模型（P、R）</td>
        <td>必须等待完整回合结束，不能用于持续任务</td>
      </tr>
      <tr>
        <td>实现简单，概念直观</td>
        <td>收敛速度较慢，方差大</td>
      </tr>
      <tr>
        <td>适合离线模拟环境（如游戏）</td>
        <td>对长期任务或高维状态空间不适用</td>
      </tr>
    </tbody>
  </table>

  <h3>八、小结</h3>
  <ul>
    <li>MC 是基于采样的<strong>策略评估与控制</strong>方法。</li>
    <li>依赖回合结束的实际回报，而非估计的下一步价值。</li>
    <li>与动态规划相比，不需知道模型，但收敛慢。</li>
    <li>是理解时序差分（TD）学习的重要过渡。</li>
  </ul>

</section>
<!-- 第七章 -->
<section id="chapter7" class="chapter">
  <h2>第七章：时间差分学习（TD Learning）</h2>

  <p>时间差分学习（Temporal Difference，简称 <b>TD</b>）是强化学习中结合了
  <strong>蒙特卡洛方法</strong>与<strong>动态规划</strong>的策略评估与控制方法。
  与蒙特卡洛不同，TD 可以在回合未结束时就更新状态价值，因此属于<strong>在线更新</strong>方法。</p>

  <h3>一、TD(0)核心公式</h3>
  <p>单步TD更新状态价值函数公式：</p>
  <p>$$
  V(s_t) \leftarrow V(s_t) + \alpha \big[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \big]
  $$</p>
  <p>其中：</p>
  <ul>
    <li><code>V(s_t)</code>：当前状态的价值估计</li>
    <li><code>r_{t+1}</code>：当前动作的即时奖励</li>
    <li><code>γ</code>：折扣因子</li>
    <li><code>α</code>：学习率</li>
    <li><code>δ_t = r_{t+1} + γ V(s_{t+1}) - V(s_t)</code>：TD误差</li>
  </ul>

  <h3>二、TD(0)算法流程</h3>
  <p><strong>TD 与 MC 最大的区别：不等待回合结束，在每一步就进行更新！</strong></p>
  <ol>
    <li><strong>初始化：</strong>对所有状态 s，设 V(s) = 0</li>
    <li><strong>交互一步：</strong>在状态 s_t 执行动作 a_t，观察即时奖励 r_{t+1} 和下一状态 s_{t+1}</li>
    <li><strong>计算 TD 误差：</strong>δ_t = r_{t+1} + γ·V(s_{t+1}) - V(s_t)</li>
    <li><strong>更新价值函数：</strong>V(s_t) ← V(s_t) + α·δ_t（使用学习率 α）</li>
    <li><strong>继续交互：</strong>s_t ← s_{t+1}，重复步骤 2-4，不需要等待回合结束</li>
  </ol>
  <p style="margin-top: 15px;"><strong>关键优势：</strong></p>
  <ul>
    <li>✓ 可以进行 <strong>在线学习</strong>，回合中实时更新</li>
    <li>✓ 可以用于 <strong>持续任务</strong>（无终止状态的任务）</li>
    <li>✓ 收敛速度比 MC 更快</li>
    <li>✓ 在不确定环境中更稳定</li>
  </ul>

  <h3>三、TD学习与MC方法对比</h3>
  <table>
    <tr><th>方法</th><th>更新时机</th><th>是否依赖环境模型</th><th>偏差/方差</th></tr>
    <tr><td>蒙特卡洛（MC）</td><td>回合结束</td><td>否</td><td>无偏，方差大</td></tr>
    <tr><td>TD(0)</td><td>每一步</td><td>否</td><td>有偏，方差小</td></tr>
  </table>
  <p>MC使用完整回报 <code>G_t</code> 更新价值，而TD使用下一状态估计 <code>V(s_{t+1})</code> 引导当前更新。</p>

  <h3>四、TD(λ)与资格迹</h3>
  <p>TD(λ)引入了 <b>资格迹（eligibility trace）</b>，可以结合多步信息更新价值：</p>
  <p>$$
  V(s) \leftarrow V(s) + \alpha \delta_t e(s)
  $$</p>
  <ul>
    <li><code>e(s)</code>：状态的资格迹，记录过去时间步的重要性</li>
    <li>λ ∈ [0,1] 控制多步信息衰减：λ=0 → TD(0)，λ=1 → 接近MC</li>
  </ul>

  <h3>五、TD控制与ε-贪婪策略</h3>
  <p>在策略评估基础上，TD也可进行策略改进，实现TD控制：</p>
  <ol>
    <li>评估当前策略：使用TD估计 Q(s,a)</li>
    <li>改进策略：采用ε-贪婪选择动作
      <p>$$
      \pi(a|s) = 
      \begin{cases}
      1 - \varepsilon + \frac{\varepsilon}{|A(s)|}, & a = \arg\max_{a'} Q(s,a') \\
      \frac{\varepsilon}{|A(s)|}, & \text{否则}
      \end{cases}
      $$</p>
    </li>
    <li>重复以上过程直到收敛</li>
  </ol>

  <h3>六、TD 控制示例代码（SARSA）</h3>
  <pre><code class="language-python">
import numpy as np
from collections import defaultdict

class SARSA:
    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.num_states = num_states
        self.num_actions = num_actions
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = defaultdict(lambda: np.zeros(num_actions))
    
    def epsilon_greedy(self, state):
        """ε-贪婪策略"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.num_actions)
        else:
            return np.argmax(self.Q[state])
    
    def train_step(self, state, action, reward, next_state, next_action, done):
        """单步SARSA更新"""
        if done:
            target = reward
        else:
            target = reward + self.gamma * self.Q[next_state][next_action]
        
        # TD误差
        td_error = target - self.Q[state][action]
        
        # Q值更新
        self.Q[state][action] += self.alpha * td_error
    
    def train_episode(self, env):
        """运行一个完整回合"""
        state = env.reset()
        action = self.epsilon_greedy(state)
        done = False
        episode_reward = 0
        
        while not done:
            next_state, reward, done, _ = env.step(action)
            next_action = self.epsilon_greedy(next_state)
            
            # 更新Q值
            self.train_step(state, action, reward, next_state, next_action, done)
            
            episode_reward += reward
            state = next_state
            action = next_action
        
        return episode_reward
    
    def train(self, env, num_episodes=1000):
        """训练过程"""
        rewards = []
        for episode in range(num_episodes):
            reward = self.train_episode(env)
            rewards.append(reward)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        return rewards
  </code></pre>

  <h3>七、Bootstrapping（自举/引导）解释</h3>
  <p><b>Bootstrapping</b> 是TD方法的核心思想：</p>
  <p>TD不是等到完整回合获得真实回报 <code>G_t</code> 才更新价值，而是使用对下一状态价值的估计 <code>V(s_{t+1})</code> 来引导当前状态价值更新：</p>
  <p>$$
  V(s_t) \leftarrow V(s_t) + \alpha \big[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \big]
  $$</p>
  <p>这种方式称为“自举”，直观理解为：</p>
  <ul>
    <li>用当前已有的估计去改进自身</li>
    <li>实现在线、逐步更新，不依赖整回合完整信息</li>
    <li>优点：收敛快、可在线学习；缺点：初始估计不准可能引入偏差</li>
  </ul>

</section>
<!-- 第八章 -->
<section id="chapter8" class="chapter">
  <h2>第八章：Q-learning</h2>

  <p>Q-learning 是强化学习中最经典的 <strong>值迭代算法</strong>，用于学习最优动作价值函数 <code>Q*(s,a)</code>，从而间接得到最优策略：
  </p>
  <p>
  $$
  \pi^*(s) = \arg\max_a Q^*(s,a)
  $$
  </p>
  <p>特点：</p>
  <ul>
    <li>属于 <strong>off-policy</strong> 算法：更新 Q 值时使用最大化未来 Q 值，而行为策略可以带探索</li>
    <li>目标是找到能获得最大累积折扣奖励的动作</li>
  </ul>

  <h3>一、核心公式</h3>
  <p>Q-learning 核心更新公式：</p>
  <p>
  $$
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \big[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \big]
  $$
  </p>
  <table>
    <tr><th>符号</th><th>含义</th></tr>
    <tr><td>s<sub>t</sub>, a<sub>t</sub></td><td>当前状态和动作</td></tr>
    <tr><td>r<sub>t+1</sub></td><td>即时奖励</td></tr>
    <tr><td>s<sub>t+1</sub></td><td>下一状态</td></tr>
    <tr><td>α</td><td>学习率（0.1~0.5，一般设置）</td></tr>
    <tr><td>γ</td><td>折扣因子（0.9~0.99，一般设置）</td></tr>
    <tr><td>max Q(s<sub>t+1</sub>,a')</td><td>未来状态最优动作价值</td></tr>
  </table>

  <h3>二、算法流程</h3>
  <p><strong>Q-learning 是 Off-policy 方法，行为策略可以探索，但学习的是最优策略。</strong></p>
  <ol>
    <li><strong>初始化：</strong>对所有状态-动作对 (s,a)，设 Q(s,a) = 0</li>
    <li><strong>策略选择：</strong>使用 ε-greedy 策略从状态 s 选择动作 a
      <ul>
        <li>以概率 ε 随机选择（探索）</li>
        <li>以概率 1-ε 选择 Q 值最大的动作（利用）</li>
      </ul>
    </li>
    <li><strong>执行并观察：</strong>执行动作 a，得到奖励 r 和下一状态 s'</li>
    <li><strong>计算目标 Q 值：</strong>y = r + γ·max_{a'} Q(s',a')
      <ul>
        <li>虽然我们用 ε-greedy 探索，但这里取 max 说明我们朝着最优策略学习</li>
      </ul>
    </li>
    <li><strong>更新 Q 值：</strong>Q(s,a) ← Q(s,a) + α·(y - Q(s,a))</li>
    <li><strong>重复：</strong>s ← s'，继续交互和更新</li>
  </ol>
  <p style="margin-top: 15px;"><strong>学习率 α、折扣因子 γ、探索率 ε 的作用：</strong></p>
  <ul>
    <li>α（学习率）：0.1~0.5，控制对新信息的吸收程度</li>
    <li>γ（折扣因子）：0.9~0.99，决定对未来奖励的重视程度</li>
    <li>ε（探索率）：通常从 1.0 衰减到 0.01~0.1，逐步从探索转向利用</li>
  </ul>

  <h3>三、特点与注意事项</h3>
  <ul>
    <li>适用场景：离散状态和动作空间小的任务</li>
    <li>探索策略：ε-greedy 或 Boltzmann，可随训练衰减 ε</li>
    <li>学习率 α：一般 0.1~0.5，太大不稳定，太小收敛慢</li>
    <li>折扣因子 γ：一般 0.9~0.99</li>
    <li>收敛性：每个状态动作对被充分访问，且 α 衰减时可收敛</li>
  </ul>

  <h3>四、Python 示例</h3>
  <pre><code class="language-python">
import numpy as np
import gym

class QLearning:
    """Q-learning 算法实现"""
    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.n_states = n_states
        self.n_actions = n_actions
        self.alpha = alpha  # 学习率
        self.gamma = gamma  # 折扣因子
        self.epsilon = epsilon  # 探索率
        self.Q = np.zeros((n_states, n_actions))
    
    def select_action(self, state):
        """ε-greedy 策略选择动作"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)
        else:
            return np.argmax(self.Q[state])
    
    def update(self, state, action, reward, next_state, done):
        """Q 值更新"""
        target = reward + self.gamma * np.max(self.Q[next_state]) * (1 - done)
        td_error = target - self.Q[state, action]
        self.Q[state, action] += self.alpha * td_error
    
    def train(self, env, n_episodes=2000, max_steps=100):
        """训练智能体"""
        for episode in range(n_episodes):
            state = env.reset()
            if isinstance(state, tuple):
                state = state[0]  # 兼容新版 gym
            done = False
            
            for _ in range(max_steps):
                action = self.select_action(state)
                result = env.step(action)
                next_state = result[0]
                reward = result[1]
                done = result[2]
                
                self.update(state, action, reward, next_state, done)
                state = next_state
                if done:
                    break
        
        return self.Q


# 使用示例
env = gym.make('FrozenLake-v1', is_slippery=False)
n_states = env.observation_space.n
n_actions = env.action_space.n

agent = QLearning(n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1)
Q_table = agent.train(env, n_episodes=2000, max_steps=100)

print("训练完成！")
print("Q 表形状：", Q_table.shape)
print("Q 表：\n", Q_table)
  </code></pre>

  <h3>五、参数说明</h3>
  <table>
    <tr><th>参数</th><th>建议范围</th><th>说明</th></tr>
    <tr><td>α（学习率）</td><td>0.1~0.5</td><td>控制 Q 更新幅度</td></tr>
    <tr><td>γ（折扣因子）</td><td>0.9~0.99</td><td>长期奖励重要性</td></tr>
    <tr><td>ε（探索率）</td><td>0.05~0.2</td><td>ε-greedy 探索概率，可随训练衰减</td></tr>
    <tr><td>n_episodes</td><td>500~5000</td><td>根据任务复杂度调整</td></tr>
    <tr><td>max_steps</td><td>100~1000</td><td>每回合最大步数，避免无限循环</td></tr>
  </table>

</section>
<!-- 第九章 -->
<section id="chapter9" class="chapter">
  <h2>第九章：DQN（Deep Q-Network）</h2>

  <p>DQN（Deep Q-Network）是强化学习中用于高维状态空间的经典算法，核心思想是用<strong>深度神经网络</strong>逼近动作价值函数 Q(s,a;θ)，从而间接得到最优策略。</p>
  
  <h3>一、背景与原理</h3>
  <ul>
    <li>传统 Q-learning 在大或连续状态空间中无法使用 Q 表</li>
    <li>DQN 使用神经网络将状态映射到动作价值</li>
    <li>适用于高维输入，如图像环境（Atari 游戏）</li>
    <li>核心挑战：训练稳定性，需要经验回放和目标网络</li>
  </ul>

  <h3>二、核心公式</h3>
  <p>DQN 最小化 TD 误差损失：</p>
  <p>$$
  L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \Big[ \big( y - Q(s,a;\theta) \big)^2 \Big]
  $$</p>
  <p>其中目标 Q 值为：</p>
  <p>$$
  y = r + \gamma \max_{a'} Q(s',a'; \theta^-)
  $$</p>
  <ul>
    <li>Q(s,a;θ)：当前 Q 网络预测</li>
    <li>y：目标 Q 值，由目标网络 θ⁻ 提供</li>
    <li>𝒟：经验回放池</li>
    <li>γ：折扣因子</li>
  </ul>

  <h3>三、训练流程</h3>
  <ol>
    <li>初始化 Q 网络 Q(s,a;θ) 和目标网络 Q(s,a;θ⁻)</li>
    <li>与环境交互，按 ε-greedy 策略选择动作，存入经验回放池</li>
    <li>从经验回放中采样小批量数据训练 Q 网络，最小化 TD loss</li>
    <li>定期更新目标网络 θ⁻ ← θ</li>
    <li>重复以上步骤，直到策略收敛</li>
  </ol>

  <h3>四、核心技术点</h3>
  <table>
    <tr><th>技术</th><th>作用</th></tr>
    <tr><td>经验回放 Replay Buffer</td><td>打破数据相关性，提高样本利用率，稳定训练</td></tr>
    <tr><td>目标网络 Target Network</td><td>减缓训练震荡，保证目标稳定</td></tr>
    <tr><td>ε-greedy 策略</td><td>平衡探索和利用</td></tr>
    <tr><td>归一化 / 图像预处理</td><td>降低高维输入维度，提升训练效率</td></tr>
  </table>

  <h3>五、DQN 示例代码</h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple

Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))

class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.memory = deque(maxlen=capacity)
    
    def add(self, state, action, reward, next_state, done):
        self.memory.append(Transition(state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        return zip(*batch)
    
    def __len__(self):
        return len(self.memory)

class DQN:
    def __init__(self, state_dim, action_dim, gamma=0.99, epsilon=1.0, alpha=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.min_epsilon = 0.01
        self.epsilon_decay = 0.995
        
        # Q网络和目标网络
        self.Q = QNetwork(state_dim, action_dim)
        self.Q_target = QNetwork(state_dim, action_dim)
        self.Q_target.load_state_dict(self.Q.state_dict())
        
        # 优化器
        self.optimizer = optim.Adam(self.Q.parameters(), lr=alpha)
        self.replay_buffer = ReplayBuffer(capacity=10000)
        self.loss_fn = nn.MSELoss()
    
    def select_action(self, state):
        """ε-greedy 策略选择动作"""
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            with torch.no_grad():
                q_values = self.Q(state_tensor)
            return q_values.argmax(dim=1).item()
    
    def train_step(self, batch_size):
        """从经验回放中采样并训练"""
        if len(self.replay_buffer) < batch_size:
            return
        
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
        
        # 转换为张量
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(np.array(actions))
        rewards = torch.FloatTensor(np.array(rewards))
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(np.array(dones))
        
        # 计算当前Q值
        q_values = self.Q(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # 计算目标Q值
        with torch.no_grad():
            max_next_q = self.Q_target(next_states).max(dim=1)[0]
            target_q = rewards + self.gamma * max_next_q * (1 - dones)
        
        # 计算损失
        loss = self.loss_fn(q_values, target_q)
        
        # 反向传播
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, env, num_episodes=1000, batch_size=32, target_update_freq=500):
        """完整训练流程"""
        rewards = []
        step_count = 0
        
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            episode_reward = 0
            
            while not done:
                # 选择动作
                action = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                # 存储经验
                self.replay_buffer.add(state, action, reward, next_state, done)
                
                # 训练
                self.train_step(batch_size)
                
                episode_reward += reward
                state = next_state
                step_count += 1
                
                # 定期更新目标网络
                if step_count % target_update_freq == 0:
                    self.Q_target.load_state_dict(self.Q.state_dict())
            
            rewards.append(episode_reward)
            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Epsilon: {self.epsilon:.3f}")
        
        return rewards
  </code></pre>

  <h3>六、参数说明</h3>
  <table>
    <tr><th>参数</th><th>建议范围</th><th>说明</th></tr>
    <tr><td>γ（折扣因子）</td><td>0.95~0.99</td><td>长期奖励权重</td></tr>
    <tr><td>学习率 lr</td><td>1e-4~1e-3</td><td>Adam 或 RMSProp 优化器</td></tr>
    <tr><td>ε 初值</td><td>1.0</td><td>初期高探索</td></tr>
    <tr><td>ε 最小值</td><td>0.01~0.1</td><td>保持一定探索</td></tr>
    <tr><td>ε 衰减率</td><td>0.995~0.999</td><td>逐步降低 ε</td></tr>
    <tr><td>经验回放大小</td><td>1e4~1e6</td><td>根据环境大小选择</td></tr>
    <tr><td>批量大小 batch_size</td><td>32~64</td><td>每次训练采样量</td></tr>
    <tr><td>目标网络更新频率</td><td>500~10000 steps</td><td>定期同步 Q 网络到目标网络</td></tr>
  </table>

  <h3>七、训练完成判断</h3>
  <ul>
    <li><strong>累计奖励稳定：</strong>连续若干回合平均奖励变化很小</li>
    <li><strong>测试环境表现：</strong>使用纯利用策略测试平均成功率或完成率达到目标</li>
    <li><strong>损失收敛：</strong>观察 TD loss 基本稳定</li>
    <li><strong>ε-greedy 探索率较低：</strong>策略主要依赖网络估计动作，测试表现稳定</li>
  </ul>
  <pre><code class="language-python">
# 每隔 N 回合测试策略
if episode % test_interval == 0:
    total_reward = 0
    for _ in range(test_episodes):
        state = env.reset()
        done = False
        while not done:
            action = np.argmax(Q.predict(state))
            state, reward, done, _ = env.step(action)
            total_reward += reward
    avg_reward = total_reward / test_episodes
    if avg_reward >= target_reward:
        print("训练完成，策略收敛")
        break
  </code></pre>

</section>

<!-- 第十章 -->
<section id="chapter10" class="chapter">
  <h2>第十章：DDQN（Double Deep Q-Network）</h2>

  <p>DDQN 是 DQN 的改进版本，用于解决 DQN 在训练中存在的<strong>Q 值过估计（Overestimation）</strong>问题。核心思想是将动作选择与动作评价分开，提高训练稳定性。</p>

  <h3>一、背景与原理</h3>
  <ul>
    <li>DQN 使用 max(Q) 同时选择最优动作和估计动作价值，容易高估 Q 值。</li>
    <li>DDQN 将动作选择和动作评价分开：<strong>主网络选择动作</strong>，<strong>目标网络评价动作价值</strong>。</li>
    <li>这样可以降低 max 操作对偶然高估的放大效应，提高训练稳定性。</li>
  </ul>

  <h3>二、核心公式</h3>
  <p>DDQN 目标 Q 值计算：</p>
  <p>$$
  y^{DDQN} = r + \gamma Q(s', \arg\max_{a'} Q(s', a'; \theta), \theta^-)
  $$</p>
  <ul>
    <li>动作选择使用主网络参数 <code>θ</code></li>
    <li>动作评价使用目标网络参数 <code>θ^-</code></li>
    <li>损失函数与 DQN 一致：
      <p>$$
      L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \Big[ (y^{DDQN} - Q(s,a;\theta))^2 \Big]
      $$</p>
    </li>
  </ul>

  <h3>三、训练流程</h3>
  <ol>
    <li>初始化主网络 Q(s,a;θ) 和目标网络 Q(s,a;θ⁻)</li>
    <li>与环境交互，使用 ε-greedy 策略选择动作，存入经验回放池</li>
    <li>从经验回放中采样小批量训练，使用 DDQN 目标更新主网络参数 θ</li>
    <li>定期同步目标网络 θ⁻ ← θ</li>
    <li>重复以上步骤，直到训练完成</li>
  </ol>

  <h3>四、DDQN 与 DQN 区别</h3>
  <table>
    <tr><th>特性</th><th>DQN</th><th>DDQN</th></tr>
    <tr><td>目标计算</td><td>y = r + γ max_a Q(s',a;θ⁻)</td><td>y = r + γ Q(s', argmax_a Q(s',a;θ), θ⁻)</td></tr>
    <tr><td>选择动作</td><td>目标网络</td><td>主网络</td></tr>
    <tr><td>评价动作</td><td>目标网络</td><td>目标网络</td></tr>
    <tr><td>过估计问题</td><td>存在</td><td>大幅减轻</td></tr>
    <tr><td>训练稳定性</td><td>一般</td><td>更稳</td></tr>
  </table>

  <h3>五、DDQN 示例代码</h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

class DDQN:
    def __init__(self, state_dim, action_dim, gamma=0.99, epsilon=1.0, alpha=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.min_epsilon = 0.01
        self.epsilon_decay = 0.995
        
        # Q网络和目标网络
        self.Q = self._build_network(state_dim, action_dim)
        self.Q_target = self._build_network(state_dim, action_dim)
        self.Q_target.load_state_dict(self.Q.state_dict())
        
        self.optimizer = optim.Adam(self.Q.parameters(), lr=alpha)
        self.replay_buffer = deque(maxlen=10000)
        self.loss_fn = nn.MSELoss()
    
    def _build_network(self, state_dim, action_dim):
        return nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def select_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            q_values = self.Q(state_tensor)
        return q_values.argmax(dim=1).item()
    
    def train_step(self, batch_size):
        if len(self.replay_buffer) < batch_size:
            return
        
        indices = np.random.choice(len(self.replay_buffer), batch_size, replace=False)
        batch = [self.replay_buffer[i] for i in indices]
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(np.array(actions))
        rewards = torch.FloatTensor(np.array(rewards))
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(np.array(dones))
        
        # 当前Q值
        q_values = self.Q(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # DDQN核心：用Q网络选择最优动作，用Q_target网络评估价值
        with torch.no_grad():
            # 用Q网络选择最优动作
            next_actions = self.Q(next_states).argmax(dim=1)
            # 用Q_target网络评估这个动作的价值
            max_next_q = self.Q_target(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
            target_q = rewards + self.gamma * max_next_q * (1 - dones)
        
        # 计算损失
        loss = self.loss_fn(q_values, target_q)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, env, num_episodes=1000, batch_size=32, target_update_freq=500):
        rewards = []
        step_count = 0
        
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            episode_reward = 0
            
            while not done:
                action = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                self.replay_buffer.append((state, action, reward, next_state, done))
                self.train_step(batch_size)
                
                episode_reward += reward
                state = next_state
                step_count += 1
                
                if step_count % target_update_freq == 0:
                    self.Q_target.load_state_dict(self.Q.state_dict())
            
            rewards.append(episode_reward)
            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        
        return rewards
  </code></pre>

  <h3>六、参数说明（一般设置）</h3>
  <table>
    <tr><th>参数</th><th>建议范围</th><th>说明</th></tr>
    <tr><td>γ（折扣因子）</td><td>0.95~0.99</td><td>长期奖励权重</td></tr>
    <tr><td>学习率 lr</td><td>1e-4~1e-3</td><td>Adam 或 RMSProp 优化器</td></tr>
    <tr><td>ε 初值</td><td>1.0</td><td>初期高探索</td></tr>
    <tr><td>ε 最小值</td><td>0.01~0.1</td><td>保持一定探索</td></tr>
    <tr><td>ε 衰减率</td><td>0.995~0.999</td><td>逐步降低 ε</td></tr>
    <tr><td>经验回放大小</td><td>1e4~1e6</td><td>根据环境大小选择</td></tr>
    <tr><td>批量大小 batch_size</td><td>32~64</td><td>每次训练采样量</td></tr>
    <tr><td>目标网络更新频率</td><td>500~10000 steps</td><td>定期同步 Q 网络到目标网络</td></tr>
  </table>

  <h3>七、训练完成判断</h3>
  同上
  <h3>八、Q 网络参数更新与过估计分析</h3>
  <ul>
    <li><strong>Q 网络参数 θ 实时更新：</strong>用于选择动作，随着梯度下降不断训练</li>
    <li><strong>目标网络参数 θ⁻ 隔一段时间更新：</strong>用于评价动作价值，保持稳定避免 max 操作把偶然高估放大</li>
    <li><strong>训练逻辑：</strong>主网络实时学 → 选择动作；目标网络稳定 → 评价动作；定期同步 → θ⁻ ← θ</li>
    <li>这样做的直觉：主网络像“冲动的玩家”，目标网络像“稳重的裁判”，降低 Q 值过估计，提高训练稳定性</li>
  </ul>

</section>
<!-- 第十一章 -->
<section id="chapter11" class="chapter">
  <h2>第十一章：策略梯度方法（Policy Gradient, PG）</h2>

  <p>策略梯度方法直接对策略 <code>&pi;(a|s;θ)</code> 参数化优化，而不是先估计 Q 值再选动作。适用于动作空间连续或复杂的环境。</p>

  <h3>一、策略梯度目标</h3>
  <p>最大化期望累计奖励：</p>
  <p>$$
  J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]
  $$</p>
  <ul>
    <li><code>θ</code>：策略参数</li>
    <li><code>τ</code>：从策略生成的轨迹（state-action序列）</li>
    <li><code>R(τ)</code>：轨迹总奖励</li>
  </ul>

  <h3>二、策略梯度定理</h3>
  <p>策略梯度公式：</p>
  <p>$$
  \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [ \nabla_\theta \log \pi_\theta(a|s) \, Q^\pi(s,a) ]
  $$</p>
  <ul>
    <li><code>∇θ log πθ(a|s)</code>：调整策略参数 θ 改变动作概率的方向</li>
    <li><code>Qπ(s,a)</code>：动作价值，用来衡量动作好坏</li>
  </ul>

  <h3>三、核心直觉</h3>
  <ul>
    <li>如果动作 a 的回报 Q 高 → 增加 π(a|s) 的概率</li>
    <li>如果动作 a 的回报 Q 低 → 减少 π(a|s) 的概率</li>
  </ul>
  <p>数学来源：更新公式 <code>Δθ ∝ ∇θ log π(a|s) * Q(s,a)</code>，Q(s,a) 大 → 强烈增加概率，Q(s,a) 小 → 可能减少概率。</p>
  <p>概率角度：log π 梯度告诉我们如何调节 θ，使选择动作概率增大/减小；乘上 Q(s,a) 让调整和动作好坏挂钩。</p>

  <h3>四、REINFORCE 算法（Monte Carlo Policy Gradient）</h3>
  <pre><code class="language-python">
for episode in range(num_episodes):
    states, actions, rewards = run_episode(env, policy)
    G = 0
    returns = []
    # 计算回合累计回报
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    
    # 更新策略
    for s, a, Gt in zip(states, actions, returns):
        theta += alpha * grad_log_pi(s, a) * Gt
  </code></pre>


  <h3>五、策略梯度训练完成判断</h3>
  <ul>
    <li>策略稳定性：连续若干回合策略选择动作概率变化不大</li>
    <li>平均累计奖励收敛：测试环境下平均奖励达到目标</li>
  </ul>

  
</section>
<!-- 第十二章 -->
<section id="chapter12" class="chapter">
  <h2>第十二章：Actor-Critic (AC) 方法</h2>

  <p>Actor-Critic 方法是策略梯度家族的一种改进方法，通过结合价值函数（Critic）来降低策略梯度的方差，提高训练稳定性和收敛速度。</p>

  <h3>一、REINFORCE 与 Actor-Critic 对比</h3>
  <table>
    <tr>
      <th>方法</th>
      <th>回报估计方式</th>
      <th>策略更新依据</th>
      <th>更新频率</th>
      <th>方差</th>
    </tr>
    <tr>
      <td>REINFORCE</td>
      <td>完整回合累计回报 G_t</td>
      <td>G_t * ∇ log π(a|s)</td>
      <td>回合结束</td>
      <td>高</td>
    </tr>
    <tr>
      <td>Actor-Critic (AC)</td>
      <td>TD 误差 δ_t = r + γ V(s') - V(s)</td>
      <td>δ_t * ∇ log π(a|s)</td>
      <td>每一步（在线）</td>
      <td>中低</td>
    </tr>
  </table>
  <p>直观理解：REINFORCE 等整回合结束才打分，而 AC 每一步都有 Critic 指导 Actor，因此训练更快更稳。</p>

  <h3>二、核心思想</h3>
  <ul>
    <li><b>Actor</b>：生成动作策略 π(a|s;θ_actor)，直接输出动作概率</li>
    <li><b>Critic</b>：评估状态价值 V(s;θ_critic)，提供 TD 误差 δ_t 来指导 Actor 更新</li>
  </ul>
  <p>直观理解：Actor 是“决策者”，Critic 是“裁判”，裁判告诉决策者动作好坏，Actor 根据反馈调整策略。</p>

  <h3>三、核心公式</h3>
  <ul>
    <li><b>TD 误差（Critic）</b>：
      <p>$$
      \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
      $$</p>
    </li>
    <li><b>Actor 更新</b>：
      <p>$$
      \theta_\text{Actor} \gets \theta_\text{Actor} + \alpha_\text{Actor} \nabla_\theta \log \pi_\theta(a|s) \, \delta_t
      $$</p>
    </li>
    <li><b>Critic 更新</b>：
      <p>$$
      \theta_\text{Critic} \gets \theta_\text{Critic} + \alpha_\text{Critic} \delta_t \nabla_\theta V_\theta(s)
      $$</p>
    </li>
  </ul>

  <h3>四、训练流程</h3>
  <ol>
    <li>初始化 Actor 和 Critic 网络</li>
    <li>与环境交互，选择动作 a_t ∼ π(a|s_t)</li>
    <li>执行动作，获得 r_{t+1}, s_{t+1}</li>
    <li>Critic 计算 TD 误差 δ_t</li>
    <li>Actor 使用 δ_t 更新策略</li>
    <li>Critic 使用 δ_t 更新价值函数</li>
    <li>重复以上步骤直到训练完成</li>
  </ol>

  <h3>五、Actor-Critic 示例代码</h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

class ActorNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(ActorNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.softmax(self.fc3(x), dim=-1)

class CriticNetwork(nn.Module):
    def __init__(self, state_dim):
        super(CriticNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 1)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class ActorCritic:
    def __init__(self, state_dim, action_dim, gamma=0.99, actor_lr=1e-3, critic_lr=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        
        self.actor = ActorNetwork(state_dim, action_dim)
        self.critic = CriticNetwork(state_dim)
        
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)
    
    def select_action(self, state):
        """采样动作"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            action_probs = self.actor(state_tensor)
        dist = Categorical(action_probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)
    
    def train_step(self, state, action, reward, next_state, done, log_prob):
        """单步训练"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
        
        # Critic评估
        with torch.no_grad():
            value = self.critic(state_tensor).squeeze()
            next_value = self.critic(next_state_tensor).squeeze()
            if done:
                target_value = reward
            else:
                target_value = reward + self.gamma * next_value
        
        # 计算TD误差（优势函数）
        advantage = target_value - value
        
        # 更新Actor（策略梯度）
        actor_loss = -log_prob * advantage.detach()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # 更新Critic（值函数）
        critic_loss = (advantage ** 2).mean()
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        return actor_loss.item(), critic_loss.item()
    
    def train_episode(self, env):
        """运行一个完整回合"""
        state = env.reset()
        done = False
        episode_reward = 0
        
        while not done:
            action, log_prob = self.select_action(state)
            next_state, reward, done, _ = env.step(action)
            
            self.train_step(state, action, reward, next_state, done, log_prob)
            
            episode_reward += reward
            state = next_state
        
        return episode_reward
    
    def train(self, env, num_episodes=1000):
        """训练过程"""
        rewards = []
        for episode in range(num_episodes):
            reward = self.train_episode(env)
            rewards.append(reward)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        return rewards
  </code></pre>

  <h3>六、训练完成判断</h3>
  <ul>
    <li>平均累计奖励稳定并收敛</li>
    <li>Actor 策略概率变化不大</li>
    <li>Critic TD loss 收敛</li>
  </ul>

  <h3>七、直观理解总结</h3>
  <ul>
    <li>Actor 决策，Critic 评价</li>
    <li>TD 误差指导 Actor 更新，比 REINFORCE 高效、低方差</li>
    <li>可在线逐步更新策略，不必等整回合结束</li>
  </ul>

</section>
<!-- 第十三章 -->
<section id="chapter13" class="chapter">
  <h2>第十三章：Advantage Actor-Critic (A2C) 与 Asynchronous Advantage Actor-Critic (A3C)</h2>

  <p>A2C 和 A3C 是 Actor-Critic 系列方法的改进，核心都是使用优势函数 <code>A(s,a) = Q(s,a) - V(s)</code> 来降低策略梯度方差，并引入多线程机制提高训练效率和稳定性。</p>

  <h3>一、Advantage Actor-Critic (A2C)</h3>
  <p><b>核心公式：</b></p>
  <ul>
    <li>Actor 更新：
      <p>$$
      \theta_\text{Actor} \gets \theta_\text{Actor} + \alpha_\text{Actor} \nabla_\theta \log \pi_\theta(a|s) \, A(s,a)
      $$</p>
    </li>
    <li>Critic 更新：
      <p>$$
      \theta_\text{Critic} \gets \theta_\text{Critic} + \alpha_\text{Critic} \delta_t \nabla_\theta V_\theta(s)
      $$</p>
    </li>
  </ul>

  <p><b>多线程机制：</b>同步多环境训练。多个环境同时采样状态-动作序列，将梯度累积后同步更新全局网络。</p>
  <ul>
    <li>不异步，所有环境在同一时刻完成一个批量更新</li>
    <li>充分利用 GPU 并行采样</li>
    <li>优点：稳定、方差低，训练效率高</li>
  </ul>

  <h3>A2C 示例代码</h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
from multiprocessing import Process, Queue

class A2CNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(A2CNetwork, self).__init__()
        self.fc = nn.Linear(state_dim, 256)
        
        # Actor头
        self.actor_head = nn.Sequential(
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )
        
        # Critic头
        self.critic_head = nn.Sequential(
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        shared = self.fc(x)
        action_probs = self.actor_head(shared)
        value = self.critic_head(shared)
        return action_probs, value

class A2C:
    def __init__(self, state_dim, action_dim, num_envs=8, gamma=0.99, lr=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.num_envs = num_envs
        self.gamma = gamma
        
        self.network = A2CNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.network.to(self.device)
    
    def compute_gae(self, rewards, values, dones, gamma=0.99, gae_lambda=0.95):
        """计算广义优势估计（Generalized Advantage Estimation）"""
        advantages = []
        gae = 0
        next_value = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_nonterminal = 1.0 - dones[t]
                next_value_t = next_value
            else:
                next_nonterminal = 1.0 - dones[t]
                next_value_t = values[t + 1]
            
            delta = rewards[t] + gamma * next_value_t * next_nonterminal - values[t]
            gae = delta + gamma * gae_lambda * next_nonterminal * gae
            advantages.insert(0, gae)
        
        return np.array(advantages)
    
    def train_iteration(self, envs, max_steps=2048):
        """执行一次迭代（多环境同步采样）"""
        batch_states = []
        batch_actions = []
        batch_rewards = []
        batch_dones = []
        batch_values = []
        batch_log_probs = []
        
        states = [env.reset() for env in envs]
        
        for step in range(max_steps):
            state_tensors = torch.FloatTensor(np.array(states)).to(self.device)
            
            with torch.no_grad():
                action_probs, values = self.network(state_tensors)
            
            # 从每个环境采样动作
            actions = []
            log_probs = []
            for i, (probs, val) in enumerate(zip(action_probs, values)):
                dist = Categorical(probs)
                action = dist.sample()
                log_prob = dist.log_prob(action)
                actions.append(action.item())
                log_probs.append(log_prob)
            
            # 执行动作
            rewards = []
            dones = []
            next_states = []
            for i, env in enumerate(envs):
                next_state, reward, done, _ = env.step(actions[i])
                rewards.append(reward)
                dones.append(done)
                if done:
                    next_state = env.reset()
                next_states.append(next_state)
            
            batch_states.extend(states)
            batch_actions.extend(actions)
            batch_rewards.extend(rewards)
            batch_dones.extend(dones)
            batch_values.extend([v.item() for v in values])
            batch_log_probs.extend(log_probs)
            
            states = next_states
        
        # 计算优势函数
        advantages = self.compute_gae(batch_rewards, batch_values, batch_dones, self.gamma)
        returns = advantages + np.array(batch_values)
        
        # 标准化优势
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # 更新网络
        batch_size = 32
        for epoch in range(3):
            indices = np.random.permutation(len(batch_states))
            for i in range(0, len(batch_states), batch_size):
                batch_indices = indices[i:i+batch_size]
                
                states_batch = torch.FloatTensor(np.array(batch_states)[batch_indices]).to(self.device)
                actions_batch = torch.LongTensor(np.array(batch_actions)[batch_indices]).to(self.device)
                advantages_batch = torch.FloatTensor(advantages[batch_indices]).to(self.device)
                returns_batch = torch.FloatTensor(returns[batch_indices]).to(self.device)
                
                action_probs, values = self.network(states_batch)
                
                # 计算策略损失
                dist = Categorical(action_probs)
                log_probs = dist.log_prob(actions_batch)
                policy_loss = -(log_probs * advantages_batch).mean()
                
                # 计算价值损失
                value_loss = ((values.squeeze() - returns_batch) ** 2).mean()
                
                # 总损失
                loss = policy_loss + 0.5 * value_loss - 0.01 * dist.entropy().mean()
                
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
                self.optimizer.step()
    
    def train(self, env_fn, num_iterations=1000):
        """训练过程"""
        envs = [env_fn() for _ in range(self.num_envs)]
        
        for iteration in range(num_iterations):
            self.train_iteration(envs)
            
            if (iteration + 1) % 100 == 0:
                print(f"Iteration {iteration+1} completed")
  </code></pre>

  <h3>二、Asynchronous Advantage Actor-Critic (A3C)</h3>
  <p><b>核心公式：</b>与 A2C 相同，使用优势函数更新 Actor，TD 误差更新 Critic。</p>

  <p><b>多线程机制：</b>异步多线程训练，每个线程独立采样环境并维护局部 Actor-Critic 副本，计算梯度后立即异步更新全局网络：</p>
  <ul>
    <li>增强探索性：各线程采样的状态分布不同</li>
    <li>避免单线程陷入局部最优</li>
    <li>训练效率更高，收敛稳定性更好</li>
    <li>缺点：梯度随机性较大，需要较小学习率</li>
  </ul>

  <h3>A3C 示例代码</h3>
  <pre><code class="language-python">
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import threading
from multiprocessing import Process, Queue
import time

class A3CNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(A3CNetwork, self).__init__()
        self.fc = nn.Linear(state_dim, 256)
        
        self.actor_head = nn.Sequential(
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )
        
        self.critic_head = nn.Sequential(
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        shared = self.fc(x)
        return self.actor_head(shared), self.critic_head(shared)

class A3CWorker(threading.Thread):
    def __init__(self, global_network, global_optimizer, env, worker_id, max_steps=100):
        super(A3CWorker, self).__init__()
        self.global_network = global_network
        self.global_optimizer = global_optimizer
        self.env = env
        self.worker_id = worker_id
        self.max_steps = max_steps
        self.daemon = True
        
        # 本地网络副本
        self.local_network = A3CNetwork(
            self.global_network.fc.in_features,
            self.global_network.actor_head[-2].out_features
        )
        self.local_network.load_state_dict(self.global_network.state_dict())
        self.local_optimizer = optim.Adam(self.local_network.parameters(), lr=1e-3)
    
    def run(self):
        """异步训练循环"""
        state = self.env.reset()
        
        while True:
            # 采集本地轨迹
            batch_states = []
            batch_actions = []
            batch_rewards = []
            batch_values = []
            
            for step in range(self.max_steps):
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                
                with torch.no_grad():
                    action_probs, value = self.local_network(state_tensor)
                
                # 采样动作
                dist = Categorical(action_probs.squeeze())
                action = dist.sample()
                
                next_state, reward, done, _ = self.env.step(action.item())
                
                batch_states.append(state)
                batch_actions.append(action)
                batch_rewards.append(reward)
                batch_values.append(value.item())
                
                state = next_state if not done else self.env.reset()
            
            # 计算TD目标
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            with torch.no_grad():
                _, bootstrap_value = self.local_network(state_tensor)
            bootstrap_value = bootstrap_value.item()
            
            # 反向计算累计奖励
            returns = []
            G = bootstrap_value
            for reward in reversed(batch_rewards):
                G = reward + 0.99 * G
                returns.insert(0, G)
            returns = np.array(returns)
            
            # 标准化优势
            advantages = returns - np.array(batch_values)
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # 更新本地网络
            for state, action, advantage, ret in zip(batch_states, batch_actions, advantages, returns):
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                action_probs, value = self.local_network(state_tensor)
                
                # 计算损失
                dist = Categorical(action_probs)
                log_prob = dist.log_prob(action)
                
                policy_loss = -(log_prob * advantage)
                value_loss = ((value.squeeze() - ret) ** 2)
                entropy_bonus = -0.01 * dist.entropy()
                
                loss = policy_loss + 0.5 * value_loss + entropy_bonus
                
                self.local_optimizer.zero_grad()
                loss.backward()
                self.local_optimizer.step()
            
            # 异步更新全局网络
            self._async_update_global()
            
            # 同步本地网络
            self.local_network.load_state_dict(self.global_network.state_dict())
    
    def _async_update_global(self):
        """异步更新全局网络"""
        # 将本地梯度累积到全局网络
        for local_param, global_param in zip(
            self.local_network.parameters(),
            self.global_network.parameters()
        ):
            if global_param.grad is not None:
                global_param.grad.data += local_param.grad.data

class A3C:
    def __init__(self, state_dim, action_dim, num_workers=8):
        self.global_network = A3CNetwork(state_dim, action_dim)
        self.global_optimizer = optim.Adam(self.global_network.parameters(), lr=1e-3)
        self.num_workers = num_workers
    
    def train(self, env_fn, num_iterations=10000):
        """启动多个异步线程训练"""
        workers = []
        for i in range(self.num_workers):
            worker = A3CWorker(
                self.global_network,
                self.global_optimizer,
                env_fn(),
                worker_id=i
            )
            worker.start()
            workers.append(worker)
        
        # 等待所有线程完成
        for worker in workers:
            worker.join()

# 使用示例
if __name__ == '__main__':
    # env_fn = lambda: gym.make('CartPole-v1')
    # a3c = A3C(state_dim=4, action_dim=2, num_workers=8)
    # a3c.train(env_fn, num_iterations=10000)
    pass
  </code></pre>

  <h3>三、训练完成判断</h3>
  <ul>
    <li>平均累计奖励稳定并收敛</li>
    <li>Actor 策略概率变化不大</li>
    <li>Critic TD loss 收敛</li>
    <li>多线程训练下，梯度更新稳定，奖励曲线平滑</li>
  </ul>

  <h3>四、A2C 与 A3C 对比</h3>
  <table>
    <tr><th>特性</th><th>A2C</th><th>A3C</th></tr>
    <tr><td>多线程类型</td><td>同步多环境</td><td>异步多线程</td></tr>
    <tr><td>梯度更新</td><td>所有环境梯度累积后同步更新</td><td>每个线程计算梯度后立即异步更新全局网络</td></tr>
    <tr><td>探索性</td><td>较好</td><td>更高</td></tr>
    <tr><td>收敛稳定性</td><td>高</td><td>高，但随机性大</td></tr>
    <tr><td>实现难度</td><td>中</td><td>高</td></tr>
  </table>

</section>

  </main>
</div>

<footer>
  <p>© 2025 <span>Haoyun Tang</span> | Reinforcement Learning Notes</p>
</footer>

<!-- ===== 动态粒子特效 ===== -->
<script>
const canvas = document.getElementById("trailCanvas");
const ctx = canvas.getContext("2d");
let particles = [];
function resize() {
  canvas.width = window.innerWidth;
  canvas.height = window.innerHeight;
}
window.addEventListener("resize", resize);
resize();
document.addEventListener("mousemove", e => {
  for (let i = 0; i < 2; i++) {
    particles.push({x: e.clientX, y: e.clientY, alpha: 1, r: Math.random()*3+1});
  }
});
function animate() {
  ctx.fillStyle = "rgba(0,0,0,0.2)";
  ctx.fillRect(0, 0, canvas.width, canvas.height);
  particles.forEach(p => {
    p.y -= 0.3;
    p.alpha -= 0.01;
    ctx.beginPath();
    ctx.arc(p.x, p.y, p.r, 0, Math.PI*2);
    ctx.fillStyle = `rgba(0,188,212,${p.alpha})`;
    ctx.fill();
  });
  particles = particles.filter(p => p.alpha > 0);
  requestAnimationFrame(animate);
}
animate();
</script>

<!-- ===== 代码高亮初始化 ===== -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  // 自动添加行号类名
  document.querySelectorAll('pre[class*="language-"]').forEach(pre => {
    if (!pre.classList.contains('line-numbers')) {
      pre.classList.add('line-numbers');
    }
  });
  
  // 触发 Prism 高亮
  if (window.Prism) {
    Prism.highlightAll();
  }
});

// 页面加载后重新高亮
window.addEventListener('load', function() {
  if (window.Prism) {
    Prism.highlightAll();
  }
});
</script>

<!-- ===== 导航自动高亮 ===== -->
<script>
const navLinks = document.querySelectorAll("nav a");
const asideLinks = document.querySelectorAll("aside a");
const allLinks = [...navLinks, ...asideLinks];

window.addEventListener("scroll", () => {
  let fromTop = window.scrollY + 150;
  
  allLinks.forEach(link => {
    const section = document.querySelector(link.getAttribute("href"));
    if (section && section.offsetTop <= fromTop && section.offsetTop + section.offsetHeight > fromTop) {
      link.classList.add("active");
    } else {
      link.classList.remove("active");
    }
  });
});
</script>
</body>
</html>
