<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>强化学习学习笔记 | 唐浩云</title>

<!-- 数学公式渲染 -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
 src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Prism.js 代码高亮 -->
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>

<style>
/* ===== 全局样式 ===== */
body {
  margin: 0;
  font-family: "Segoe UI", "Roboto", sans-serif;
  background-color: #0d0d0d;
  color: #e0e0e0;
  line-height: 1.7;
  scroll-behavior: smooth;
}

/* 背景粒子 */
#trailCanvas {
  position: fixed;
  top: 0; left: 0;
  width: 100%; height: 100%;
  z-index: 0;
  pointer-events: none;
}

/* Header */
header {
  position: sticky;
  top: 0;
  z-index: 10;
  text-align: center;
  background: rgba(20, 20, 20, 0.8);
  padding: 20px;
  backdrop-filter: blur(6px);
  box-shadow: 0 2px 8px rgba(0,0,0,0.6);
}
header h1 {
  font-size: 2.4rem;
  color: #00bcd4;
  margin: 0;
  text-shadow: 0 0 10px rgba(0,188,212,0.6);
}

/* 导航栏 */
nav ul {
  list-style: none;
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  padding: 0;
  margin: 10px 0 0;
}
nav li { margin: 5px 15px; }
nav a {
  color: #e0e0e0;
  text-decoration: none;
  font-weight: bold;
  transition: 0.3s;
}
nav a:hover, nav a.active { color: #00bcd4; text-shadow: 0 0 5px #00bcd4; }

/* 内容区 */
main {
  position: relative;
  z-index: 5;
  max-width: 900px;
  margin: 20px auto;
  padding: 20px;
}
.chapter {
  background: rgba(30,30,30,0.85);
  border-radius: 12px;
  padding: 25px;
  margin-bottom: 40px;
  box-shadow: 0 0 20px rgba(0,188,212,0.1);
  transition: transform 0.2s;
}
.chapter:hover { transform: translateY(-3px); }
.chapter h2 {
  color: #00bcd4;
  border-bottom: 2px solid #00bcd4;
  padding-bottom: 6px;
}
pre {
  background: #1e1e1e;
  padding: 15px;
  border-radius: 8px;
  overflow-x: auto;
  font-size: 0.95rem;
}

/* 表格优化 */
table {
  width: 100%;
  border-collapse: collapse;
  margin-top: 15px;
  background: rgba(255,255,255,0.05);
  border-radius: 8px;
  overflow: hidden;
}
table th, table td {
  padding: 10px 15px;
  border-bottom: 1px solid rgba(255,255,255,0.1);
}
table th {
  background: rgba(0,188,212,0.2);
  color: #00e5ff;
}

/* Footer */
footer {
  text-align: center;
  padding: 15px;
  margin: 40px 0 10px;
  font-size: 0.9rem;
  color: #aaa;
  background: rgba(20,20,20,0.7);
  border-radius: 8px;
}
footer span {
  color: #00bcd4;
  animation: glow 2s infinite alternate;
}
@keyframes glow {
  from { text-shadow: 0 0 5px #00bcd4; }
  to { text-shadow: 0 0 15px #00bcd4; }
}
</style>
</head>

<body>
<canvas id="trailCanvas"></canvas>

<header>
  <h1>强化学习学习笔记</h1>
  <nav>
    <ul>
      <li><a href="#chapter0">环境配置</a></li>
      <li><a href="#chapter1">基本概念</a></li>
      <li><a href="#chapter2">贝尔曼方程</a></li>
      <li><a href="#chapter3">贝尔曼最优方程</a></li>
      <li><a href="#chapter4">策略迭代</a></li>
      <li><a href="#chapter5">值迭代</a></li>
      <li><a href="#chapter6">蒙特卡洛方法</a></li>
    </ul>
  </nav>
</header>

<main>
  <section id="chapter0" class="chapter">
    <h2>第零章：环境配置 & GitHub 项目上传</h2>
    <pre><code class="language-bash"># 配置镜像源
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config --set show_channel_urls yes

# 创建虚拟环境
cd /d G:\test
conda create --prefix .\.venv python=3.11
conda activate G:\test\.venv

# 安装依赖
conda install numpy pandas matplotlib pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia

# 上传到 GitHub
git init
git remote add origin https://github.com/HaoyunT/test.git
git checkout -b main
git add .
git commit -m "初始化项目"
git push -u origin main</code></pre>
  </section>

<!-- 第一章 -->
<section id="chapter1" class="chapter">
  <h2>第一章：强化学习基本概念</h2>
  <p>强化学习（Reinforcement Learning, RL）核心概念：</p>
  <dl>
    <dt>智能体（Agent）</dt>
    <dd>决策执行主体。在每个时间步 \(t\) 观察状态 \(S_t\)，选择动作 \(A_t\)，并通过奖励信号调整策略。<br><em>示例：</em>游戏中的玩家角色。</dd>

    <dt>环境（Environment）</dt>
    <dd>智能体交互对象，定义状态空间、动作空间和状态转移规则。动作后返回状态 \(S_{t+1}\) 和奖励 \(R_t\)。<br><em>示例：</em>游戏关卡或物理模拟器。</dd>

    <dt>状态（State）</dt>
    <dd>描述环境在某一时刻的特征，满足马尔可夫性质。<br><em>示例：</em>角色位置、敌人位置。</dd>

    <dt>动作（Action）</dt>
    <dd>智能体可执行的操作，改变环境状态。<br><em>示例：</em>左右移动、跳跃。</dd>

    <dt>奖励（Reward）</dt>
    <dd>环境对动作的反馈，用于衡量优劣。强化学习目标最大化累计奖励：
      <br>\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k}\)
    </dd>

    <dt>策略（Policy）</dt>
    <dd>选择动作的规则，\(\pi(a|s)\) 或 \(a = \pi(s)\)。目标是找到最优策略 \(\pi^*\)。</dd>

    <dt>价值函数（Value Function）</dt>
    <dd>衡量状态或状态-动作对的长期回报：
      <ul>
        <li>状态价值：\(V^\pi(s) = \mathbb{E}[G_t | S_t=s]\)</li>
        <li>动作价值：\(Q^\pi(s,a) = \mathbb{E}[G_t | S_t=s, A_t=a]\)</li>
      </ul>
    </dd>
  </dl>
</section>

<!-- 第二章 -->
<section id="chapter2" class="chapter">
  <h2>第二章：贝尔曼方程（Bellman Equation）</h2>
  <p>贝尔曼方程用于描述策略下状态或状态-动作的价值递归关系，是强化学习中<strong>策略评估的核心工具</strong>。</p>

  <dl>
    <dt><b>状态价值函数（State Value Function）</b></dt>
    <dd>
      对于策略 <code>π</code> 下的状态价值函数 <code>V^π(s)</code>：<br>
      <p>$$ V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s,a) \big[ R(s,a) + \gamma V^\pi(s') \big] $$</p>
      说明：当前状态价值 = 即时奖励 + 折扣后的未来价值期望。
    </dd>

    <dt><b>状态-动作价值函数（Action-Value Function）</b></dt>
    <dd>
      对于状态-动作对的价值函数 <code>Q^π(s,a)</code>：<br>
      <p>$$ Q^\pi(s,a) = R(s,a) + \gamma \sum_{s' \in S} \sum_{a' \in A} \pi(a'|s') Q^\pi(s',a') $$</p>
    </dd>

    <dt><b>状态价值函数与状态-动作价值函数的关系</b></dt>
    <dd>
      状态价值函数可以由状态-动作价值函数得到：<br>
      <p>$$ V^\pi(s) = \sum_{a \in A} \pi(a|s) Q^\pi(s,a) $$</p>
      解释：在状态 <code>s</code> 下，价值函数 <code>V^π(s)</code> 是策略选择动作后的期望 <code>Q^π(s,a)</code>。
    </dd>
  </dl>

  <p><strong>用途：</strong>策略评估、策略迭代、价值迭代的理论基础。</p>
</section>


<!-- 第三章 -->
<section id="chapter3" class="chapter">
  <h2>第三章：贝尔曼最优方程（Bellman Optimality Equation）</h2>

  <p>在寻找最优策略 <code>π*</code> 时，状态和状态-动作的价值函数满足<strong>贝尔曼最优方程</strong>，体现最优性原则。</p>

  <dl>
    <dt><b>最优状态价值函数</b></dt>
    <dd>
      <p>$$
      V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s,a) \big[ R(s,a) + \gamma V^*(s') \big]
      $$</p>
    </dd>

    <dt><b>最优状态-动作价值函数</b></dt>
    <dd>
      <p>$$
      Q^*(s,a) = R(s,a) + \gamma \sum_{s' \in S} \max_{a' \in A} Q^*(s',a')
      $$</p>
    </dd>
  </dl>

  <p><strong>关系总结：</strong></p>
  <ul>
    <li>第二章贝尔曼方程：给定策略 π → 计算 V^π 或 Q^π</li>
    <li>第三章贝尔曼最优方程：求最优策略 π* → V* 或 Q*</li>
  </ul>
</section>
<!-- 第四章 -->
<section id="chapter4" class="chapter">
  <h2>第四章：策略迭代（Policy Iteration）</h2>

  <p>策略迭代通过交替进行策略评估和策略改进来收敛到最优策略。</p>

  <ol>
    <li><strong>初始化策略：</strong>选择初始策略 <code>π_0</code></li>
    <li><strong>策略评估：</strong>计算状态价值函数：
      <p>$$
      v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}
      $$</p>
    </li>
    <li><strong>策略改进：</strong>更新策略：
      <p>$$
      \pi_{k+1} = \arg\max_\pi (r_\pi + \gamma P_\pi v_{\pi_k})
      $$</p>
    </li>
    <li>重复评估和改进，直到策略收敛。</li>
  </ol>
</section>


<!-- 第五章 -->
<section id="chapter5" class="chapter">
  <h2>第五章：值迭代（Value Iteration）</h2>

  <p>值迭代直接迭代状态价值函数，通过贝尔曼最优方程收敛到最优值函数，然后导出最优策略。</p>

  <ol>
    <li><strong>初始化价值函数：</strong>选择初始值 <code>v_0</code></li>
    <li><strong>迭代更新：</strong>使用贝尔曼最优方程：
      <p>$$
      v_{k+1} = \max_\pi (r_\pi + \gamma P_\pi v_k)
      $$</p>
    </li>
    <li><strong>策略导出：</strong>收敛后选择最优动作：
      <p>$$
      \pi^*(s) = \arg\max_a \sum_{s'} P(s'|s,a) \big[R(s,a) + \gamma v^*(s') \big]
      $$</p>
    </li>
  </ol>
</section>


<!-- 对比-->
<section id="chapterX" class="chapter">
  <h2>策略迭代与值迭代的对比</h2>

  <table border="1" cellpadding="8" cellspacing="0">
    <thead>
      <tr>
        <th></th>
        <th>Policy Iteration</th>
        <th>Value Iteration</th>
        <th>Comments</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>(1)Policy</td>
        <td>\(\pi_0\)</td>
        <td>N/A</td>
        <td></td>
      </tr>
      <tr>
        <td>(2)Value</td>
        <td>\(v_{\pi_0} = r_{\pi_0} + \gamma P_{\pi_0} v_{\pi_0}\)</td>
        <td>\(v_0 := v_{\pi_0}\)</td>
        <td></td>
      </tr>
      <tr>
        <td>(3)Policy</td>
        <td>\(\pi_1 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_{\pi_0} )\)</td>
        <td>\(\pi_1 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_0 )\)</td>
        <td>The two policies are the same</td>
      </tr>
      <tr>
        <td>(4)Value</td>
        <td>\(v_{\pi_1} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}\)</td>
        <td>\(v_1 = r_{\pi_1} + \gamma P_{\pi_1} v_0\)</td>
        <td>\(v_{\pi_1} \ge v_1 \text{ since } v_{\pi_1} \ge v_{\pi_0}\)</td>
      </tr>
      <tr>
        <td>5) Policy</td>
        <td>\(\pi_2 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_{\pi_1} )\)</td>
        <td>\(\pi'_2 = \arg\max_\pi ( r_\pi + \gamma P_\pi v_1 )\)</td>
        <td></td>
      </tr>
    </tbody>
  </table>
</section>

 <!-- 第六章 -->
<section id="chapter6" class="chapter">
  <h2>第六章：蒙特卡洛方法（Monte Carlo Methods）</h2>

  <p>蒙特卡洛方法（Monte Carlo，简称 <b>MC</b>）是强化学习中一种基于<strong>采样</strong>的策略评估与优化方法。
  与动态规划不同，MC 不依赖环境的状态转移概率模型 <code>P(s'|s,a)</code>，而是通过反复从策略 <code>π</code> 生成完整回合（Episode），
  根据实际获得的回报估计状态或状态-动作价值。</p>

  <h3>一、基本思想</h3>
  <p>给定策略 \( \pi \)，智能体与环境交互多次，得到若干完整回合：</p>
  <p>$$
  (S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_T)
  $$</p>
  <p>对于每个回合，定义从时间步 \(t\) 开始的累计回报（Return）：</p>
  <p>$$
  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}
  $$</p>
  <p>MC 方法通过多次采样，利用这些 \(G_t\) 的平均值来近似价值函数。</p>

  <dl>
    <dt><b>状态价值估计：</b></dt>
    <dd>$$
    V(s) = \mathbb{E}_\pi[G_t \mid S_t = s] \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G^{(i)}(s)
    $$</dd>

    <dt><b>动作价值估计：</b></dt>
    <dd>$$
    Q(s,a) = \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a] \approx \frac{1}{N(s,a)} \sum_{i=1}^{N(s,a)} G^{(i)}(s,a)
    $$</dd>
  </dl>

  <h3>二、首次访问与每次访问</h3>
  <ul>
    <li><b>First-Visit MC：</b> 仅在一个回合中第一次访问状态（或状态-动作对）时进行更新。</li>
    <li><b>Every-Visit MC：</b> 对回合中每次访问状态（或状态-动作对）都进行更新。</li>
  </ul>
  <p>二者最终都会收敛到真实的价值函数，但首次访问 MC 方差略小。</p>

  <h3>三、蒙特卡洛策略评估算法</h3>
  <pre><code>
# 给定策略 π，评估其状态价值函数 V(s)
初始化：
  对所有状态 s，设 Returns(s) = []
  对所有状态 s，设 V(s) = 0

循环多次：
  生成一个完整回合 (S_0, A_0, R_1, S_1, ..., S_T)
  G ← 0
  对 t = T-1, T-2, ..., 0：
      G ← γ * G + R_{t+1}
      如果 S_t 在该回合中第一次出现：
          将 G 加入 Returns(S_t)
          V(S_t) ← 平均(Returns(S_t))
  </code></pre>

  <p>算法通过样本平均值逼近状态的期望回报，属于<strong>无模型、基于采样</strong>的策略评估。</p>

  <h3>四、蒙特卡洛控制（MC Control）</h3>
  <p>在策略评估的基础上，MC 还可以实现<strong>策略改进</strong>，形成一个与策略迭代类似的过程：</p>
  <ol>
    <li>评估当前策略 \(\pi\)：使用 MC 方法估计 \(Q^\pi(s,a)\)</li>
    <li>改进策略：采用贪婪或 ε-贪婪方式更新策略
      <p>$$
      \pi'(s) = \arg\max_a Q(s,a)
      $$</p>
    </li>
    <li>重复以上过程，直到收敛</li>
  </ol>

  <h3>五、ε-贪婪（ε-Greedy）探索策略</h3>
  <p>为了避免过早陷入次优策略，MC 控制中常用 <b>ε-贪婪策略</b> 进行探索：</p>
  <p>$$
  \pi(a|s) = 
  \begin{cases}
  1 - \varepsilon + \frac{\varepsilon}{|A(s)|}, & a = \arg\max_{a'} Q(s,a') \\
  \frac{\varepsilon}{|A(s)|}, & \text{否则}
  \end{cases}
  $$</p>
  <p>其中 \(\varepsilon \in [0,1]\) 表示随机探索概率。</p>

  <h3>六、MC 控制伪代码</h3>
  <pre><code>
# 蒙特卡洛控制（ε-贪婪）
初始化：
  对所有 (s,a)，Q(s,a) = 0
  对所有 (s,a)，Returns(s,a) = []
  定义 ε-贪婪策略 π 基于 Q

循环多次：
  生成一个回合 (S_0, A_0, R_1, ..., S_T)
  G ← 0
  对 t = T-1, ..., 0：
      G ← γ * G + R_{t+1}
      如果 (S_t, A_t) 第一次出现：
          将 G 加入 Returns(S_t, A_t)
          Q(S_t, A_t) ← 平均(Returns(S_t, A_t))
          π(S_t) ← ε-贪婪(Q)
  </code></pre>


  <h3>七、MC 方法的特点与局限</h3>
  <table border="1" cellpadding="8" cellspacing="0">
    <thead>
      <tr><th>优点</th><th>缺点</th></tr>
    </thead>
    <tbody>
      <tr>
        <td>无需知道环境模型（P、R）</td>
        <td>必须等待完整回合结束，不能用于持续任务</td>
      </tr>
      <tr>
        <td>实现简单，概念直观</td>
        <td>收敛速度较慢，方差大</td>
      </tr>
      <tr>
        <td>适合离线模拟环境（如游戏）</td>
        <td>对长期任务或高维状态空间不适用</td>
      </tr>
    </tbody>
  </table>

  <h3>八、小结</h3>
  <ul>
    <li>MC 是基于采样的<strong>策略评估与控制</strong>方法。</li>
    <li>依赖回合结束的实际回报，而非估计的下一步价值。</li>
    <li>与动态规划相比，不需知道模型，但收敛慢。</li>
    <li>是理解时序差分（TD）学习的重要过渡。</li>
  </ul>

</section>

</main>

<footer>
  <p>© 2025 <span>Haoyun Tang</span> | Reinforcement Learning Notes</p>
</footer>

<!-- ===== 动态粒子特效 ===== -->
<script>
const canvas = document.getElementById("trailCanvas");
const ctx = canvas.getContext("2d");
let particles = [];
function resize() {
  canvas.width = window.innerWidth;
  canvas.height = window.innerHeight;
}
window.addEventListener("resize", resize);
resize();
document.addEventListener("mousemove", e => {
  for (let i = 0; i < 2; i++) {
    particles.push({x: e.clientX, y: e.clientY, alpha: 1, r: Math.random()*3+1});
  }
});
function animate() {
  ctx.fillStyle = "rgba(0,0,0,0.2)";
  ctx.fillRect(0, 0, canvas.width, canvas.height);
  particles.forEach(p => {
    p.y -= 0.3;
    p.alpha -= 0.01;
    ctx.beginPath();
    ctx.arc(p.x, p.y, p.r, 0, Math.PI*2);
    ctx.fillStyle = `rgba(0,188,212,${p.alpha})`;
    ctx.fill();
  });
  particles = particles.filter(p => p.alpha > 0);
  requestAnimationFrame(animate);
}
animate();
</script>

<!-- ===== 导航自动高亮 ===== -->
<script>
const navLinks = document.querySelectorAll("nav a");
window.addEventListener("scroll", () => {
  let fromTop = window.scrollY + 120;
  navLinks.forEach(link => {
    const section = document.querySelector(link.getAttribute("href"));
    if (section.offsetTop <= fromTop && section.offsetTop + section.offsetHeight > fromTop)
      link.classList.add("active");
    else
      link.classList.remove("active");
  });
});
</script>
</body>
</html>
